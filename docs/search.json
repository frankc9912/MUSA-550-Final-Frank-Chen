[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Me",
    "section": "",
    "text": "Hello everyone, My name is Frank Chen, the author of this project. I’m currently enrolled in a dual degree program of Master of City Planning and Master of Urban Spatial Analytics at University of Pennsylvania’s Weitzman School of Design. Before I came to Philadelphia, I completed the 5-year undergraduate study and research at Tianjin University. I’m a data enthusiast and urban planner, and I’m passionate about using data and technology to solve urban problems and make our cities more sustainable, equitable, and resilient. I’m interested in a wide range of topics, including spatial optimization, housing, urban mobility, and public health. I have experience in data analysis, spatial analysis, and data visualization, and I currently work as a research assistant at Housing Initiative at Penn.",
    "crumbs": [
      "About Me"
    ]
  },
  {
    "objectID": "analysis/4-folium.html",
    "href": "analysis/4-folium.html",
    "title": "Interactive Maps with Folium",
    "section": "",
    "text": "This page is generated from a Jupyter notebook and shows examples of embedding interactive maps produced using Folium.",
    "crumbs": [
      "Analysis",
      "Interactive Maps with Folium"
    ]
  },
  {
    "objectID": "analysis/4-folium.html#finding-the-shortest-route",
    "href": "analysis/4-folium.html#finding-the-shortest-route",
    "title": "Interactive Maps with Folium",
    "section": "Finding the shortest route",
    "text": "Finding the shortest route\nThis example finds the shortest route between the Art Musuem and the Liberty Bell using osmnx.\n\nimport osmnx as ox\n\nFirst, identify the lat/lng coordinates for our places of interest. Use osmnx to download the geometries for the Libery Bell and Art Museum.\n\nphilly_tourism = ox.features_from_place(\"Philadelphia, PA\", tags={\"tourism\": True})\n\n\nart_museum = philly_tourism.query(\"name == 'Philadelphia Museum of Art'\").squeeze()\n\nart_museum.geometry\n\n\n\n\n\n\n\n\n\nliberty_bell = philly_tourism.query(\"name == 'Liberty Bell'\").squeeze()\n\nliberty_bell.geometry\n\n\n\n\n\n\n\n\nNow, extract the lat and lng coordinates\nFor the Art Museum geometry, we can use the .geometry.centroid attribute to calculate the centroid of the building footprint.\n\nliberty_bell_x = liberty_bell.geometry.x\nliberty_bell_y = liberty_bell.geometry.y\n\n\nart_museum_x = art_museum.geometry.centroid.x\nart_museum_y = art_museum.geometry.centroid.y\n\nNext, use osmnx to download the street graph around Center City.\n\nG_cc = ox.graph_from_address(\n    \"City Hall, Philadelphia, USA\", dist=1500, network_type=\"drive\"\n)\n\nNext, identify the nodes in the graph closest to our points of interest.\n\n# Get the origin node (Liberty Bell)\norig_node = ox.nearest_nodes(G_cc, liberty_bell_x, liberty_bell_y)\n\n# Get the destination node (Art Musuem)\ndest_node = ox.nearest_nodes(G_cc, art_museum_x, art_museum_y)\n\nFind the shortest path, based on the distance of the edges:\n\n# Get the shortest path --&gt; just a list of node IDs\nroute = ox.shortest_path(G_cc, orig_node, dest_node, weight=\"length\")\n\nHow about an interactive version?\nosmnx has a helper function ox.utils_graph.route_to_gdf() to convert a route to a GeoDataFrame of edges.\n\nox.utils_graph.route_to_gdf(G_cc, route, weight=\"length\").explore(\n    tiles=\"cartodb positron\",\n    color=\"red\",\n)\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook",
    "crumbs": [
      "Analysis",
      "Interactive Maps with Folium"
    ]
  },
  {
    "objectID": "analysis/4-folium.html#examining-trash-related-311-requests",
    "href": "analysis/4-folium.html#examining-trash-related-311-requests",
    "title": "Interactive Maps with Folium",
    "section": "Examining Trash-Related 311 Requests",
    "text": "Examining Trash-Related 311 Requests\nFirst, let’s load the dataset from a CSV file and convert to a GeoDataFrame:\n\n\nCode\n# Load the data from a CSV file into a pandas DataFrame\ntrash_requests_df = pd.read_csv(\n    \"https://raw.githubusercontent.com/MUSA-550-Fall-2023/week-4/main/data/trash_311_requests_2020.csv\"\n)\n\n# Remove rows with missing geometry\ntrash_requests_df = trash_requests_df.dropna(subset=[\"lat\", \"lon\"])\n\n\n# Create our GeoDataFrame with geometry column created from lon/lat\ntrash_requests = gpd.GeoDataFrame(\n    trash_requests_df,\n    geometry=gpd.points_from_xy(trash_requests_df[\"lon\"], trash_requests_df[\"lat\"]),\n    crs=\"EPSG:4326\",\n)\n\n\nLoad neighborhoods and do the spatial join to associate a neighborhood with each ticket:\n\n\nCode\n# Load the neighborhoods\nneighborhoods = gpd.read_file(\n    \"https://raw.githubusercontent.com/MUSA-550-Fall-2023/week-4/main/data/zillow_neighborhoods.geojson\"\n)\n\n# Do the spatial join to add the \"ZillowName\" column\nrequests_with_hood = gpd.sjoin(\n    trash_requests,\n    neighborhoods.to_crs(trash_requests.crs),\n    predicate=\"within\",\n)\n\n\nLet’s explore the 311 requests in the Greenwich neighborhood of the city:\n\n# Extract out the point tickets for Greenwich\ngreenwich_tickets = requests_with_hood.query(\"ZillowName == 'Greenwich'\")\n\n\n# Get the neighborhood boundary for Greenwich\ngreenwich_geo = neighborhoods.query(\"ZillowName == 'Greenwich'\")\n\ngreenwich_geo.squeeze().geometry\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nQuarto has callout blocks that you can use to emphasize content in different ways. This is a “Note” callout block. More info is available on the Quarto documentation.\n\n\nImport the packages we need:\n\nimport folium\nimport xyzservices\n\nCombine the tickets as markers and the neighborhood boundary on the same Folium map:\n\n# Plot the neighborhood boundary\nm = greenwich_geo.explore(\n    style_kwds={\"weight\": 4, \"color\": \"black\", \"fillColor\": \"none\"},\n    name=\"Neighborhood boundary\",\n    tiles=xyzservices.providers.CartoDB.Voyager,\n)\n\n\n# Add the individual tickets as circle markers and style them\ngreenwich_tickets.explore(\n    m=m,  # Add to the existing map!\n    marker_kwds={\"radius\": 7, \"fill\": True, \"color\": \"crimson\"},\n    marker_type=\"circle_marker\", # or 'marker' or 'circle'\n    name=\"Tickets\",\n)\n\n# Hse folium to add layer control\nfolium.LayerControl().add_to(m)\n\nm  # show map\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook",
    "crumbs": [
      "Analysis",
      "Interactive Maps with Folium"
    ]
  },
  {
    "objectID": "analysis/1-python-code-blocks.html",
    "href": "analysis/1-python-code-blocks.html",
    "title": "Python code blocks",
    "section": "",
    "text": "This is an example from the Quarto documentation that shows how to mix executable Python code blocks into a markdown file in a “Quarto markdown” .qmd file.\nFor a demonstration of a line plot on a polar axis, see Figure 1.\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nr = np.arange(0, 2, 0.01)\ntheta = 2 * np.pi * r\nfig, ax = plt.subplots(\n  subplot_kw = {'projection': 'polar'} \n)\nax.plot(theta, r)\nax.set_rticks([0.5, 1, 1.5, 2])\nax.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\nFigure 1: A line plot on a polar axis",
    "crumbs": [
      "Analysis",
      "Python code blocks"
    ]
  },
  {
    "objectID": "analysis/3-altair-hvplot.html",
    "href": "analysis/3-altair-hvplot.html",
    "title": "Altair and Hvplot Charts",
    "section": "",
    "text": "This page is generated from a Jupyter notebook and shows examples of embedding interactive charts produced using Altair and hvPlot.",
    "crumbs": [
      "Analysis",
      "Altair and Hvplot Charts"
    ]
  },
  {
    "objectID": "analysis/3-altair-hvplot.html#example-measles-incidence-in-altair",
    "href": "analysis/3-altair-hvplot.html#example-measles-incidence-in-altair",
    "title": "Altair and Hvplot Charts",
    "section": "Example: Measles Incidence in Altair",
    "text": "Example: Measles Incidence in Altair\nFirst, let’s load the data for measles incidence in wide format:\n\n\nCode\nurl = \"https://raw.githubusercontent.com/MUSA-550-Fall-2023/week-2/main/data/measles_incidence.csv\"\ndata = pd.read_csv(url, skiprows=2, na_values=\"-\")\n\n\n\n\n\n\n\n\n\n\n\nYEAR\nWEEK\nALABAMA\nALASKA\nARIZONA\nARKANSAS\nCALIFORNIA\nCOLORADO\nCONNECTICUT\nDELAWARE\n...\nSOUTH DAKOTA\nTENNESSEE\nTEXAS\nUTAH\nVERMONT\nVIRGINIA\nWASHINGTON\nWEST VIRGINIA\nWISCONSIN\nWYOMING\n\n\n\n\n0\n1928\n1\n3.67\nNaN\n1.90\n4.11\n1.38\n8.38\n4.50\n8.58\n...\n5.69\n22.03\n1.18\n0.4\n0.28\nNaN\n14.83\n3.36\n1.54\n0.91\n\n\n1\n1928\n2\n6.25\nNaN\n6.40\n9.91\n1.80\n6.02\n9.00\n7.30\n...\n6.57\n16.96\n0.63\nNaN\n0.56\nNaN\n17.34\n4.19\n0.96\nNaN\n\n\n2\n1928\n3\n7.95\nNaN\n4.50\n11.15\n1.31\n2.86\n8.81\n15.88\n...\n2.04\n24.66\n0.62\n0.2\n1.12\nNaN\n15.67\n4.19\n4.79\n1.36\n\n\n3\n1928\n4\n12.58\nNaN\n1.90\n13.75\n1.87\n13.71\n10.40\n4.29\n...\n2.19\n18.86\n0.37\n0.2\n6.70\nNaN\n12.77\n4.66\n1.64\n3.64\n\n\n4\n1928\n5\n8.03\nNaN\n0.47\n20.79\n2.38\n5.13\n16.80\n5.58\n...\n3.94\n20.05\n1.57\n0.4\n6.70\nNaN\n18.83\n7.37\n2.91\n0.91\n\n\n\n\n5 rows × 53 columns\n\n\n\nThen, use the pandas.melt() function to convert it to tidy format:\n\n\nCode\nannual = data.drop(\"WEEK\", axis=1)\nmeasles = annual.groupby(\"YEAR\").sum().reset_index()\nmeasles = measles.melt(id_vars=\"YEAR\", var_name=\"state\", value_name=\"incidence\")\n\n\n\n\n\n\n\n\n\n\n\nYEAR\nstate\nincidence\n\n\n\n\n0\n1928\nALABAMA\n334.99\n\n\n1\n1929\nALABAMA\n111.93\n\n\n2\n1930\nALABAMA\n157.00\n\n\n3\n1931\nALABAMA\n337.29\n\n\n4\n1932\nALABAMA\n10.21\n\n\n\n\n\n\n\nFinally, load altair:\n\nimport altair as alt\n\nAnd generate our final data viz:\n\n# use a custom color map\ncolormap = alt.Scale(\n    domain=[0, 100, 200, 300, 1000, 3000],\n    range=[\n        \"#F0F8FF\",\n        \"cornflowerblue\",\n        \"mediumseagreen\",\n        \"#FFEE00\",\n        \"darkorange\",\n        \"firebrick\",\n    ],\n    type=\"sqrt\",\n)\n\n# Vertical line for vaccination year\nthreshold = pd.DataFrame([{\"threshold\": 1963}])\n\n# plot YEAR vs state, colored by incidence\nchart = (\n    alt.Chart(measles)\n    .mark_rect()\n    .encode(\n        x=alt.X(\"YEAR:O\", axis=alt.Axis(title=None, ticks=False)),\n        y=alt.Y(\"state:N\", axis=alt.Axis(title=None, ticks=False)),\n        color=alt.Color(\"incidence:Q\", sort=\"ascending\", scale=colormap, legend=None),\n        tooltip=[\"state\", \"YEAR\", \"incidence\"],\n    )\n    .properties(width=650, height=500)\n)\n\nrule = alt.Chart(threshold).mark_rule(strokeWidth=4).encode(x=\"threshold:O\")\n\nout = chart + rule\nout",
    "crumbs": [
      "Analysis",
      "Altair and Hvplot Charts"
    ]
  },
  {
    "objectID": "analysis/3-altair-hvplot.html#example-measles-incidence-in-hvplot",
    "href": "analysis/3-altair-hvplot.html#example-measles-incidence-in-hvplot",
    "title": "Altair and Hvplot Charts",
    "section": "Example: Measles Incidence in hvplot",
    "text": "Example: Measles Incidence in hvplot\n\n\n\n\n\n\n\n\n\n\n\nGenerate the same data viz in hvplot:\n\n# Make the heatmap with hvplot\nheatmap = measles.hvplot.heatmap(\n    x=\"YEAR\",\n    y=\"state\",\n    C=\"incidence\", # color each square by the incidence\n    reduce_function=np.sum, # sum the incidence for each state/year\n    frame_height=450,\n    frame_width=600,\n    flip_yaxis=True,\n    rot=90,\n    colorbar=False,\n    cmap=\"viridis\",\n    xlabel=\"\",\n    ylabel=\"\",\n)\n\n# Some additional formatting using holoviews \n# For more info: http://holoviews.org/user_guide/Customizing_Plots.html\nheatmap = heatmap.redim(state=\"State\", YEAR=\"Year\")\nheatmap = heatmap.opts(fontsize={\"xticks\": 0, \"yticks\": 6}, toolbar=\"above\")\nheatmap",
    "crumbs": [
      "Analysis",
      "Altair and Hvplot Charts"
    ]
  },
  {
    "objectID": "analysis/index.html",
    "href": "analysis/index.html",
    "title": "Analysis",
    "section": "",
    "text": "Analysis\nIn this project, I will analyze the income changes of Philadelphia retail stores from 2019 to 2024, and forecast future trends. This is crucial for understanding the city’s economic health and guiding strategic decisions for businesses and policymakers.\nYou can access each section on the left or click the links below:\n\nBackground\nEstimated read time: 2 min\n“The retail landscape in Philadelphia has undergone significant changes between 2019 and 2024, which is caused by economic shifts, consumer behavior changes, and the long-term effects of the COVID-19 pandemic.”\n\n\nData Source\nEstimated read time: 2 min\n“In this section, I will show how to get the data for this project. I will give an example of using API to call the foot traffic data provided by Dewey.”\n\n\nExploratory Analysis\nEstimated read time: 4 min\n“In this section, I will explore the foot traffic data and the spend patterns data, and try to have more insights about the changes over time by visualizing them.”\n\n\nModeling\nEstimated read time: 8 min\n“In this section, I will construct the model using Random Forest Regressor from the scikit-learn library. There will be 4 models in total, each with different features, and I will compare the performance of each model using the median absolute error and the R-squared score.”\n\n\nConclusion and Limitation\nEstimated read time: 3 min\n“This project can inform business owner to strategize cost management, or adapt to changing consumer behaviors. However, the project has a key limitation: the dataset used does not cover all stores in Philadelphia but only a relatively big sample.”",
    "crumbs": [
      "Analysis"
    ]
  },
  {
    "objectID": "analysis/2-static-images.html",
    "href": "analysis/2-static-images.html",
    "title": "Showing static visualizations",
    "section": "",
    "text": "This page is generated from a Jupyter notebook and demonstrates how to generate static visualizations with matplotlib, pandas, and seaborn.\nStart by importing the packages we need:\nimport pandas as pd\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\nLoad the “Palmer penguins” dataset from week 2:\n# Load data on Palmer penguins\npenguins = pd.read_csv(\"https://raw.githubusercontent.com/MUSA-550-Fall-2023/week-2/main/data/penguins.csv\")\n# Show the first ten rows\npenguins.head(n=10)    \n\n\n\n\n\n\n\n\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\nyear\n\n\n\n\n0\nAdelie\nTorgersen\n39.1\n18.7\n181.0\n3750.0\nmale\n2007\n\n\n1\nAdelie\nTorgersen\n39.5\n17.4\n186.0\n3800.0\nfemale\n2007\n\n\n2\nAdelie\nTorgersen\n40.3\n18.0\n195.0\n3250.0\nfemale\n2007\n\n\n3\nAdelie\nTorgersen\nNaN\nNaN\nNaN\nNaN\nNaN\n2007\n\n\n4\nAdelie\nTorgersen\n36.7\n19.3\n193.0\n3450.0\nfemale\n2007\n\n\n5\nAdelie\nTorgersen\n39.3\n20.6\n190.0\n3650.0\nmale\n2007\n\n\n6\nAdelie\nTorgersen\n38.9\n17.8\n181.0\n3625.0\nfemale\n2007\n\n\n7\nAdelie\nTorgersen\n39.2\n19.6\n195.0\n4675.0\nmale\n2007\n\n\n8\nAdelie\nTorgersen\n34.1\n18.1\n193.0\n3475.0\nNaN\n2007\n\n\n9\nAdelie\nTorgersen\n42.0\n20.2\n190.0\n4250.0\nNaN\n2007",
    "crumbs": [
      "Analysis",
      "Showing static visualizations"
    ]
  },
  {
    "objectID": "analysis/2-static-images.html#a-simple-visualization-3-different-ways",
    "href": "analysis/2-static-images.html#a-simple-visualization-3-different-ways",
    "title": "Showing static visualizations",
    "section": "A simple visualization, 3 different ways",
    "text": "A simple visualization, 3 different ways\n\nI want to scatter flipper length vs. bill length, colored by the penguin species\n\n\nUsing matplotlib\n\n# Setup a dict to hold colors for each species\ncolor_map = {\"Adelie\": \"#1f77b4\", \"Gentoo\": \"#ff7f0e\", \"Chinstrap\": \"#D62728\"}\n\n# Initialize the figure \"fig\" and axes \"ax\"\nfig, ax = plt.subplots(figsize=(10, 6))\n\n# Group the data frame by species and loop over each group\n# NOTE: \"group\" will be the dataframe holding the data for \"species\"\nfor species, group_df in penguins.groupby(\"species\"):\n\n    # Plot flipper length vs bill length for this group\n    # Note: we are adding this plot to the existing \"ax\" object\n    ax.scatter(\n        group_df[\"flipper_length_mm\"],\n        group_df[\"bill_length_mm\"],\n        marker=\"o\",\n        label=species,\n        color=color_map[species],\n        alpha=0.75,\n        zorder=10\n    )\n\n# Plotting is done...format the axes!\n\n## Add a legend to the axes\nax.legend(loc=\"best\")\n\n## Add x-axis and y-axis labels\nax.set_xlabel(\"Flipper Length (mm)\")\nax.set_ylabel(\"Bill Length (mm)\")\n\n## Add the grid of lines\nax.grid(True);\n\n\n\n\n\n\n\n\n\n\nHow about in pandas?\nDataFrames have a built-in “plot” function that can make all of the basic type of matplotlib plots!\nFirst, we need to add a new “color” column specifying the color to use for each species type.\nUse the pd.replace() function: it use a dict to replace values in a DataFrame column.\n\n# Calculate a list of colors\ncolor_map = {\"Adelie\": \"#1f77b4\", \"Gentoo\": \"#ff7f0e\", \"Chinstrap\": \"#D62728\"}\n\n# Map species name to color \npenguins[\"color\"] = penguins[\"species\"].replace(color_map)\n\npenguins.head()\n\n\n\n\n\n\n\n\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\nyear\ncolor\n\n\n\n\n0\nAdelie\nTorgersen\n39.1\n18.7\n181.0\n3750.0\nmale\n2007\n#1f77b4\n\n\n1\nAdelie\nTorgersen\n39.5\n17.4\n186.0\n3800.0\nfemale\n2007\n#1f77b4\n\n\n2\nAdelie\nTorgersen\n40.3\n18.0\n195.0\n3250.0\nfemale\n2007\n#1f77b4\n\n\n3\nAdelie\nTorgersen\nNaN\nNaN\nNaN\nNaN\nNaN\n2007\n#1f77b4\n\n\n4\nAdelie\nTorgersen\n36.7\n19.3\n193.0\n3450.0\nfemale\n2007\n#1f77b4\n\n\n\n\n\n\n\nNow plot!\n\n# Same as before: Start by initializing the figure and axes\nfig, myAxes = plt.subplots(figsize=(10, 6))\n\n# Scatter plot two columns, colored by third\n# Use the built-in pandas plot.scatter function\npenguins.plot.scatter(\n    x=\"flipper_length_mm\",\n    y=\"bill_length_mm\",\n    c=\"color\",\n    alpha=0.75,\n    ax=myAxes, # IMPORTANT: Make sure to plot on the axes object we created already!\n    zorder=10\n)\n\n# Format the axes finally\nmyAxes.set_xlabel(\"Flipper Length (mm)\")\nmyAxes.set_ylabel(\"Bill Length (mm)\")\nmyAxes.grid(True);\n\n\n\n\n\n\n\n\nNote: no easy way to get legend added to the plot in this case…\n\n\nSeaborn: statistical data visualization\nSeaborn is designed to plot two columns colored by a third column…\n\n# Initialize the figure and axes\nfig, ax = plt.subplots(figsize=(10, 6))\n\n# style keywords as dict\ncolor_map = {\"Adelie\": \"#1f77b4\", \"Gentoo\": \"#ff7f0e\", \"Chinstrap\": \"#D62728\"}\nstyle = dict(palette=color_map, s=60, edgecolor=\"none\", alpha=0.75, zorder=10)\n\n# use the scatterplot() function\nsns.scatterplot(\n    x=\"flipper_length_mm\",  # the x column\n    y=\"bill_length_mm\",  # the y column\n    hue=\"species\",  # the third dimension (color)\n    data=penguins,  # pass in the data\n    ax=ax,  # plot on the axes object we made\n    **style  # add our style keywords\n)\n\n# Format with matplotlib commands\nax.set_xlabel(\"Flipper Length (mm)\")\nax.set_ylabel(\"Bill Length (mm)\")\nax.grid(True)\nax.legend(loc=\"best\");",
    "crumbs": [
      "Analysis",
      "Showing static visualizations"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "MUSA 5500: Final Project",
    "section": "",
    "text": "This is the website of my final project for MUSA 5500 Geospatial Data Science In Python. The repository of this project is here. Please access each part on the left, thank you!",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#welcome",
    "href": "index.html#welcome",
    "title": "MUSA 5500: Final Project",
    "section": "",
    "text": "This is the website of my final project for MUSA 5500 Geospatial Data Science In Python. The repository of this project is here. Please access each part on the left, thank you!",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "analysis/assignment-1.html",
    "href": "analysis/assignment-1.html",
    "title": "Assignment #1: The Donut Effect for Philadelphia ZIP Codes",
    "section": "",
    "text": "Frank Chen\nIn this assignment, we will practice our pandas skills and explore the “Donut Effect” within Philadelphia. The “Donut Effect” describes the following phenomenon: with more flexible working options and pandemic-driven density fears, people left urban dense cores and opted for more space in city suburbs, driving home and rental prices up in the suburbs relative to city centers.\nWe will be working with Zillow data for the Zillow Home Value Index (ZHVI) for Philadelphia ZIP codes. The goal will be to calculate home price appreciation in Philadelphia, comparing those ZIP codes in Center City (the central business district) to those not in Center City.",
    "crumbs": [
      "Analysis",
      "Assignment #1: The Donut Effect for Philadelphia ZIP Codes"
    ]
  },
  {
    "objectID": "analysis/assignment-1.html#load-the-data",
    "href": "analysis/assignment-1.html#load-the-data",
    "title": "Assignment #1: The Donut Effect for Philadelphia ZIP Codes",
    "section": "1. Load the data",
    "text": "1. Load the data\nThe relevant data file is downloaded and put in the data/ folder. Let’s load it using pandas and a relative path.\n\n# Import pandas and numpy\nimport pandas as pd\nimport numpy as np\n\n\n# Load the data from the CSV file\nzhvi_df = pd.read_csv(r\".\\data\\Zip_zhvi_uc_sfrcondo_tier_0.33_0.67_sm_sa_month.csv\")",
    "crumbs": [
      "Analysis",
      "Assignment #1: The Donut Effect for Philadelphia ZIP Codes"
    ]
  },
  {
    "objectID": "analysis/assignment-1.html#trim-the-data-to-just-philadelphia",
    "href": "analysis/assignment-1.html#trim-the-data-to-just-philadelphia",
    "title": "Assignment #1: The Donut Effect for Philadelphia ZIP Codes",
    "section": "2. Trim the data to just Philadelphia",
    "text": "2. Trim the data to just Philadelphia\nSelect the subset of the dataframe that includes only rows where the “City” column is “Philadelphia” and the “State” column is “PA”. Use the .loc() function to accomplish this.\n\n# Filter to only Philadelphia using .loc[]\nphilly_zhvi = zhvi_df.loc[(zhvi_df[\"City\"] == \"Philadelphia\") & (zhvi_df[\"State\"] == \"PA\")]",
    "crumbs": [
      "Analysis",
      "Assignment #1: The Donut Effect for Philadelphia ZIP Codes"
    ]
  },
  {
    "objectID": "analysis/assignment-1.html#melt-the-data-into-tidy-format",
    "href": "analysis/assignment-1.html#melt-the-data-into-tidy-format",
    "title": "Assignment #1: The Donut Effect for Philadelphia ZIP Codes",
    "section": "3. Melt the data into tidy format",
    "text": "3. Melt the data into tidy format\nTransform the data from wide to tidy using the pd.melt() function. Create a new column in your data called “ZHVI” that holds the ZHVI values.\nBy observing the data, we can find that the name of many columns is formatted as “year-month-date”. Thus, a function called looks_like_a_date() is created to identify such columns.\n\ndef looks_like_a_date(column_name):\n    \"\"\"A function that tests if a string starts with '20'\"\"\"\n\n    return column_name.startswith(\"20\")\n\n\n# Melt the data into tidy format\nphilly_zhvi_tidy = pd.melt(philly_zhvi, \n                           id_vars='RegionName', \n                           value_vars=list(filter(looks_like_a_date, philly_zhvi.columns)),\n                           var_name='Date', \n                           value_name='ZHVI')",
    "crumbs": [
      "Analysis",
      "Assignment #1: The Donut Effect for Philadelphia ZIP Codes"
    ]
  },
  {
    "objectID": "analysis/assignment-1.html#split-the-data-for-zip-codes-inoutside-center-city",
    "href": "analysis/assignment-1.html#split-the-data-for-zip-codes-inoutside-center-city",
    "title": "Assignment #1: The Donut Effect for Philadelphia ZIP Codes",
    "section": "4. Split the data for ZIP codes in/outside Center City",
    "text": "4. Split the data for ZIP codes in/outside Center City\nTo compare home appreciation in Center City vs. outside Center City, we’ll need to split the data into two dataframes, one that holds the Center City ZIP codes and one that holds the data for the rest of the ZIP codes in Philadelphia.\nTo help with this process, I’ve included a list of ZIP codes that make up the “greater Center City” region of Philadelphia. Use this list to split the melted data into two dataframes.\n\ngreater_center_city_zip_codes = [\n    19123,\n    19102,\n    19103,\n    19106,\n    19107,\n    19109,\n    19130,\n    19146,\n    19147,\n]\n\nTo split the data, I use .loc() and .isin() functions to filter the zip codes based on whether they are within or outside the Center City area.\n\nzhvi_center_city = philly_zhvi_tidy.loc[philly_zhvi_tidy['RegionName'].isin(greater_center_city_zip_codes)]\n\nzhvi_non_center_city = philly_zhvi_tidy.loc[~philly_zhvi_tidy['RegionName'].isin(greater_center_city_zip_codes)]",
    "crumbs": [
      "Analysis",
      "Assignment #1: The Donut Effect for Philadelphia ZIP Codes"
    ]
  },
  {
    "objectID": "analysis/assignment-1.html#compare-home-value-appreciation-in-philadelpia",
    "href": "analysis/assignment-1.html#compare-home-value-appreciation-in-philadelpia",
    "title": "Assignment #1: The Donut Effect for Philadelphia ZIP Codes",
    "section": "5. Compare home value appreciation in Philadelpia",
    "text": "5. Compare home value appreciation in Philadelpia\nIn this step, we’ll calculate the average percent increase in ZHVI from March 2020 to March 2022 for ZIP codes in/out of Center City. We’ll do this by:\n\nWriting a function (see the template below) that will calculate the percent increase in ZHVI from March 31, 2020 to March 31, 2022\nGroup your data and apply this function to calculate the ZHVI percent change for each ZIP code in Philadelphia. Do this for both of your dataframes from the previous step.\nCalculate the average value across ZIP codes for both sets of ZIP codes and then compare\n\nYou should see much larger growth for ZIP codes outside of Center City…the Donut Effect!\n\ndef calculate_percent_increase(group_df):\n    \"\"\"\n    Calculate the percent increase from 2020-03-31 to 2022-03-31.\n    \n    Note that `group_df` is the DataFrame for each group.\n    \"\"\"\n    \n    # Create selections for the March 31, 2020 to March 31, 2022\n    sel_2020 = group_df['Date'] == '2020-03-31'\n    sel_2022 = group_df['Date'] == '2022-03-31'\n\n    # Get data for each month\n    march_2020 = group_df.loc[sel_2020].squeeze()\n    march_2022 = group_df.loc[sel_2022].squeeze()\n\n    # Return the percent change for ZHVI\n    return 100 * (march_2022['ZHVI'] / march_2020['ZHVI'] - 1)\n\nUse groupby() to group the data, and then apply two datasets to the function.\n\nzhvi_center_city_grouped = zhvi_center_city.groupby('RegionName')\n\nzhvi_non_center_city_grouped = zhvi_non_center_city.groupby('RegionName')\n\n\nresult_center_city = pd.DataFrame(zhvi_center_city_grouped.apply(calculate_percent_increase), columns=['Increase Percent'])\n\nresult_center_city['Area'] = 'Center City'\n\nresult_non_center_city = pd.DataFrame(zhvi_non_center_city_grouped.apply(calculate_percent_increase), columns=['Increase Percent'])\n\nresult_non_center_city['Area'] = 'Non Center City'\n\nSince the pandemic began, Philadelphia’s Center City region has experienced significantly lower home and rent value appreciation compared to areas outside of Center City.\nIn fact, two zip codes in the Center City region saw a decrease in home values between March 31, 2020, and March 31, 2022. On average, home values in Center City increased by only 3.31%, while areas outside of Center City saw a much larger average increase of 25.02%.\nThis supports the “donut effect” theory, where higher demand in areas outside the urban core is driving up prices.\n\n# Bind the two result data frames and sort them by increase percent\nresult_all = pd.concat([result_center_city, result_non_center_city], axis=0).sort_values('Increase Percent')\n\nresult_all\n\n\n\n\n\n\n\n\nIncrease Percent\nArea\n\n\nRegionName\n\n\n\n\n\n\n19102\n-1.716711\nCenter City\n\n\n19103\n-1.696176\nCenter City\n\n\n19106\n2.520802\nCenter City\n\n\n19107\n2.883181\nCenter City\n\n\n19123\n5.212747\nCenter City\n\n\n19147\n6.139806\nCenter City\n\n\n19146\n6.480788\nCenter City\n\n\n19130\n6.673031\nCenter City\n\n\n19148\n6.963237\nNon Center City\n\n\n19145\n7.634693\nNon Center City\n\n\n19122\n10.275804\nNon Center City\n\n\n19125\n11.007135\nNon Center City\n\n\n19104\n14.539797\nNon Center City\n\n\n19129\n15.598565\nNon Center City\n\n\n19119\n17.478667\nNon Center City\n\n\n19118\n17.585001\nNon Center City\n\n\n19154\n17.930932\nNon Center City\n\n\n19150\n18.735248\nNon Center City\n\n\n19151\n19.651429\nNon Center City\n\n\n19127\n20.023926\nNon Center City\n\n\n19126\n20.819254\nNon Center City\n\n\n19114\n21.074312\nNon Center City\n\n\n19144\n21.094020\nNon Center City\n\n\n19128\n21.887555\nNon Center City\n\n\n19152\n21.993528\nNon Center City\n\n\n19115\n22.455454\nNon Center City\n\n\n19116\n23.079842\nNon Center City\n\n\n19137\n23.248505\nNon Center City\n\n\n19131\n23.363129\nNon Center City\n\n\n19134\n23.936841\nNon Center City\n\n\n19143\n23.951077\nNon Center City\n\n\n19138\n24.662626\nNon Center City\n\n\n19149\n24.916458\nNon Center City\n\n\n19121\n26.228643\nNon Center City\n\n\n19141\n26.441684\nNon Center City\n\n\n19136\n26.487833\nNon Center City\n\n\n19120\n26.927423\nNon Center City\n\n\n19135\n28.115259\nNon Center City\n\n\n19111\n28.690446\nNon Center City\n\n\n19124\n28.743474\nNon Center City\n\n\n19133\n36.143992\nNon Center City\n\n\n19139\n37.008969\nNon Center City\n\n\n19153\n38.240461\nNon Center City\n\n\n19142\n44.564396\nNon Center City\n\n\n19140\n57.150847\nNon Center City\n\n\n19132\n72.218386\nNon Center City\n\n\n\n\n\n\n\n\n# Mean value of home value increase percent in Center City zip codes\nresult_center_city['Increase Percent'].mean()\n\n3.3121833477945413\n\n\n\n# Mean value of home value increase percent in zip codes outside of Center City\nresult_non_center_city['Increase Percent'].mean()\n\n25.022864415313734",
    "crumbs": [
      "Analysis",
      "Assignment #1: The Donut Effect for Philadelphia ZIP Codes"
    ]
  },
  {
    "objectID": "analysis/assignment-1.html#another-way",
    "href": "analysis/assignment-1.html#another-way",
    "title": "Assignment #1: The Donut Effect for Philadelphia ZIP Codes",
    "section": "Another Way",
    "text": "Another Way\nThere seems to be a more straightforward way that we can select the wanted columns of 2020-03-31 and 2022-03-31, and mutate a new column to calculate the increase percentage.\nThe result is exactly the same as above.\n\n# Copy the data frame\nzhvi = philly_zhvi.copy()\n\n# Mutate a new column to distinguish zip codes in or outside of Center City region\nzhvi[\"Area\"] = np.where(zhvi['RegionName'].isin(greater_center_city_zip_codes), 'Center City', 'Non Center City')\n\n\n# These are the columns I want to keep\nkept_columns = ['RegionName', 'Area', '2020-03-31', '2022-03-31']\n\n# Iterate the data frame to contain only selected columns\nzhvi = zhvi[kept_columns]\n\n\n# Calculate the increase percent\nzhvi['Increase Percent'] = (100 * (zhvi['2022-03-31'] - zhvi['2020-03-31']) / zhvi['2020-03-31'])\n\n\n# Display the result, sort by increase percent\nresult = zhvi.sort_values('Area').sort_values('Increase Percent').reset_index(drop=True)\n\nresult\n\n\n\n\n\n\n\n\nRegionName\nArea\n2020-03-31\n2022-03-31\nIncrease Percent\n\n\n\n\n0\n19102\nCenter City\n350321.0\n344307.0\n-1.716711\n\n\n1\n19103\nCenter City\n478429.0\n470314.0\n-1.696176\n\n\n2\n19106\nCenter City\n391185.0\n401046.0\n2.520802\n\n\n3\n19107\nCenter City\n317670.0\n326829.0\n2.883181\n\n\n4\n19123\nCenter City\n414906.0\n436534.0\n5.212747\n\n\n5\n19147\nCenter City\n429867.0\n456260.0\n6.139806\n\n\n6\n19146\nCenter City\n365866.0\n389577.0\n6.480788\n\n\n7\n19130\nCenter City\n397031.0\n423525.0\n6.673031\n\n\n8\n19148\nNon Center City\n251205.0\n268697.0\n6.963237\n\n\n9\n19145\nNon Center City\n253409.0\n272756.0\n7.634693\n\n\n10\n19122\nNon Center City\n269974.0\n297716.0\n10.275804\n\n\n11\n19125\nNon Center City\n303703.0\n337132.0\n11.007135\n\n\n12\n19104\nNon Center City\n188971.0\n216447.0\n14.539797\n\n\n13\n19129\nNon Center City\n253690.0\n293262.0\n15.598565\n\n\n14\n19119\nNon Center City\n294376.0\n345829.0\n17.478667\n\n\n15\n19118\nNon Center City\n616025.0\n724353.0\n17.585001\n\n\n16\n19154\nNon Center City\n241181.0\n284427.0\n17.930932\n\n\n17\n19150\nNon Center City\n198284.0\n235433.0\n18.735248\n\n\n18\n19151\nNon Center City\n167484.0\n200397.0\n19.651429\n\n\n19\n19127\nNon Center City\n257462.0\n309016.0\n20.023926\n\n\n20\n19126\nNon Center City\n190588.0\n230267.0\n20.819254\n\n\n21\n19114\nNon Center City\n229654.0\n278052.0\n21.074312\n\n\n22\n19144\nNon Center City\n170783.0\n206808.0\n21.094020\n\n\n23\n19128\nNon Center City\n271812.0\n331305.0\n21.887555\n\n\n24\n19152\nNon Center City\n235492.0\n287285.0\n21.993528\n\n\n25\n19115\nNon Center City\n269832.0\n330424.0\n22.455454\n\n\n26\n19116\nNon Center City\n279352.0\n343826.0\n23.079842\n\n\n27\n19137\nNon Center City\n176592.0\n217647.0\n23.248505\n\n\n28\n19131\nNon Center City\n139168.0\n171682.0\n23.363129\n\n\n29\n19134\nNon Center City\n98292.0\n121820.0\n23.936841\n\n\n30\n19143\nNon Center City\n138666.0\n171878.0\n23.951077\n\n\n31\n19138\nNon Center City\n158207.0\n197225.0\n24.662626\n\n\n32\n19149\nNon Center City\n177156.0\n221297.0\n24.916458\n\n\n33\n19121\nNon Center City\n191451.0\n241666.0\n26.228643\n\n\n34\n19141\nNon Center City\n144189.0\n182315.0\n26.441684\n\n\n35\n19136\nNon Center City\n181808.0\n229965.0\n26.487833\n\n\n36\n19120\nNon Center City\n129214.0\n164008.0\n26.927423\n\n\n37\n19135\nNon Center City\n151416.0\n193987.0\n28.115259\n\n\n38\n19111\nNon Center City\n206452.0\n265684.0\n28.690446\n\n\n39\n19124\nNon Center City\n124884.0\n160780.0\n28.743474\n\n\n40\n19133\nNon Center City\n65726.0\n89482.0\n36.143992\n\n\n41\n19139\nNon Center City\n108598.0\n148789.0\n37.008969\n\n\n42\n19153\nNon Center City\n169624.0\n234489.0\n38.240461\n\n\n43\n19142\nNon Center City\n87350.0\n126277.0\n44.564396\n\n\n44\n19140\nNon Center City\n64370.0\n101158.0\n57.150847\n\n\n45\n19132\nNon Center City\n52128.0\n89774.0\n72.218386\n\n\n\n\n\n\n\n\n# Get the mean value\npd.DataFrame(result.groupby('Area')['Increase Percent'].mean())\n\n\n\n\n\n\n\n\nIncrease Percent\n\n\nArea\n\n\n\n\n\nCenter City\n3.312183\n\n\nNon Center City\n25.022864",
    "crumbs": [
      "Analysis",
      "Assignment #1: The Donut Effect for Philadelphia ZIP Codes"
    ]
  },
  {
    "objectID": "analysis/data_source.html",
    "href": "analysis/data_source.html",
    "title": "Data Sources",
    "section": "",
    "text": "In this section, I will show how to get the data for this project. I will give an example of using API to call the foot traffic data provided by Dewey. There is also an official guide on youtube that you can refer to here. For the spend pattern data, the process would be exactly the same except for the API Key and file path obtained from the website. Here are the details about the data sets:\n\nADVAN monthly foot traffic data. It includes aggregated raw counts of visits to POIs from a panel of mobile devices over a given month, detailing how often people visit, how long they stay, where they came from, where else they go, and more.\nSafeGraph monthly spend patterns data. It includes aggregated, anonymized credit and debit transaction data associated to specific stores, including median spend per day, median spend per customer, and other detailed statistics, as well as where else consumers spend money and the breakdown of online/offline spending.\n\nThe two data sets can be joined by a shared column called PlaceKey, which is the unique and persistent ID tied to a POI.\nFirst, we need to install the deweydatapy package, and then import it to use the API.\n\n\nCode\npip install deweydatapy\n\n\n\n\nCode\nimport deweydatapy as ddp\nimport pandas as pd\nimport os\nimport geopandas as gpd\n\n\nThen, we need to get the API key and the file path from the Dewey website. After that, we can use the get_meta function to have an overview of the wanted data. It suggests that the total size of the data is about 750,000 MB, and is aggregated by month from 1/1/2019 to 10/1/2024.\n\n\nCode\n# Monthly patterns api key\napikey_ = \"Your API Key\"\n\n# File path\npath = \"Your file path\"\n\n\n\n\nCode\nmeta = ddp.get_meta(apikey_, path, print_meta=True)\n\n\n \nMetadata summary ------------------------------------------------\nTotal number of files: 3,607\nTotal files size (MB): 748,164.8\nDate aggregation: MONTH\nDate partition column: DATE_RANGE_START\nData min available date: 2019-01-01\nData max available date: 2024-10-01\n-----------------------------------------------------------------\n \n\n\nThen, we can specify the date range for which we want to get the data. If we did not specify a specific date range, the API will return the whole data of a year and the file size would be too large for the computer to handle. So, we will use the start_date and end_date parameters to get the data of July from 2019 to 2024. The date format should be YYYY-MM-DD. For example, to get the data for the month of July 2021, we can set the start_date to 2021-07-01 and the end_date to 2021-07-31.\nHere we can write a for loop to get the data of each year and store them in a list, the data size is 62.5 GB in total.\n\n\nCode\nyear_var = [str(year) for year in range(19, 25)]\nfile_lists = {}\n\nfor i in year_var:\n    start_date = \"20\" + i + \"-07-01\"\n    end_date = \"20\" + i + \"-07-31\"\n\n    file_list = ddp.get_file_list(apikey_, path, start_date=start_date, end_date=end_date, print_info=True)\n\n    dataframe_name = \"file_df_\" + i\n    file_lists[dataframe_name] = pd.DataFrame(file_list)\n\n\nCollecting files information for page 1/1...\nFiles information collection completed.\n \nFiles information summary ---------------------------------------\nTotal number of pages: 1\nTotal number of files: 60\nTotal files size (MB): 12,389.41\nAverage single file size (MB): 206.49\nDate partition column: DATE_RANGE_START\nExpires at: 2024-11-15T13:58:41.429Z\n-----------------------------------------------------------------\nCollecting files information for page 1/1...\nFiles information collection completed.\n \nFiles information summary ---------------------------------------\nTotal number of pages: 1\nTotal number of files: 47\nTotal files size (MB): 9,768.66\nAverage single file size (MB): 207.84\nDate partition column: DATE_RANGE_START\nExpires at: 2024-11-15T13:58:42.285Z\n-----------------------------------------------------------------\nCollecting files information for page 1/1...\nFiles information collection completed.\n \nFiles information summary ---------------------------------------\nTotal number of pages: 1\nTotal number of files: 48\nTotal files size (MB): 10,033.68\nAverage single file size (MB): 209.03\nDate partition column: DATE_RANGE_START\nExpires at: 2024-11-15T13:58:43.121Z\n-----------------------------------------------------------------\nCollecting files information for page 1/1...\nFiles information collection completed.\n \nFiles information summary ---------------------------------------\nTotal number of pages: 1\nTotal number of files: 53\nTotal files size (MB): 11,003.53\nAverage single file size (MB): 207.61\nDate partition column: DATE_RANGE_START\nExpires at: 2024-11-15T13:58:43.995Z\n-----------------------------------------------------------------\nCollecting files information for page 1/1...\nFiles information collection completed.\n \nFiles information summary ---------------------------------------\nTotal number of pages: 1\nTotal number of files: 61\nTotal files size (MB): 12,659.24\nAverage single file size (MB): 207.53\nDate partition column: DATE_RANGE_START\nExpires at: 2024-11-15T13:58:44.852Z\n-----------------------------------------------------------------\nCollecting files information for page 1/1...\nFiles information collection completed.\n \nFiles information summary ---------------------------------------\nTotal number of pages: 1\nTotal number of files: 54\nTotal files size (MB): 11,261.46\nAverage single file size (MB): 208.55\nDate partition column: DATE_RANGE_START\nExpires at: 2024-11-15T13:58:45.741Z\n-----------------------------------------------------------------\n\n\n\n\nCode\nsize = []\n\nfor i in year_var:\n    file_df = \"file_df_\" + i\n    size.append(file_lists[file_df][\"file_size_bytes\"].sum())\n\nsum(size) / (1024 ** 3)\n\n\n62.5066275279969\n\n\nFinally, we can use the download_files function to download the data to the specified file path, and use the CITY and REGION parameters to filter the data to Philadelphia. The data will be saved in a .csv file format.\n\n\nCode\nfor i in year_var:\n    save_path = \"data/ADVAN/data_\" + i\n    file_df = \"file_df_\" + i\n    df_to_download = file_lists[file_df]\n\n    ddp.download_files(df_to_download, save_path, skip_exists=True)\n\n\n\n\nCode\n# 2019\nfolder_path = \"data/ADVAN/data_19/\"\nfiles = [file for file in os.listdir(folder_path)]\ndfs = []\n\nfor file in files:\n    file_path = os.path.join(folder_path, file)\n    df = pd.read_csv(file_path)\n    dfs.append(df)\n\ndf_final_19 = pd.concat(dfs, ignore_index=True)\n\ndf_philly_19 = df_final_19.loc[(df_final_19[\"CITY\"] == \"Philadelphia\") & (df_final_19[\"REGION\"] == \"PA\")]\n\ndf_final_19.to_csv(\"data/ADVAN/df_19.csv\", index=False)\ndf_philly_19.to_csv(\"data/ADVAN/df_philly_19.csv\", index=False)\n\n#2020\nfolder_path = \"data/ADVAN/data_20/\"\nfiles = [file for file in os.listdir(folder_path)]\ndfs = []\n\nfor file in files:\n    file_path = os.path.join(folder_path, file)\n    df = pd.read_csv(file_path)\n    dfs.append(df)\n\ndf_final_20 = pd.concat(dfs, ignore_index=True)\n\ndf_philly_20 = df_final_20.loc[(df_final_20[\"CITY\"] == \"Philadelphia\") & (df_final_20[\"REGION\"] == \"PA\")]\n\ndf_final_20.to_csv(\"data/ADVAN/df_20.csv\", index=False)\ndf_philly_20.to_csv(\"data/ADVAN/df_philly_20.csv\", index=False)\n\n#2021\nfolder_path = \"data/ADVAN/data_21/\"\nfiles = [file for file in os.listdir(folder_path)]\ndfs = []\n\nfor file in files:\n    file_path = os.path.join(folder_path, file)\n    df = pd.read_csv(file_path)\n    dfs.append(df)\n\ndf_final_21 = pd.concat(dfs, ignore_index=True)\n\ndf_philly_21 = df_final_21.loc[(df_final_21[\"CITY\"] == \"Philadelphia\") & (df_final_21[\"REGION\"] == \"PA\")]\n\ndf_final_21.to_csv(\"data/ADVAN/df_21.csv\", index=False)\ndf_philly_21.to_csv(\"data/ADVAN/df_philly_21.csv\", index=False)\n\n#2022\nfolder_path = \"data/ADVAN/data_22/\"\nfiles = [file for file in os.listdir(folder_path)]\ndfs = []\n\nfor file in files:\n    file_path = os.path.join(folder_path, file)\n    df = pd.read_csv(file_path)\n    dfs.append(df)\n\ndf_final_22 = pd.concat(dfs, ignore_index=True)\n\ndf_philly_22 = df_final_22.loc[(df_final_22[\"CITY\"] == \"Philadelphia\") & (df_final_22[\"REGION\"] == \"PA\")]\n\ndf_final_22.to_csv(\"data/ADVAN/df_22.csv\", index=False)\ndf_philly_22.to_csv(\"data/ADVAN/df_philly_22.csv\", index=False)\n\n#2023\nfolder_path = \"data/ADVAN/data_23/\"\nfiles = [file for file in os.listdir(folder_path)]\ndfs = []\n\nfor file in files:\n    file_path = os.path.join(folder_path, file)\n    df = pd.read_csv(file_path)\n    dfs.append(df)\n\ndf_final_23 = pd.concat(dfs, ignore_index=True)\n\ndf_philly_23 = df_final_23.loc[(df_final_23[\"CITY\"] == \"Philadelphia\") & (df_final_23[\"REGION\"] == \"PA\")]\n\ndf_final_23.to_csv(\"data/ADVAN/df_23.csv\", index=False)\ndf_philly_23.to_csv(\"data/ADVAN/df_philly_23.csv\", index=False)\n\n#2024\nfolder_path = \"data/ADVAN/data_24/\"\nfiles = [file for file in os.listdir(folder_path)]\ndfs = []\n\nfor file in files:\n    file_path = os.path.join(folder_path, file)\n    df = pd.read_csv(file_path)\n    dfs.append(df)\n\ndf_final_24 = pd.concat(dfs, ignore_index=True)\n\ndf_philly_24 = df_final_24.loc[(df_final_24[\"CITY\"] == \"Philadelphia\") & (df_final_24[\"REGION\"] == \"PA\")]\n\ndf_final_24.to_csv(\"data/ADVAN/df_24.csv\", index=False)\ndf_philly_24.to_csv(\"data/ADVAN/df_philly_24.csv\", index=False)\n\n\nNow, we have downloaded the foot traffic data and spend pattern data. For the sake of computer RAM capacity, we can restart the kernel and load the data in the following sections, where we will filter the data by NAICS_CODE and join the two data sets by the PlaceKey column.\n\n\nCode\n# Load the ADVAN data\nadvan_raw = pd.read_csv('data/ADVAN/df_philly_19_24.csv')\n\n# Load the SafeGraph data\nsg_raw = pd.read_csv('data/SafeGraph/df_philly_19_24.csv')\n\n\nC:\\Users\\chenf\\AppData\\Local\\Temp\\ipykernel_1524\\2411687974.py:2: DtypeWarning: Columns (17,18,19) have mixed types. Specify dtype option on import or set low_memory=False.\n  advan_raw = pd.read_csv('data/ADVAN/df_philly_19_24.csv')\n\n\n\n\nCode\nprint(f\"Advan foot traffic data has {advan_raw.shape[0]} rows and SafeGraph spend pattern data has {sg_raw.shape[0]} rows.\")\n\n\nAdvan foot traffic data has 373583 rows and SafeGraph spend pattern data has 24083 rows.\n\n\n\n\nCode\nadvan_raw['NAICS_CODE'] = advan_raw['NAICS_CODE'].astype(str)\n\nadvan = advan_raw.loc[\n    ~advan_raw['POPULARITY_BY_HOUR'].isnull()\n    ].loc[\n    advan_raw['NAICS_CODE'].str.startswith('42') |  # Wholesale Trade\n    advan_raw['NAICS_CODE'].str.startswith('44') |  # Retail Trade\n    advan_raw['NAICS_CODE'].str.startswith('45') |  # Retail Trade\n    advan_raw['NAICS_CODE'].str.startswith('72')    # Accommodation and Food Services\n]\n\nadvan_gdf = gpd.GeoDataFrame(\n    advan, \n    geometry=gpd.points_from_xy(advan['LONGITUDE'], advan['LATITUDE']),\n    crs='EPSG:4326'\n)\n\n\nFinally, we can save the joined data of each year to .geojson file for further analysis.\n\n\nCode\nadvan_sg = advan_gdf.merge(\n    sg_raw,\n    left_on=['PLACEKEY', 'DATE_RANGE_START'],\n    right_on=['PLACEKEY', 'SPEND_DATE_RANGE_START'],\n    how='left'\n)\n\n# Convert to the same CRS\nadvan_gdf = advan_gdf.to_crs(2272)\nadvan_sg = advan_sg.to_crs(2272)\n\n# Select columns\nadvan_sg = advan_sg[['PLACEKEY', 'LOCATION_NAME', 'TOP_CATEGORY', 'LATITUDE', 'LONGITUDE', 'DATE_RANGE_START', 'RAW_VISIT_COUNTS', 'RAW_VISITOR_COUNTS', 'RAW_TOTAL_SPEND', 'RAW_NUM_TRANSACTIONS', 'RAW_NUM_CUSTOMERS']]\nadvan_sg['DATE_RANGE_START'] = advan_sg['DATE_RANGE_START'].dt.year\n\nadvan_sg = gpd.GeoDataFrame(advan_sg, geometry=gpd.points_from_xy(advan_sg['LONGITUDE'], advan_sg['LATITUDE']), crs='EPSG:4326')\n\n# Save the data\nadvan_sg.to_file('data/advan_sg.geojson', driver='GeoJSON')",
    "crumbs": [
      "Analysis",
      "Data Sources"
    ]
  },
  {
    "objectID": "about.html#my-story",
    "href": "about.html#my-story",
    "title": "About Me",
    "section": "",
    "text": "Hello everyone, My name is Frank Chen, the author of this project. I’m currently enrolled in a dual degree program of Master of City Planning and Master of Urban Spatial Analytics at University of Pennsylvania’s Weitzman School of Design. Before I came to Philadelphia, I completed the 5-year undergraduate study and research at Tianjin University. I’m a data enthusiast and urban planner, and I’m passionate about using data and technology to solve urban problems and make our cities more sustainable, equitable, and resilient. I’m interested in a wide range of topics, including spatial optimization, housing, urban mobility, and public health. I have experience in data analysis, spatial analysis, and data visualization, and I currently work as a research assistant at Housing Initiative at Penn.",
    "crumbs": [
      "About Me"
    ]
  },
  {
    "objectID": "about.html#contact",
    "href": "about.html#contact",
    "title": "About Me",
    "section": "Contact",
    "text": "Contact\nIf you have any question or suggestions about my project, I am always looking forward to hearing from you!\n\nEmail: fchen9@upenn.edu\nGitHub: https://github.com/frankc9912\nLinkedIn: https://www.linkedin.com/in/fang-chen-0648812b0",
    "crumbs": [
      "About Me"
    ]
  },
  {
    "objectID": "analysis/about.html",
    "href": "analysis/about.html",
    "title": "About Me",
    "section": "",
    "text": "Hello everyone\n\n\nOn this about page, you might want to add more information about yourself, the project, or course.\n\n\nMy name is Eric Delmelle, the instructor for the course.\nYou can find more information about me on my personal website.\nThis site is an example site showing how to use Quarto for the final project for MUSA 550, during fall 2024.\nWrite something about you\n\nor about something you like"
  },
  {
    "objectID": "about1.html",
    "href": "about1.html",
    "title": "About Me",
    "section": "",
    "text": "My Story\n\n\nHello everyone, My name is Frank Chen, the author of this project. I’m currently enrolled in a dual degree program of Master of City Planning and Master of Urban Spatial Analytics at University of Pennsylvania’s Weitzman School of Design. Before I came to Philadelphia, I completed the 5-year undergraduate study and research at Tianjin University.\n\n\nI’m a data enthusiast and urban planner, and I’m passionate about using data and technology to solve urban problems and make our cities more sustainable, equitable, and resilient. I’m interested in a wide range of topics, including spatial optimization, housing, urban mobility, and public health. I have experience in data analysis, spatial analysis, and data visualization, and I currently work as a research assistant at Housing Initiative at Penn.\n\n\nContact\n\n\nIf you have any question or suggestions about my project, I am always looking forward to hearing from you!\n\n\nEmail: fchen9@upenn.edu\nGitHub: https://github.com/frankc9912\nLinkedIn: https://www.linkedin.com/in/fang-chen-0648812b0",
    "crumbs": [
      "About Me"
    ]
  },
  {
    "objectID": "about2.html",
    "href": "about2.html",
    "title": "About Me",
    "section": "",
    "text": "Hello everyone, My name is Frank Chen, the author of this project. I’m currently enrolled in a dual degree program of Master of City Planning and Master of Urban Spatial Analytics at University of Pennsylvania’s Weitzman School of Design. Before I came to Philadelphia, I completed the 5-year undergraduate study and research at Tianjin University.\n\n\nI’m a data enthusiast and urban planner, and I’m passionate about using data and technology to solve urban problems and make our cities more sustainable, equitable, and resilient. I’m interested in a wide range of topics, including spatial optimization, housing, urban mobility, and public health. I have experience in data analysis, spatial analysis, and data visualization, and I currently work as a research assistant at Housing Initiative at Penn.",
    "crumbs": [
      "About Me"
    ]
  },
  {
    "objectID": "about2.html#my-story",
    "href": "about2.html#my-story",
    "title": "About Me",
    "section": "",
    "text": "Hello everyone, My name is Frank Chen, the author of this project. I’m currently enrolled in a dual degree program of Master of City Planning and Master of Urban Spatial Analytics at University of Pennsylvania’s Weitzman School of Design. Before I came to Philadelphia, I completed the 5-year undergraduate study and research at Tianjin University.\n\n\nI’m a data enthusiast and urban planner, and I’m passionate about using data and technology to solve urban problems and make our cities more sustainable, equitable, and resilient. I’m interested in a wide range of topics, including spatial optimization, housing, urban mobility, and public health. I have experience in data analysis, spatial analysis, and data visualization, and I currently work as a research assistant at Housing Initiative at Penn.",
    "crumbs": [
      "About Me"
    ]
  },
  {
    "objectID": "about2.html#contact",
    "href": "about2.html#contact",
    "title": "About Me",
    "section": "Contact",
    "text": "Contact\nIf you have any question or suggestions about my project, I am always looking forward to hearing from you!\n\nEmail: fchen9@upenn.edu\nGitHub: https://github.com/frankc9912\nLinkedIn: https://www.linkedin.com/in/fang-chen-0648812b0",
    "crumbs": [
      "About Me"
    ]
  },
  {
    "objectID": "analysis/background.html",
    "href": "analysis/background.html",
    "title": "Background",
    "section": "",
    "text": "Background\nThe retail landscape in Philadelphia has undergone significant changes between 2019 and 2024, which is caused by economic shifts, consumer behavior changes, and the long-term effects of the COVID-19 pandemic. The pandemic caused widespread store closures and a sharp decline in foot traffic, leading to serious challenges for physical retail stores. For example, according to The Sun, a major supermarket chain closed its downtown Philadelphia location, leaving residents with fewer shopping options and highlighting the difficulties faced by urban retailers that rely on consistent foot traffic. Similarly, another report noted that a well-loved mall with stores like Walmart and Marshalls announced it would close after 35 years because fewer people were visiting and business had dropped.\n\n\n\nGiant will pull the plug on its Heirloom Market store at Strawbridge’s building in Philadelphia, Pennsylvania in late December.\n\n\nThe recovery of pedestrian activity was slow in many areas, which makes businesses in some neighborhoods continuing to struggle. However, Center City began to show signs of a turnaround by 2023. According to a report by CBRE, tourism in Philadelphia rebounded to exceed pre-pandemic levels, helping to boost foot traffic and retail demand. Additionally, the number of retailers in Center City increased by 15% compared to 2019, reflecting renewed confidence in physical retail spaces, as noted by Metro Philadelphia.\n\n\n\nReal estate demand from restaurateurs and experiential retail concepts demonstrated shifting retail trends within the Greater Philadelphia region during 2023.\n\n\nThis study will use foot traffic data from ADVAN to examine the conclusions of the reports, and use spend patterns data from SafeGraph to explore the income changes of retail stores in Phialdelphia from 2019 to 2024. Note that the spend data is aggregated by places, so it can also be seen as the income of a place.\nMeanwhile, it is important to understand income trends in Philadelphia’s retail sector and predict future income. This will offer valuable insights into the city’s economic development, and support strategic decision-making for stakeholders like the government and developers. I will use features that are related to foot traffic and spend patterns to model the income trend. Foot traffic is a critical driver of retail success, especially for businesses that depend heavily on in-person customer interactions. Reduced pedestrian activity and cautious spending can have negative effects on local economies, contributing to store closures and declining revenues.\nIn general, analyzing these trends provides valuable references for understanding broader economic patterns, including employment, real estate, and city revenues. A strong retail sector, supported by steady foot traffic and spending, can enhance neighborhood vitality and urban livability. This study aims to examine these shifts and provide insights into the factors driving Philadelphia’s retail recovery.",
    "crumbs": [
      "Analysis",
      "Background"
    ]
  },
  {
    "objectID": "analysis/get_data_ADVAN.html",
    "href": "analysis/get_data_ADVAN.html",
    "title": "ADVAN Foot Traffic Data",
    "section": "",
    "text": "# pip install deweydatapy@git+https://github.com/Dewey-Data/deweydatapy\nimport deweydatapy as ddp\nimport pandas as pd\nimport os\n# Monthly patterns api key\napikey_ = \"Your API Key\"\n\n# File path\npath = \"https://app.deweydata.io/external-api/v3/products/5acc9f39-1ca6-4535-b3ff-38f6b9baf85e/files\"\nmeta = ddp.get_meta(apikey_, path, print_meta=True)\n\n \nMetadata summary ------------------------------------------------\nTotal number of files: 3,607\nTotal files size (MB): 748,164.8\nDate aggregation: MONTH\nDate partition column: DATE_RANGE_START\nData min available date: 2019-01-01\nData max available date: 2024-10-01\n-----------------------------------------------------------------\nyear_var = [str(year) for year in range(19, 25)]\nfile_lists = {}\n\nfor i in year_var:\n    start_date = \"20\" + i + \"-07-01\"\n    end_date = \"20\" + i + \"-07-31\"\n\n    file_list = ddp.get_file_list(apikey_, path, start_date=start_date, end_date=end_date, print_info=True)\n\n    dataframe_name = \"file_df_\" + i\n    file_lists[dataframe_name] = pd.DataFrame(file_list)\n\nCollecting files information for page 1/1...\nFiles information collection completed.\n \nFiles information summary ---------------------------------------\nTotal number of pages: 1\nTotal number of files: 60\nTotal files size (MB): 12,389.41\nAverage single file size (MB): 206.49\nDate partition column: DATE_RANGE_START\nExpires at: 2024-11-15T13:58:41.429Z\n-----------------------------------------------------------------\nCollecting files information for page 1/1...\nFiles information collection completed.\n \nFiles information summary ---------------------------------------\nTotal number of pages: 1\nTotal number of files: 47\nTotal files size (MB): 9,768.66\nAverage single file size (MB): 207.84\nDate partition column: DATE_RANGE_START\nExpires at: 2024-11-15T13:58:42.285Z\n-----------------------------------------------------------------\nCollecting files information for page 1/1...\nFiles information collection completed.\n \nFiles information summary ---------------------------------------\nTotal number of pages: 1\nTotal number of files: 48\nTotal files size (MB): 10,033.68\nAverage single file size (MB): 209.03\nDate partition column: DATE_RANGE_START\nExpires at: 2024-11-15T13:58:43.121Z\n-----------------------------------------------------------------\nCollecting files information for page 1/1...\nFiles information collection completed.\n \nFiles information summary ---------------------------------------\nTotal number of pages: 1\nTotal number of files: 53\nTotal files size (MB): 11,003.53\nAverage single file size (MB): 207.61\nDate partition column: DATE_RANGE_START\nExpires at: 2024-11-15T13:58:43.995Z\n-----------------------------------------------------------------\nCollecting files information for page 1/1...\nFiles information collection completed.\n \nFiles information summary ---------------------------------------\nTotal number of pages: 1\nTotal number of files: 61\nTotal files size (MB): 12,659.24\nAverage single file size (MB): 207.53\nDate partition column: DATE_RANGE_START\nExpires at: 2024-11-15T13:58:44.852Z\n-----------------------------------------------------------------\nCollecting files information for page 1/1...\nFiles information collection completed.\n \nFiles information summary ---------------------------------------\nTotal number of pages: 1\nTotal number of files: 54\nTotal files size (MB): 11,261.46\nAverage single file size (MB): 208.55\nDate partition column: DATE_RANGE_START\nExpires at: 2024-11-15T13:58:45.741Z\n-----------------------------------------------------------------\nsize = []\n\nfor i in year_var:\n    file_df = \"file_df_\" + i\n    size.append(file_lists[file_df][\"file_size_bytes\"].sum())\n\nsum(size) / (1024 ** 3)\n\n62.5066275279969\nfor i in year_var:\n    save_path = \"data/ADVAN/data_\" + i\n    file_df = \"file_df_\" + i\n    df_to_download = file_lists[file_df]\n\n    ddp.download_files(df_to_download, save_path, skip_exists=True)\n\nDownloading 1/60 (file index = 0)\nWriting data/ADVAN/data_19/Monthly_Patterns_Foot_Traffic-0-DATE_RANGE_START-2019-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 2/60 (file index = 1)\nWriting data/ADVAN/data_19/Monthly_Patterns_Foot_Traffic-1-DATE_RANGE_START-2019-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 3/60 (file index = 2)\nWriting data/ADVAN/data_19/Monthly_Patterns_Foot_Traffic-2-DATE_RANGE_START-2019-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 4/60 (file index = 3)\nWriting data/ADVAN/data_19/Monthly_Patterns_Foot_Traffic-3-DATE_RANGE_START-2019-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 5/60 (file index = 4)\nWriting data/ADVAN/data_19/Monthly_Patterns_Foot_Traffic-4-DATE_RANGE_START-2019-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 6/60 (file index = 5)\nWriting data/ADVAN/data_19/Monthly_Patterns_Foot_Traffic-5-DATE_RANGE_START-2019-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 7/60 (file index = 6)\nWriting data/ADVAN/data_19/Monthly_Patterns_Foot_Traffic-6-DATE_RANGE_START-2019-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 8/60 (file index = 7)\nWriting data/ADVAN/data_19/Monthly_Patterns_Foot_Traffic-7-DATE_RANGE_START-2019-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 9/60 (file index = 8)\nWriting data/ADVAN/data_19/Monthly_Patterns_Foot_Traffic-8-DATE_RANGE_START-2019-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 10/60 (file index = 9)\nWriting data/ADVAN/data_19/Monthly_Patterns_Foot_Traffic-9-DATE_RANGE_START-2019-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 11/60 (file index = 10)\nWriting data/ADVAN/data_19/Monthly_Patterns_Foot_Traffic-10-DATE_RANGE_START-2019-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 12/60 (file index = 11)\nWriting data/ADVAN/data_19/Monthly_Patterns_Foot_Traffic-11-DATE_RANGE_START-2019-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 13/60 (file index = 12)\nWriting data/ADVAN/data_19/Monthly_Patterns_Foot_Traffic-12-DATE_RANGE_START-2019-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 14/60 (file index = 13)\nWriting data/ADVAN/data_19/Monthly_Patterns_Foot_Traffic-13-DATE_RANGE_START-2019-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 15/60 (file index = 14)\nWriting data/ADVAN/data_19/Monthly_Patterns_Foot_Traffic-14-DATE_RANGE_START-2019-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 16/60 (file index = 15)\nWriting data/ADVAN/data_19/Monthly_Patterns_Foot_Traffic-15-DATE_RANGE_START-2019-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 17/60 (file index = 16)\nWriting data/ADVAN/data_19/Monthly_Patterns_Foot_Traffic-16-DATE_RANGE_START-2019-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 18/60 (file index = 17)\nWriting data/ADVAN/data_19/Monthly_Patterns_Foot_Traffic-17-DATE_RANGE_START-2019-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 19/60 (file index = 18)\nWriting data/ADVAN/data_19/Monthly_Patterns_Foot_Traffic-18-DATE_RANGE_START-2019-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 20/60 (file index = 19)\nWriting data/ADVAN/data_19/Monthly_Patterns_Foot_Traffic-19-DATE_RANGE_START-2019-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 21/60 (file index = 20)\nWriting data/ADVAN/data_19/Monthly_Patterns_Foot_Traffic-20-DATE_RANGE_START-2019-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 22/60 (file index = 21)\nWriting data/ADVAN/data_19/Monthly_Patterns_Foot_Traffic-21-DATE_RANGE_START-2019-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 23/60 (file index = 22)\nWriting data/ADVAN/data_19/Monthly_Patterns_Foot_Traffic-22-DATE_RANGE_START-2019-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 24/60 (file index = 23)\nWriting data/ADVAN/data_19/Monthly_Patterns_Foot_Traffic-23-DATE_RANGE_START-2019-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 25/60 (file index = 24)\nWriting data/ADVAN/data_19/Monthly_Patterns_Foot_Traffic-24-DATE_RANGE_START-2019-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 26/60 (file index = 25)\nWriting data/ADVAN/data_19/Monthly_Patterns_Foot_Traffic-25-DATE_RANGE_START-2019-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 27/60 (file index = 26)\nWriting data/ADVAN/data_19/Monthly_Patterns_Foot_Traffic-26-DATE_RANGE_START-2019-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 28/60 (file index = 27)\nWriting data/ADVAN/data_19/Monthly_Patterns_Foot_Traffic-27-DATE_RANGE_START-2019-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 29/60 (file index = 28)\nWriting data/ADVAN/data_19/Monthly_Patterns_Foot_Traffic-28-DATE_RANGE_START-2019-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 30/60 (file index = 29)\nWriting data/ADVAN/data_19/Monthly_Patterns_Foot_Traffic-29-DATE_RANGE_START-2019-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 31/60 (file index = 30)\nWriting data/ADVAN/data_19/Monthly_Patterns_Foot_Traffic-30-DATE_RANGE_START-2019-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 32/60 (file index = 31)\nWriting data/ADVAN/data_19/Monthly_Patterns_Foot_Traffic-31-DATE_RANGE_START-2019-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 33/60 (file index = 32)\nWriting data/ADVAN/data_19/Monthly_Patterns_Foot_Traffic-32-DATE_RANGE_START-2019-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 34/60 (file index = 33)\nWriting data/ADVAN/data_19/Monthly_Patterns_Foot_Traffic-33-DATE_RANGE_START-2019-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 35/60 (file index = 34)\nWriting data/ADVAN/data_19/Monthly_Patterns_Foot_Traffic-34-DATE_RANGE_START-2019-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 36/60 (file index = 35)\nWriting data/ADVAN/data_19/Monthly_Patterns_Foot_Traffic-35-DATE_RANGE_START-2019-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 37/60 (file index = 36)\nWriting data/ADVAN/data_19/Monthly_Patterns_Foot_Traffic-36-DATE_RANGE_START-2019-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 38/60 (file index = 37)\nWriting data/ADVAN/data_19/Monthly_Patterns_Foot_Traffic-37-DATE_RANGE_START-2019-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 39/60 (file index = 38)\nWriting data/ADVAN/data_19/Monthly_Patterns_Foot_Traffic-38-DATE_RANGE_START-2019-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 40/60 (file index = 39)\nWriting data/ADVAN/data_19/Monthly_Patterns_Foot_Traffic-39-DATE_RANGE_START-2019-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 41/60 (file index = 40)\nWriting data/ADVAN/data_19/Monthly_Patterns_Foot_Traffic-40-DATE_RANGE_START-2019-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 42/60 (file index = 41)\nWriting data/ADVAN/data_19/Monthly_Patterns_Foot_Traffic-41-DATE_RANGE_START-2019-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 43/60 (file index = 42)\nWriting data/ADVAN/data_19/Monthly_Patterns_Foot_Traffic-42-DATE_RANGE_START-2019-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 44/60 (file index = 43)\nWriting data/ADVAN/data_19/Monthly_Patterns_Foot_Traffic-43-DATE_RANGE_START-2019-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 45/60 (file index = 44)\nWriting data/ADVAN/data_19/Monthly_Patterns_Foot_Traffic-44-DATE_RANGE_START-2019-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 46/60 (file index = 45)\nWriting data/ADVAN/data_19/Monthly_Patterns_Foot_Traffic-45-DATE_RANGE_START-2019-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 47/60 (file index = 46)\nWriting data/ADVAN/data_19/Monthly_Patterns_Foot_Traffic-46-DATE_RANGE_START-2019-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 48/60 (file index = 47)\nWriting data/ADVAN/data_19/Monthly_Patterns_Foot_Traffic-47-DATE_RANGE_START-2019-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 49/60 (file index = 48)\nWriting data/ADVAN/data_19/Monthly_Patterns_Foot_Traffic-48-DATE_RANGE_START-2019-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 50/60 (file index = 49)\nWriting data/ADVAN/data_19/Monthly_Patterns_Foot_Traffic-49-DATE_RANGE_START-2019-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 51/60 (file index = 50)\nWriting data/ADVAN/data_19/Monthly_Patterns_Foot_Traffic-50-DATE_RANGE_START-2019-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 52/60 (file index = 51)\nWriting data/ADVAN/data_19/Monthly_Patterns_Foot_Traffic-51-DATE_RANGE_START-2019-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 53/60 (file index = 52)\nWriting data/ADVAN/data_19/Monthly_Patterns_Foot_Traffic-52-DATE_RANGE_START-2019-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 54/60 (file index = 53)\nWriting data/ADVAN/data_19/Monthly_Patterns_Foot_Traffic-53-DATE_RANGE_START-2019-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 55/60 (file index = 54)\nWriting data/ADVAN/data_19/Monthly_Patterns_Foot_Traffic-54-DATE_RANGE_START-2019-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 56/60 (file index = 55)\nWriting data/ADVAN/data_19/Monthly_Patterns_Foot_Traffic-55-DATE_RANGE_START-2019-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 57/60 (file index = 56)\nWriting data/ADVAN/data_19/Monthly_Patterns_Foot_Traffic-56-DATE_RANGE_START-2019-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 58/60 (file index = 57)\nWriting data/ADVAN/data_19/Monthly_Patterns_Foot_Traffic-57-DATE_RANGE_START-2019-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 59/60 (file index = 58)\nWriting data/ADVAN/data_19/Monthly_Patterns_Foot_Traffic-58-DATE_RANGE_START-2019-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 60/60 (file index = 59)\nWriting data/ADVAN/data_19/Monthly_Patterns_Foot_Traffic-59-DATE_RANGE_START-2019-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 1/47 (file index = 0)\nWriting data/ADVAN/data_20/Monthly_Patterns_Foot_Traffic-0-DATE_RANGE_START-2020-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 2/47 (file index = 1)\nWriting data/ADVAN/data_20/Monthly_Patterns_Foot_Traffic-1-DATE_RANGE_START-2020-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 3/47 (file index = 2)\nWriting data/ADVAN/data_20/Monthly_Patterns_Foot_Traffic-2-DATE_RANGE_START-2020-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 4/47 (file index = 3)\nWriting data/ADVAN/data_20/Monthly_Patterns_Foot_Traffic-3-DATE_RANGE_START-2020-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 5/47 (file index = 4)\nWriting data/ADVAN/data_20/Monthly_Patterns_Foot_Traffic-4-DATE_RANGE_START-2020-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 6/47 (file index = 5)\nWriting data/ADVAN/data_20/Monthly_Patterns_Foot_Traffic-5-DATE_RANGE_START-2020-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 7/47 (file index = 6)\nWriting data/ADVAN/data_20/Monthly_Patterns_Foot_Traffic-6-DATE_RANGE_START-2020-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 8/47 (file index = 7)\nWriting data/ADVAN/data_20/Monthly_Patterns_Foot_Traffic-7-DATE_RANGE_START-2020-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 9/47 (file index = 8)\nWriting data/ADVAN/data_20/Monthly_Patterns_Foot_Traffic-8-DATE_RANGE_START-2020-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 10/47 (file index = 9)\nWriting data/ADVAN/data_20/Monthly_Patterns_Foot_Traffic-9-DATE_RANGE_START-2020-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 11/47 (file index = 10)\nWriting data/ADVAN/data_20/Monthly_Patterns_Foot_Traffic-10-DATE_RANGE_START-2020-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 12/47 (file index = 11)\nWriting data/ADVAN/data_20/Monthly_Patterns_Foot_Traffic-11-DATE_RANGE_START-2020-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 13/47 (file index = 12)\nWriting data/ADVAN/data_20/Monthly_Patterns_Foot_Traffic-12-DATE_RANGE_START-2020-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 14/47 (file index = 13)\nWriting data/ADVAN/data_20/Monthly_Patterns_Foot_Traffic-13-DATE_RANGE_START-2020-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 15/47 (file index = 14)\nWriting data/ADVAN/data_20/Monthly_Patterns_Foot_Traffic-14-DATE_RANGE_START-2020-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 16/47 (file index = 15)\nWriting data/ADVAN/data_20/Monthly_Patterns_Foot_Traffic-15-DATE_RANGE_START-2020-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 17/47 (file index = 16)\nWriting data/ADVAN/data_20/Monthly_Patterns_Foot_Traffic-16-DATE_RANGE_START-2020-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 18/47 (file index = 17)\nWriting data/ADVAN/data_20/Monthly_Patterns_Foot_Traffic-17-DATE_RANGE_START-2020-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 19/47 (file index = 18)\nWriting data/ADVAN/data_20/Monthly_Patterns_Foot_Traffic-18-DATE_RANGE_START-2020-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 20/47 (file index = 19)\nWriting data/ADVAN/data_20/Monthly_Patterns_Foot_Traffic-19-DATE_RANGE_START-2020-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 21/47 (file index = 20)\nWriting data/ADVAN/data_20/Monthly_Patterns_Foot_Traffic-20-DATE_RANGE_START-2020-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 22/47 (file index = 21)\nWriting data/ADVAN/data_20/Monthly_Patterns_Foot_Traffic-21-DATE_RANGE_START-2020-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 23/47 (file index = 22)\nWriting data/ADVAN/data_20/Monthly_Patterns_Foot_Traffic-22-DATE_RANGE_START-2020-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 24/47 (file index = 23)\nWriting data/ADVAN/data_20/Monthly_Patterns_Foot_Traffic-23-DATE_RANGE_START-2020-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 25/47 (file index = 24)\nWriting data/ADVAN/data_20/Monthly_Patterns_Foot_Traffic-24-DATE_RANGE_START-2020-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 26/47 (file index = 25)\nWriting data/ADVAN/data_20/Monthly_Patterns_Foot_Traffic-25-DATE_RANGE_START-2020-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 27/47 (file index = 26)\nWriting data/ADVAN/data_20/Monthly_Patterns_Foot_Traffic-26-DATE_RANGE_START-2020-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 28/47 (file index = 27)\nWriting data/ADVAN/data_20/Monthly_Patterns_Foot_Traffic-27-DATE_RANGE_START-2020-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 29/47 (file index = 28)\nWriting data/ADVAN/data_20/Monthly_Patterns_Foot_Traffic-28-DATE_RANGE_START-2020-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 30/47 (file index = 29)\nWriting data/ADVAN/data_20/Monthly_Patterns_Foot_Traffic-29-DATE_RANGE_START-2020-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 31/47 (file index = 30)\nWriting data/ADVAN/data_20/Monthly_Patterns_Foot_Traffic-30-DATE_RANGE_START-2020-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 32/47 (file index = 31)\nWriting data/ADVAN/data_20/Monthly_Patterns_Foot_Traffic-31-DATE_RANGE_START-2020-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 33/47 (file index = 32)\nWriting data/ADVAN/data_20/Monthly_Patterns_Foot_Traffic-32-DATE_RANGE_START-2020-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 34/47 (file index = 33)\nWriting data/ADVAN/data_20/Monthly_Patterns_Foot_Traffic-33-DATE_RANGE_START-2020-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 35/47 (file index = 34)\nWriting data/ADVAN/data_20/Monthly_Patterns_Foot_Traffic-34-DATE_RANGE_START-2020-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 36/47 (file index = 35)\nWriting data/ADVAN/data_20/Monthly_Patterns_Foot_Traffic-35-DATE_RANGE_START-2020-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 37/47 (file index = 36)\nWriting data/ADVAN/data_20/Monthly_Patterns_Foot_Traffic-36-DATE_RANGE_START-2020-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 38/47 (file index = 37)\nWriting data/ADVAN/data_20/Monthly_Patterns_Foot_Traffic-37-DATE_RANGE_START-2020-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 39/47 (file index = 38)\nWriting data/ADVAN/data_20/Monthly_Patterns_Foot_Traffic-38-DATE_RANGE_START-2020-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 40/47 (file index = 39)\nWriting data/ADVAN/data_20/Monthly_Patterns_Foot_Traffic-39-DATE_RANGE_START-2020-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 41/47 (file index = 40)\nWriting data/ADVAN/data_20/Monthly_Patterns_Foot_Traffic-40-DATE_RANGE_START-2020-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 42/47 (file index = 41)\nWriting data/ADVAN/data_20/Monthly_Patterns_Foot_Traffic-41-DATE_RANGE_START-2020-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 43/47 (file index = 42)\nWriting data/ADVAN/data_20/Monthly_Patterns_Foot_Traffic-42-DATE_RANGE_START-2020-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 44/47 (file index = 43)\nWriting data/ADVAN/data_20/Monthly_Patterns_Foot_Traffic-43-DATE_RANGE_START-2020-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 45/47 (file index = 44)\nWriting data/ADVAN/data_20/Monthly_Patterns_Foot_Traffic-44-DATE_RANGE_START-2020-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 46/47 (file index = 45)\nWriting data/ADVAN/data_20/Monthly_Patterns_Foot_Traffic-45-DATE_RANGE_START-2020-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 47/47 (file index = 46)\nWriting data/ADVAN/data_20/Monthly_Patterns_Foot_Traffic-46-DATE_RANGE_START-2020-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 1/48 (file index = 0)\nWriting data/ADVAN/data_21/Monthly_Patterns_Foot_Traffic-0-DATE_RANGE_START-2021-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 2/48 (file index = 1)\nWriting data/ADVAN/data_21/Monthly_Patterns_Foot_Traffic-1-DATE_RANGE_START-2021-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 3/48 (file index = 2)\nWriting data/ADVAN/data_21/Monthly_Patterns_Foot_Traffic-2-DATE_RANGE_START-2021-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 4/48 (file index = 3)\nWriting data/ADVAN/data_21/Monthly_Patterns_Foot_Traffic-3-DATE_RANGE_START-2021-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 5/48 (file index = 4)\nWriting data/ADVAN/data_21/Monthly_Patterns_Foot_Traffic-4-DATE_RANGE_START-2021-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 6/48 (file index = 5)\nWriting data/ADVAN/data_21/Monthly_Patterns_Foot_Traffic-5-DATE_RANGE_START-2021-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 7/48 (file index = 6)\nWriting data/ADVAN/data_21/Monthly_Patterns_Foot_Traffic-6-DATE_RANGE_START-2021-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 8/48 (file index = 7)\nWriting data/ADVAN/data_21/Monthly_Patterns_Foot_Traffic-7-DATE_RANGE_START-2021-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 9/48 (file index = 8)\nWriting data/ADVAN/data_21/Monthly_Patterns_Foot_Traffic-8-DATE_RANGE_START-2021-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 10/48 (file index = 9)\nWriting data/ADVAN/data_21/Monthly_Patterns_Foot_Traffic-9-DATE_RANGE_START-2021-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 11/48 (file index = 10)\nWriting data/ADVAN/data_21/Monthly_Patterns_Foot_Traffic-10-DATE_RANGE_START-2021-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 12/48 (file index = 11)\nWriting data/ADVAN/data_21/Monthly_Patterns_Foot_Traffic-11-DATE_RANGE_START-2021-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 13/48 (file index = 12)\nWriting data/ADVAN/data_21/Monthly_Patterns_Foot_Traffic-12-DATE_RANGE_START-2021-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 14/48 (file index = 13)\nWriting data/ADVAN/data_21/Monthly_Patterns_Foot_Traffic-13-DATE_RANGE_START-2021-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 15/48 (file index = 14)\nWriting data/ADVAN/data_21/Monthly_Patterns_Foot_Traffic-14-DATE_RANGE_START-2021-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 16/48 (file index = 15)\nWriting data/ADVAN/data_21/Monthly_Patterns_Foot_Traffic-15-DATE_RANGE_START-2021-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 17/48 (file index = 16)\nWriting data/ADVAN/data_21/Monthly_Patterns_Foot_Traffic-16-DATE_RANGE_START-2021-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 18/48 (file index = 17)\nWriting data/ADVAN/data_21/Monthly_Patterns_Foot_Traffic-17-DATE_RANGE_START-2021-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 19/48 (file index = 18)\nWriting data/ADVAN/data_21/Monthly_Patterns_Foot_Traffic-18-DATE_RANGE_START-2021-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 20/48 (file index = 19)\nWriting data/ADVAN/data_21/Monthly_Patterns_Foot_Traffic-19-DATE_RANGE_START-2021-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 21/48 (file index = 20)\nWriting data/ADVAN/data_21/Monthly_Patterns_Foot_Traffic-20-DATE_RANGE_START-2021-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 22/48 (file index = 21)\nWriting data/ADVAN/data_21/Monthly_Patterns_Foot_Traffic-21-DATE_RANGE_START-2021-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 23/48 (file index = 22)\nWriting data/ADVAN/data_21/Monthly_Patterns_Foot_Traffic-22-DATE_RANGE_START-2021-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 24/48 (file index = 23)\nWriting data/ADVAN/data_21/Monthly_Patterns_Foot_Traffic-23-DATE_RANGE_START-2021-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 25/48 (file index = 24)\nWriting data/ADVAN/data_21/Monthly_Patterns_Foot_Traffic-24-DATE_RANGE_START-2021-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 26/48 (file index = 25)\nWriting data/ADVAN/data_21/Monthly_Patterns_Foot_Traffic-25-DATE_RANGE_START-2021-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 27/48 (file index = 26)\nWriting data/ADVAN/data_21/Monthly_Patterns_Foot_Traffic-26-DATE_RANGE_START-2021-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 28/48 (file index = 27)\nWriting data/ADVAN/data_21/Monthly_Patterns_Foot_Traffic-27-DATE_RANGE_START-2021-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 29/48 (file index = 28)\nWriting data/ADVAN/data_21/Monthly_Patterns_Foot_Traffic-28-DATE_RANGE_START-2021-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 30/48 (file index = 29)\nWriting data/ADVAN/data_21/Monthly_Patterns_Foot_Traffic-29-DATE_RANGE_START-2021-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 31/48 (file index = 30)\nWriting data/ADVAN/data_21/Monthly_Patterns_Foot_Traffic-30-DATE_RANGE_START-2021-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 32/48 (file index = 31)\nWriting data/ADVAN/data_21/Monthly_Patterns_Foot_Traffic-31-DATE_RANGE_START-2021-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 33/48 (file index = 32)\nWriting data/ADVAN/data_21/Monthly_Patterns_Foot_Traffic-32-DATE_RANGE_START-2021-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 34/48 (file index = 33)\nWriting data/ADVAN/data_21/Monthly_Patterns_Foot_Traffic-33-DATE_RANGE_START-2021-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 35/48 (file index = 34)\nWriting data/ADVAN/data_21/Monthly_Patterns_Foot_Traffic-34-DATE_RANGE_START-2021-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 36/48 (file index = 35)\nWriting data/ADVAN/data_21/Monthly_Patterns_Foot_Traffic-35-DATE_RANGE_START-2021-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 37/48 (file index = 36)\nWriting data/ADVAN/data_21/Monthly_Patterns_Foot_Traffic-36-DATE_RANGE_START-2021-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 38/48 (file index = 37)\nWriting data/ADVAN/data_21/Monthly_Patterns_Foot_Traffic-37-DATE_RANGE_START-2021-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 39/48 (file index = 38)\nWriting data/ADVAN/data_21/Monthly_Patterns_Foot_Traffic-38-DATE_RANGE_START-2021-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 40/48 (file index = 39)\nWriting data/ADVAN/data_21/Monthly_Patterns_Foot_Traffic-39-DATE_RANGE_START-2021-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 41/48 (file index = 40)\nWriting data/ADVAN/data_21/Monthly_Patterns_Foot_Traffic-40-DATE_RANGE_START-2021-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 42/48 (file index = 41)\nWriting data/ADVAN/data_21/Monthly_Patterns_Foot_Traffic-41-DATE_RANGE_START-2021-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 43/48 (file index = 42)\nWriting data/ADVAN/data_21/Monthly_Patterns_Foot_Traffic-42-DATE_RANGE_START-2021-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 44/48 (file index = 43)\nWriting data/ADVAN/data_21/Monthly_Patterns_Foot_Traffic-43-DATE_RANGE_START-2021-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 45/48 (file index = 44)\nWriting data/ADVAN/data_21/Monthly_Patterns_Foot_Traffic-44-DATE_RANGE_START-2021-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 46/48 (file index = 45)\nWriting data/ADVAN/data_21/Monthly_Patterns_Foot_Traffic-45-DATE_RANGE_START-2021-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 47/48 (file index = 46)\nWriting data/ADVAN/data_21/Monthly_Patterns_Foot_Traffic-46-DATE_RANGE_START-2021-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 48/48 (file index = 47)\nWriting data/ADVAN/data_21/Monthly_Patterns_Foot_Traffic-47-DATE_RANGE_START-2021-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 1/53 (file index = 0)\nWriting data/ADVAN/data_22/Monthly_Patterns_Foot_Traffic-0-DATE_RANGE_START-2022-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 2/53 (file index = 1)\nWriting data/ADVAN/data_22/Monthly_Patterns_Foot_Traffic-1-DATE_RANGE_START-2022-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 3/53 (file index = 2)\nWriting data/ADVAN/data_22/Monthly_Patterns_Foot_Traffic-2-DATE_RANGE_START-2022-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 4/53 (file index = 3)\nWriting data/ADVAN/data_22/Monthly_Patterns_Foot_Traffic-3-DATE_RANGE_START-2022-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 5/53 (file index = 4)\nWriting data/ADVAN/data_22/Monthly_Patterns_Foot_Traffic-4-DATE_RANGE_START-2022-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 6/53 (file index = 5)\nWriting data/ADVAN/data_22/Monthly_Patterns_Foot_Traffic-5-DATE_RANGE_START-2022-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 7/53 (file index = 6)\nWriting data/ADVAN/data_22/Monthly_Patterns_Foot_Traffic-6-DATE_RANGE_START-2022-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 8/53 (file index = 7)\nWriting data/ADVAN/data_22/Monthly_Patterns_Foot_Traffic-7-DATE_RANGE_START-2022-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 9/53 (file index = 8)\nWriting data/ADVAN/data_22/Monthly_Patterns_Foot_Traffic-8-DATE_RANGE_START-2022-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 10/53 (file index = 9)\nWriting data/ADVAN/data_22/Monthly_Patterns_Foot_Traffic-9-DATE_RANGE_START-2022-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 11/53 (file index = 10)\nWriting data/ADVAN/data_22/Monthly_Patterns_Foot_Traffic-10-DATE_RANGE_START-2022-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 12/53 (file index = 11)\nWriting data/ADVAN/data_22/Monthly_Patterns_Foot_Traffic-11-DATE_RANGE_START-2022-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 13/53 (file index = 12)\nWriting data/ADVAN/data_22/Monthly_Patterns_Foot_Traffic-12-DATE_RANGE_START-2022-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 14/53 (file index = 13)\nWriting data/ADVAN/data_22/Monthly_Patterns_Foot_Traffic-13-DATE_RANGE_START-2022-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 15/53 (file index = 14)\nWriting data/ADVAN/data_22/Monthly_Patterns_Foot_Traffic-14-DATE_RANGE_START-2022-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 16/53 (file index = 15)\nWriting data/ADVAN/data_22/Monthly_Patterns_Foot_Traffic-15-DATE_RANGE_START-2022-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 17/53 (file index = 16)\nWriting data/ADVAN/data_22/Monthly_Patterns_Foot_Traffic-16-DATE_RANGE_START-2022-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 18/53 (file index = 17)\nWriting data/ADVAN/data_22/Monthly_Patterns_Foot_Traffic-17-DATE_RANGE_START-2022-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 19/53 (file index = 18)\nWriting data/ADVAN/data_22/Monthly_Patterns_Foot_Traffic-18-DATE_RANGE_START-2022-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 20/53 (file index = 19)\nWriting data/ADVAN/data_22/Monthly_Patterns_Foot_Traffic-19-DATE_RANGE_START-2022-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 21/53 (file index = 20)\nWriting data/ADVAN/data_22/Monthly_Patterns_Foot_Traffic-20-DATE_RANGE_START-2022-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 22/53 (file index = 21)\nWriting data/ADVAN/data_22/Monthly_Patterns_Foot_Traffic-21-DATE_RANGE_START-2022-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 23/53 (file index = 22)\nWriting data/ADVAN/data_22/Monthly_Patterns_Foot_Traffic-22-DATE_RANGE_START-2022-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 24/53 (file index = 23)\nWriting data/ADVAN/data_22/Monthly_Patterns_Foot_Traffic-23-DATE_RANGE_START-2022-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 25/53 (file index = 24)\nWriting data/ADVAN/data_22/Monthly_Patterns_Foot_Traffic-24-DATE_RANGE_START-2022-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 26/53 (file index = 25)\nWriting data/ADVAN/data_22/Monthly_Patterns_Foot_Traffic-25-DATE_RANGE_START-2022-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 27/53 (file index = 26)\nWriting data/ADVAN/data_22/Monthly_Patterns_Foot_Traffic-26-DATE_RANGE_START-2022-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 28/53 (file index = 27)\nWriting data/ADVAN/data_22/Monthly_Patterns_Foot_Traffic-27-DATE_RANGE_START-2022-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 29/53 (file index = 28)\nWriting data/ADVAN/data_22/Monthly_Patterns_Foot_Traffic-28-DATE_RANGE_START-2022-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 30/53 (file index = 29)\nWriting data/ADVAN/data_22/Monthly_Patterns_Foot_Traffic-29-DATE_RANGE_START-2022-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 31/53 (file index = 30)\nWriting data/ADVAN/data_22/Monthly_Patterns_Foot_Traffic-30-DATE_RANGE_START-2022-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 32/53 (file index = 31)\nWriting data/ADVAN/data_22/Monthly_Patterns_Foot_Traffic-31-DATE_RANGE_START-2022-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 33/53 (file index = 32)\nWriting data/ADVAN/data_22/Monthly_Patterns_Foot_Traffic-32-DATE_RANGE_START-2022-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 34/53 (file index = 33)\nWriting data/ADVAN/data_22/Monthly_Patterns_Foot_Traffic-33-DATE_RANGE_START-2022-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 35/53 (file index = 34)\nWriting data/ADVAN/data_22/Monthly_Patterns_Foot_Traffic-34-DATE_RANGE_START-2022-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 36/53 (file index = 35)\nWriting data/ADVAN/data_22/Monthly_Patterns_Foot_Traffic-35-DATE_RANGE_START-2022-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 37/53 (file index = 36)\nWriting data/ADVAN/data_22/Monthly_Patterns_Foot_Traffic-36-DATE_RANGE_START-2022-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 38/53 (file index = 37)\nWriting data/ADVAN/data_22/Monthly_Patterns_Foot_Traffic-37-DATE_RANGE_START-2022-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 39/53 (file index = 38)\nWriting data/ADVAN/data_22/Monthly_Patterns_Foot_Traffic-38-DATE_RANGE_START-2022-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 40/53 (file index = 39)\nWriting data/ADVAN/data_22/Monthly_Patterns_Foot_Traffic-39-DATE_RANGE_START-2022-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 41/53 (file index = 40)\nWriting data/ADVAN/data_22/Monthly_Patterns_Foot_Traffic-40-DATE_RANGE_START-2022-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 42/53 (file index = 41)\nWriting data/ADVAN/data_22/Monthly_Patterns_Foot_Traffic-41-DATE_RANGE_START-2022-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 43/53 (file index = 42)\nWriting data/ADVAN/data_22/Monthly_Patterns_Foot_Traffic-42-DATE_RANGE_START-2022-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 44/53 (file index = 43)\nWriting data/ADVAN/data_22/Monthly_Patterns_Foot_Traffic-43-DATE_RANGE_START-2022-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 45/53 (file index = 44)\nWriting data/ADVAN/data_22/Monthly_Patterns_Foot_Traffic-44-DATE_RANGE_START-2022-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 46/53 (file index = 45)\nWriting data/ADVAN/data_22/Monthly_Patterns_Foot_Traffic-45-DATE_RANGE_START-2022-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 47/53 (file index = 46)\nWriting data/ADVAN/data_22/Monthly_Patterns_Foot_Traffic-46-DATE_RANGE_START-2022-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 48/53 (file index = 47)\nWriting data/ADVAN/data_22/Monthly_Patterns_Foot_Traffic-47-DATE_RANGE_START-2022-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 49/53 (file index = 48)\nWriting data/ADVAN/data_22/Monthly_Patterns_Foot_Traffic-48-DATE_RANGE_START-2022-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 50/53 (file index = 49)\nWriting data/ADVAN/data_22/Monthly_Patterns_Foot_Traffic-49-DATE_RANGE_START-2022-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 51/53 (file index = 50)\nWriting data/ADVAN/data_22/Monthly_Patterns_Foot_Traffic-50-DATE_RANGE_START-2022-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 52/53 (file index = 51)\nWriting data/ADVAN/data_22/Monthly_Patterns_Foot_Traffic-51-DATE_RANGE_START-2022-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 53/53 (file index = 52)\nWriting data/ADVAN/data_22/Monthly_Patterns_Foot_Traffic-52-DATE_RANGE_START-2022-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 1/61 (file index = 0)\nWriting data/ADVAN/data_23/Monthly_Patterns_Foot_Traffic-0-DATE_RANGE_START-2023-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 2/61 (file index = 1)\nWriting data/ADVAN/data_23/Monthly_Patterns_Foot_Traffic-1-DATE_RANGE_START-2023-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 3/61 (file index = 2)\nWriting data/ADVAN/data_23/Monthly_Patterns_Foot_Traffic-2-DATE_RANGE_START-2023-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 4/61 (file index = 3)\nWriting data/ADVAN/data_23/Monthly_Patterns_Foot_Traffic-3-DATE_RANGE_START-2023-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 5/61 (file index = 4)\nWriting data/ADVAN/data_23/Monthly_Patterns_Foot_Traffic-4-DATE_RANGE_START-2023-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 6/61 (file index = 5)\nWriting data/ADVAN/data_23/Monthly_Patterns_Foot_Traffic-5-DATE_RANGE_START-2023-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 7/61 (file index = 6)\nWriting data/ADVAN/data_23/Monthly_Patterns_Foot_Traffic-6-DATE_RANGE_START-2023-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 8/61 (file index = 7)\nWriting data/ADVAN/data_23/Monthly_Patterns_Foot_Traffic-7-DATE_RANGE_START-2023-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 9/61 (file index = 8)\nWriting data/ADVAN/data_23/Monthly_Patterns_Foot_Traffic-8-DATE_RANGE_START-2023-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 10/61 (file index = 9)\nWriting data/ADVAN/data_23/Monthly_Patterns_Foot_Traffic-9-DATE_RANGE_START-2023-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 11/61 (file index = 10)\nWriting data/ADVAN/data_23/Monthly_Patterns_Foot_Traffic-10-DATE_RANGE_START-2023-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 12/61 (file index = 11)\nWriting data/ADVAN/data_23/Monthly_Patterns_Foot_Traffic-11-DATE_RANGE_START-2023-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 13/61 (file index = 12)\nWriting data/ADVAN/data_23/Monthly_Patterns_Foot_Traffic-12-DATE_RANGE_START-2023-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 14/61 (file index = 13)\nWriting data/ADVAN/data_23/Monthly_Patterns_Foot_Traffic-13-DATE_RANGE_START-2023-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 15/61 (file index = 14)\nWriting data/ADVAN/data_23/Monthly_Patterns_Foot_Traffic-14-DATE_RANGE_START-2023-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 16/61 (file index = 15)\nWriting data/ADVAN/data_23/Monthly_Patterns_Foot_Traffic-15-DATE_RANGE_START-2023-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 17/61 (file index = 16)\nWriting data/ADVAN/data_23/Monthly_Patterns_Foot_Traffic-16-DATE_RANGE_START-2023-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 18/61 (file index = 17)\nWriting data/ADVAN/data_23/Monthly_Patterns_Foot_Traffic-17-DATE_RANGE_START-2023-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 19/61 (file index = 18)\nWriting data/ADVAN/data_23/Monthly_Patterns_Foot_Traffic-18-DATE_RANGE_START-2023-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 20/61 (file index = 19)\nWriting data/ADVAN/data_23/Monthly_Patterns_Foot_Traffic-19-DATE_RANGE_START-2023-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 21/61 (file index = 20)\nWriting data/ADVAN/data_23/Monthly_Patterns_Foot_Traffic-20-DATE_RANGE_START-2023-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 22/61 (file index = 21)\nWriting data/ADVAN/data_23/Monthly_Patterns_Foot_Traffic-21-DATE_RANGE_START-2023-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 23/61 (file index = 22)\nWriting data/ADVAN/data_23/Monthly_Patterns_Foot_Traffic-22-DATE_RANGE_START-2023-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 24/61 (file index = 23)\nWriting data/ADVAN/data_23/Monthly_Patterns_Foot_Traffic-23-DATE_RANGE_START-2023-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 25/61 (file index = 24)\nWriting data/ADVAN/data_23/Monthly_Patterns_Foot_Traffic-24-DATE_RANGE_START-2023-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 26/61 (file index = 25)\nWriting data/ADVAN/data_23/Monthly_Patterns_Foot_Traffic-25-DATE_RANGE_START-2023-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 27/61 (file index = 26)\nWriting data/ADVAN/data_23/Monthly_Patterns_Foot_Traffic-26-DATE_RANGE_START-2023-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 28/61 (file index = 27)\nWriting data/ADVAN/data_23/Monthly_Patterns_Foot_Traffic-27-DATE_RANGE_START-2023-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 29/61 (file index = 28)\nWriting data/ADVAN/data_23/Monthly_Patterns_Foot_Traffic-28-DATE_RANGE_START-2023-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 30/61 (file index = 29)\nWriting data/ADVAN/data_23/Monthly_Patterns_Foot_Traffic-29-DATE_RANGE_START-2023-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 31/61 (file index = 30)\nWriting data/ADVAN/data_23/Monthly_Patterns_Foot_Traffic-30-DATE_RANGE_START-2023-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 32/61 (file index = 31)\nWriting data/ADVAN/data_23/Monthly_Patterns_Foot_Traffic-31-DATE_RANGE_START-2023-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 33/61 (file index = 32)\nWriting data/ADVAN/data_23/Monthly_Patterns_Foot_Traffic-32-DATE_RANGE_START-2023-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 34/61 (file index = 33)\nWriting data/ADVAN/data_23/Monthly_Patterns_Foot_Traffic-33-DATE_RANGE_START-2023-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 35/61 (file index = 34)\nWriting data/ADVAN/data_23/Monthly_Patterns_Foot_Traffic-34-DATE_RANGE_START-2023-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 36/61 (file index = 35)\nWriting data/ADVAN/data_23/Monthly_Patterns_Foot_Traffic-35-DATE_RANGE_START-2023-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 37/61 (file index = 36)\nWriting data/ADVAN/data_23/Monthly_Patterns_Foot_Traffic-36-DATE_RANGE_START-2023-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 38/61 (file index = 37)\nWriting data/ADVAN/data_23/Monthly_Patterns_Foot_Traffic-37-DATE_RANGE_START-2023-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 39/61 (file index = 38)\nWriting data/ADVAN/data_23/Monthly_Patterns_Foot_Traffic-38-DATE_RANGE_START-2023-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 40/61 (file index = 39)\nWriting data/ADVAN/data_23/Monthly_Patterns_Foot_Traffic-39-DATE_RANGE_START-2023-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 41/61 (file index = 40)\nWriting data/ADVAN/data_23/Monthly_Patterns_Foot_Traffic-40-DATE_RANGE_START-2023-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 42/61 (file index = 41)\nWriting data/ADVAN/data_23/Monthly_Patterns_Foot_Traffic-41-DATE_RANGE_START-2023-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 43/61 (file index = 42)\nWriting data/ADVAN/data_23/Monthly_Patterns_Foot_Traffic-42-DATE_RANGE_START-2023-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 44/61 (file index = 43)\nWriting data/ADVAN/data_23/Monthly_Patterns_Foot_Traffic-43-DATE_RANGE_START-2023-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 45/61 (file index = 44)\nWriting data/ADVAN/data_23/Monthly_Patterns_Foot_Traffic-44-DATE_RANGE_START-2023-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 46/61 (file index = 45)\nWriting data/ADVAN/data_23/Monthly_Patterns_Foot_Traffic-45-DATE_RANGE_START-2023-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 47/61 (file index = 46)\nWriting data/ADVAN/data_23/Monthly_Patterns_Foot_Traffic-46-DATE_RANGE_START-2023-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 48/61 (file index = 47)\nWriting data/ADVAN/data_23/Monthly_Patterns_Foot_Traffic-47-DATE_RANGE_START-2023-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 49/61 (file index = 48)\nWriting data/ADVAN/data_23/Monthly_Patterns_Foot_Traffic-48-DATE_RANGE_START-2023-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 50/61 (file index = 49)\nWriting data/ADVAN/data_23/Monthly_Patterns_Foot_Traffic-49-DATE_RANGE_START-2023-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 51/61 (file index = 50)\nWriting data/ADVAN/data_23/Monthly_Patterns_Foot_Traffic-50-DATE_RANGE_START-2023-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 52/61 (file index = 51)\nWriting data/ADVAN/data_23/Monthly_Patterns_Foot_Traffic-51-DATE_RANGE_START-2023-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 53/61 (file index = 52)\nWriting data/ADVAN/data_23/Monthly_Patterns_Foot_Traffic-52-DATE_RANGE_START-2023-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 54/61 (file index = 53)\nWriting data/ADVAN/data_23/Monthly_Patterns_Foot_Traffic-53-DATE_RANGE_START-2023-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 55/61 (file index = 54)\nWriting data/ADVAN/data_23/Monthly_Patterns_Foot_Traffic-54-DATE_RANGE_START-2023-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 56/61 (file index = 55)\nWriting data/ADVAN/data_23/Monthly_Patterns_Foot_Traffic-55-DATE_RANGE_START-2023-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 57/61 (file index = 56)\nWriting data/ADVAN/data_23/Monthly_Patterns_Foot_Traffic-56-DATE_RANGE_START-2023-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 58/61 (file index = 57)\nWriting data/ADVAN/data_23/Monthly_Patterns_Foot_Traffic-57-DATE_RANGE_START-2023-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 59/61 (file index = 58)\nWriting data/ADVAN/data_23/Monthly_Patterns_Foot_Traffic-58-DATE_RANGE_START-2023-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 60/61 (file index = 59)\nWriting data/ADVAN/data_23/Monthly_Patterns_Foot_Traffic-59-DATE_RANGE_START-2023-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 61/61 (file index = 60)\nWriting data/ADVAN/data_23/Monthly_Patterns_Foot_Traffic-60-DATE_RANGE_START-2023-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 1/54 (file index = 0)\nWriting data/ADVAN/data_24/Monthly_Patterns_Foot_Traffic-0-DATE_RANGE_START-2024-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 2/54 (file index = 1)\nWriting data/ADVAN/data_24/Monthly_Patterns_Foot_Traffic-1-DATE_RANGE_START-2024-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 3/54 (file index = 2)\nWriting data/ADVAN/data_24/Monthly_Patterns_Foot_Traffic-2-DATE_RANGE_START-2024-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 4/54 (file index = 3)\nWriting data/ADVAN/data_24/Monthly_Patterns_Foot_Traffic-3-DATE_RANGE_START-2024-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 5/54 (file index = 4)\nWriting data/ADVAN/data_24/Monthly_Patterns_Foot_Traffic-4-DATE_RANGE_START-2024-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 6/54 (file index = 5)\nWriting data/ADVAN/data_24/Monthly_Patterns_Foot_Traffic-5-DATE_RANGE_START-2024-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 7/54 (file index = 6)\nWriting data/ADVAN/data_24/Monthly_Patterns_Foot_Traffic-6-DATE_RANGE_START-2024-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 8/54 (file index = 7)\nWriting data/ADVAN/data_24/Monthly_Patterns_Foot_Traffic-7-DATE_RANGE_START-2024-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 9/54 (file index = 8)\nWriting data/ADVAN/data_24/Monthly_Patterns_Foot_Traffic-8-DATE_RANGE_START-2024-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 10/54 (file index = 9)\nWriting data/ADVAN/data_24/Monthly_Patterns_Foot_Traffic-9-DATE_RANGE_START-2024-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 11/54 (file index = 10)\nWriting data/ADVAN/data_24/Monthly_Patterns_Foot_Traffic-10-DATE_RANGE_START-2024-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 12/54 (file index = 11)\nWriting data/ADVAN/data_24/Monthly_Patterns_Foot_Traffic-11-DATE_RANGE_START-2024-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 13/54 (file index = 12)\nWriting data/ADVAN/data_24/Monthly_Patterns_Foot_Traffic-12-DATE_RANGE_START-2024-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 14/54 (file index = 13)\nWriting data/ADVAN/data_24/Monthly_Patterns_Foot_Traffic-13-DATE_RANGE_START-2024-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 15/54 (file index = 14)\nWriting data/ADVAN/data_24/Monthly_Patterns_Foot_Traffic-14-DATE_RANGE_START-2024-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 16/54 (file index = 15)\nWriting data/ADVAN/data_24/Monthly_Patterns_Foot_Traffic-15-DATE_RANGE_START-2024-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 17/54 (file index = 16)\nWriting data/ADVAN/data_24/Monthly_Patterns_Foot_Traffic-16-DATE_RANGE_START-2024-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 18/54 (file index = 17)\nWriting data/ADVAN/data_24/Monthly_Patterns_Foot_Traffic-17-DATE_RANGE_START-2024-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 19/54 (file index = 18)\nWriting data/ADVAN/data_24/Monthly_Patterns_Foot_Traffic-18-DATE_RANGE_START-2024-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 20/54 (file index = 19)\nWriting data/ADVAN/data_24/Monthly_Patterns_Foot_Traffic-19-DATE_RANGE_START-2024-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 21/54 (file index = 20)\nWriting data/ADVAN/data_24/Monthly_Patterns_Foot_Traffic-20-DATE_RANGE_START-2024-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 22/54 (file index = 21)\nWriting data/ADVAN/data_24/Monthly_Patterns_Foot_Traffic-21-DATE_RANGE_START-2024-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 23/54 (file index = 22)\nWriting data/ADVAN/data_24/Monthly_Patterns_Foot_Traffic-22-DATE_RANGE_START-2024-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 24/54 (file index = 23)\nWriting data/ADVAN/data_24/Monthly_Patterns_Foot_Traffic-23-DATE_RANGE_START-2024-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 25/54 (file index = 24)\nWriting data/ADVAN/data_24/Monthly_Patterns_Foot_Traffic-24-DATE_RANGE_START-2024-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 26/54 (file index = 25)\nWriting data/ADVAN/data_24/Monthly_Patterns_Foot_Traffic-25-DATE_RANGE_START-2024-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 27/54 (file index = 26)\nWriting data/ADVAN/data_24/Monthly_Patterns_Foot_Traffic-26-DATE_RANGE_START-2024-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 28/54 (file index = 27)\nWriting data/ADVAN/data_24/Monthly_Patterns_Foot_Traffic-27-DATE_RANGE_START-2024-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 29/54 (file index = 28)\nWriting data/ADVAN/data_24/Monthly_Patterns_Foot_Traffic-28-DATE_RANGE_START-2024-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 30/54 (file index = 29)\nWriting data/ADVAN/data_24/Monthly_Patterns_Foot_Traffic-29-DATE_RANGE_START-2024-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 31/54 (file index = 30)\nWriting data/ADVAN/data_24/Monthly_Patterns_Foot_Traffic-30-DATE_RANGE_START-2024-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 32/54 (file index = 31)\nWriting data/ADVAN/data_24/Monthly_Patterns_Foot_Traffic-31-DATE_RANGE_START-2024-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 33/54 (file index = 32)\nWriting data/ADVAN/data_24/Monthly_Patterns_Foot_Traffic-32-DATE_RANGE_START-2024-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 34/54 (file index = 33)\nWriting data/ADVAN/data_24/Monthly_Patterns_Foot_Traffic-33-DATE_RANGE_START-2024-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 35/54 (file index = 34)\nWriting data/ADVAN/data_24/Monthly_Patterns_Foot_Traffic-34-DATE_RANGE_START-2024-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 36/54 (file index = 35)\nWriting data/ADVAN/data_24/Monthly_Patterns_Foot_Traffic-35-DATE_RANGE_START-2024-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 37/54 (file index = 36)\nWriting data/ADVAN/data_24/Monthly_Patterns_Foot_Traffic-36-DATE_RANGE_START-2024-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 38/54 (file index = 37)\nWriting data/ADVAN/data_24/Monthly_Patterns_Foot_Traffic-37-DATE_RANGE_START-2024-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 39/54 (file index = 38)\nWriting data/ADVAN/data_24/Monthly_Patterns_Foot_Traffic-38-DATE_RANGE_START-2024-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 40/54 (file index = 39)\nWriting data/ADVAN/data_24/Monthly_Patterns_Foot_Traffic-39-DATE_RANGE_START-2024-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 41/54 (file index = 40)\nWriting data/ADVAN/data_24/Monthly_Patterns_Foot_Traffic-40-DATE_RANGE_START-2024-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 42/54 (file index = 41)\nWriting data/ADVAN/data_24/Monthly_Patterns_Foot_Traffic-41-DATE_RANGE_START-2024-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 43/54 (file index = 42)\nWriting data/ADVAN/data_24/Monthly_Patterns_Foot_Traffic-42-DATE_RANGE_START-2024-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 44/54 (file index = 43)\nWriting data/ADVAN/data_24/Monthly_Patterns_Foot_Traffic-43-DATE_RANGE_START-2024-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 45/54 (file index = 44)\nWriting data/ADVAN/data_24/Monthly_Patterns_Foot_Traffic-44-DATE_RANGE_START-2024-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 46/54 (file index = 45)\nWriting data/ADVAN/data_24/Monthly_Patterns_Foot_Traffic-45-DATE_RANGE_START-2024-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 47/54 (file index = 46)\nWriting data/ADVAN/data_24/Monthly_Patterns_Foot_Traffic-46-DATE_RANGE_START-2024-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 48/54 (file index = 47)\nWriting data/ADVAN/data_24/Monthly_Patterns_Foot_Traffic-47-DATE_RANGE_START-2024-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 49/54 (file index = 48)\nWriting data/ADVAN/data_24/Monthly_Patterns_Foot_Traffic-48-DATE_RANGE_START-2024-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 50/54 (file index = 49)\nWriting data/ADVAN/data_24/Monthly_Patterns_Foot_Traffic-49-DATE_RANGE_START-2024-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 51/54 (file index = 50)\nWriting data/ADVAN/data_24/Monthly_Patterns_Foot_Traffic-50-DATE_RANGE_START-2024-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 52/54 (file index = 51)\nWriting data/ADVAN/data_24/Monthly_Patterns_Foot_Traffic-51-DATE_RANGE_START-2024-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 53/54 (file index = 52)\nWriting data/ADVAN/data_24/Monthly_Patterns_Foot_Traffic-52-DATE_RANGE_START-2024-07-01.csv.gz\nPlease be patient. It may take a while...\n   \nDownloading 54/54 (file index = 53)\nWriting data/ADVAN/data_24/Monthly_Patterns_Foot_Traffic-53-DATE_RANGE_START-2024-07-01.csv.gz\nPlease be patient. It may take a while..."
  },
  {
    "objectID": "analysis/get_data_ADVAN.html#section",
    "href": "analysis/get_data_ADVAN.html#section",
    "title": "ADVAN Foot Traffic Data",
    "section": "2019",
    "text": "2019\n\n# 2019\nfolder_path = \"data/ADVAN/data_19/\"\nfiles = [file for file in os.listdir(folder_path)]\ndfs = []\n\nfor file in files:\n    file_path = os.path.join(folder_path, file)\n    df = pd.read_csv(file_path)\n    dfs.append(df)\n\ndf_final_19 = pd.concat(dfs, ignore_index=True)\n\ndf_philly_19 = df_final_19.loc[(df_final_19[\"CITY\"] == \"Philadelphia\") & (df_final_19[\"REGION\"] == \"PA\")]\n\ndf_final_19.to_csv(\"data/ADVAN/df_19.csv\", index=False)\ndf_philly_19.to_csv(\"data/ADVAN/df_philly_19.csv\", index=False)\n\nC:\\Users\\chenf\\AppData\\Local\\Temp\\ipykernel_1464\\1236377464.py:8: DtypeWarning: Columns (18) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(file_path)\nC:\\Users\\chenf\\AppData\\Local\\Temp\\ipykernel_1464\\1236377464.py:8: DtypeWarning: Columns (18) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(file_path)\nC:\\Users\\chenf\\AppData\\Local\\Temp\\ipykernel_1464\\1236377464.py:8: DtypeWarning: Columns (18) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(file_path)\nC:\\Users\\chenf\\AppData\\Local\\Temp\\ipykernel_1464\\1236377464.py:8: DtypeWarning: Columns (18) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(file_path)\nC:\\Users\\chenf\\AppData\\Local\\Temp\\ipykernel_1464\\1236377464.py:8: DtypeWarning: Columns (18) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(file_path)\nC:\\Users\\chenf\\AppData\\Local\\Temp\\ipykernel_1464\\1236377464.py:8: DtypeWarning: Columns (18) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(file_path)\nC:\\Users\\chenf\\AppData\\Local\\Temp\\ipykernel_1464\\1236377464.py:8: DtypeWarning: Columns (18) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(file_path)\nC:\\Users\\chenf\\AppData\\Local\\Temp\\ipykernel_1464\\1236377464.py:8: DtypeWarning: Columns (18) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(file_path)\nC:\\Users\\chenf\\AppData\\Local\\Temp\\ipykernel_1464\\1236377464.py:8: DtypeWarning: Columns (18) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(file_path)\nC:\\Users\\chenf\\AppData\\Local\\Temp\\ipykernel_1464\\1236377464.py:8: DtypeWarning: Columns (18) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(file_path)\nC:\\Users\\chenf\\AppData\\Local\\Temp\\ipykernel_1464\\1236377464.py:8: DtypeWarning: Columns (18) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(file_path)\nC:\\Users\\chenf\\AppData\\Local\\Temp\\ipykernel_1464\\1236377464.py:8: DtypeWarning: Columns (18) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(file_path)\nC:\\Users\\chenf\\AppData\\Local\\Temp\\ipykernel_1464\\1236377464.py:8: DtypeWarning: Columns (18) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(file_path)\nC:\\Users\\chenf\\AppData\\Local\\Temp\\ipykernel_1464\\1236377464.py:8: DtypeWarning: Columns (18) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(file_path)\nC:\\Users\\chenf\\AppData\\Local\\Temp\\ipykernel_1464\\1236377464.py:8: DtypeWarning: Columns (18) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(file_path)\nC:\\Users\\chenf\\AppData\\Local\\Temp\\ipykernel_1464\\1236377464.py:8: DtypeWarning: Columns (18) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(file_path)\nC:\\Users\\chenf\\AppData\\Local\\Temp\\ipykernel_1464\\1236377464.py:8: DtypeWarning: Columns (18) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(file_path)\nC:\\Users\\chenf\\AppData\\Local\\Temp\\ipykernel_1464\\1236377464.py:8: DtypeWarning: Columns (18) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(file_path)\nC:\\Users\\chenf\\AppData\\Local\\Temp\\ipykernel_1464\\1236377464.py:8: DtypeWarning: Columns (18) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(file_path)\nC:\\Users\\chenf\\AppData\\Local\\Temp\\ipykernel_1464\\1236377464.py:8: DtypeWarning: Columns (18) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(file_path)\nC:\\Users\\chenf\\AppData\\Local\\Temp\\ipykernel_1464\\1236377464.py:8: DtypeWarning: Columns (18) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(file_path)\nC:\\Users\\chenf\\AppData\\Local\\Temp\\ipykernel_1464\\1236377464.py:8: DtypeWarning: Columns (18) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(file_path)\nC:\\Users\\chenf\\AppData\\Local\\Temp\\ipykernel_1464\\1236377464.py:8: DtypeWarning: Columns (18) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(file_path)\nC:\\Users\\chenf\\AppData\\Local\\Temp\\ipykernel_1464\\1236377464.py:8: DtypeWarning: Columns (18) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(file_path)\nC:\\Users\\chenf\\AppData\\Local\\Temp\\ipykernel_1464\\1236377464.py:8: DtypeWarning: Columns (18) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(file_path)\nC:\\Users\\chenf\\AppData\\Local\\Temp\\ipykernel_1464\\1236377464.py:8: DtypeWarning: Columns (18) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(file_path)\nC:\\Users\\chenf\\AppData\\Local\\Temp\\ipykernel_1464\\1236377464.py:8: DtypeWarning: Columns (2,4,17,18) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(file_path)\nC:\\Users\\chenf\\AppData\\Local\\Temp\\ipykernel_1464\\1236377464.py:8: DtypeWarning: Columns (18) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(file_path)\nC:\\Users\\chenf\\AppData\\Local\\Temp\\ipykernel_1464\\1236377464.py:8: DtypeWarning: Columns (18) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(file_path)\nC:\\Users\\chenf\\AppData\\Local\\Temp\\ipykernel_1464\\1236377464.py:8: DtypeWarning: Columns (18) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(file_path)\nC:\\Users\\chenf\\AppData\\Local\\Temp\\ipykernel_1464\\1236377464.py:8: DtypeWarning: Columns (18) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(file_path)\nC:\\Users\\chenf\\AppData\\Local\\Temp\\ipykernel_1464\\1236377464.py:8: DtypeWarning: Columns (18) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(file_path)\nC:\\Users\\chenf\\AppData\\Local\\Temp\\ipykernel_1464\\1236377464.py:8: DtypeWarning: Columns (18) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(file_path)\nC:\\Users\\chenf\\AppData\\Local\\Temp\\ipykernel_1464\\1236377464.py:8: DtypeWarning: Columns (18) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(file_path)\nC:\\Users\\chenf\\AppData\\Local\\Temp\\ipykernel_1464\\1236377464.py:8: DtypeWarning: Columns (18) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(file_path)\nC:\\Users\\chenf\\AppData\\Local\\Temp\\ipykernel_1464\\1236377464.py:8: DtypeWarning: Columns (18,34,36,37,38,39,42,43,44,45,46,47) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(file_path)\nC:\\Users\\chenf\\AppData\\Local\\Temp\\ipykernel_1464\\1236377464.py:8: DtypeWarning: Columns (18) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(file_path)\nC:\\Users\\chenf\\AppData\\Local\\Temp\\ipykernel_1464\\1236377464.py:8: DtypeWarning: Columns (18) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(file_path)\nC:\\Users\\chenf\\AppData\\Local\\Temp\\ipykernel_1464\\1236377464.py:8: DtypeWarning: Columns (18) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(file_path)\nC:\\Users\\chenf\\AppData\\Local\\Temp\\ipykernel_1464\\1236377464.py:8: DtypeWarning: Columns (18) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(file_path)\nC:\\Users\\chenf\\AppData\\Local\\Temp\\ipykernel_1464\\1236377464.py:8: DtypeWarning: Columns (18) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(file_path)\nC:\\Users\\chenf\\AppData\\Local\\Temp\\ipykernel_1464\\1236377464.py:8: DtypeWarning: Columns (18) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(file_path)\nC:\\Users\\chenf\\AppData\\Local\\Temp\\ipykernel_1464\\1236377464.py:8: DtypeWarning: Columns (18) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(file_path)\nC:\\Users\\chenf\\AppData\\Local\\Temp\\ipykernel_1464\\1236377464.py:8: DtypeWarning: Columns (18) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(file_path)\nC:\\Users\\chenf\\AppData\\Local\\Temp\\ipykernel_1464\\1236377464.py:8: DtypeWarning: Columns (18) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(file_path)\nC:\\Users\\chenf\\AppData\\Local\\Temp\\ipykernel_1464\\1236377464.py:8: DtypeWarning: Columns (18) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(file_path)\nC:\\Users\\chenf\\AppData\\Local\\Temp\\ipykernel_1464\\1236377464.py:8: DtypeWarning: Columns (18) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(file_path)\nC:\\Users\\chenf\\AppData\\Local\\Temp\\ipykernel_1464\\1236377464.py:8: DtypeWarning: Columns (18) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(file_path)\nC:\\Users\\chenf\\AppData\\Local\\Temp\\ipykernel_1464\\1236377464.py:8: DtypeWarning: Columns (18) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(file_path)\nC:\\Users\\chenf\\AppData\\Local\\Temp\\ipykernel_1464\\1236377464.py:8: DtypeWarning: Columns (18) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(file_path)\nC:\\Users\\chenf\\AppData\\Local\\Temp\\ipykernel_1464\\1236377464.py:8: DtypeWarning: Columns (18) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(file_path)\nC:\\Users\\chenf\\AppData\\Local\\Temp\\ipykernel_1464\\1236377464.py:8: DtypeWarning: Columns (18) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(file_path)\nC:\\Users\\chenf\\AppData\\Local\\Temp\\ipykernel_1464\\1236377464.py:8: DtypeWarning: Columns (18) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(file_path)\nC:\\Users\\chenf\\AppData\\Local\\Temp\\ipykernel_1464\\1236377464.py:8: DtypeWarning: Columns (18) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(file_path)\nC:\\Users\\chenf\\AppData\\Local\\Temp\\ipykernel_1464\\1236377464.py:8: DtypeWarning: Columns (18) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(file_path)\nC:\\Users\\chenf\\AppData\\Local\\Temp\\ipykernel_1464\\1236377464.py:8: DtypeWarning: Columns (18) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(file_path)\nC:\\Users\\chenf\\AppData\\Local\\Temp\\ipykernel_1464\\1236377464.py:8: DtypeWarning: Columns (18) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(file_path)\nC:\\Users\\chenf\\AppData\\Local\\Temp\\ipykernel_1464\\1236377464.py:8: DtypeWarning: Columns (18) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(file_path)\nC:\\Users\\chenf\\AppData\\Local\\Temp\\ipykernel_1464\\1236377464.py:8: DtypeWarning: Columns (18) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(file_path)"
  },
  {
    "objectID": "analysis/get_data_ADVAN.html#section-1",
    "href": "analysis/get_data_ADVAN.html#section-1",
    "title": "ADVAN Foot Traffic Data",
    "section": "2020",
    "text": "2020\n\n#2020\nfolder_path = \"data/ADVAN/data_20/\"\nfiles = [file for file in os.listdir(folder_path)]\ndfs = []\n\nfor file in files:\n    file_path = os.path.join(folder_path, file)\n    df = pd.read_csv(file_path)\n    dfs.append(df)\n\ndf_final_20 = pd.concat(dfs, ignore_index=True)\n\ndf_philly_20 = df_final_20.loc[(df_final_20[\"CITY\"] == \"Philadelphia\") & (df_final_20[\"REGION\"] == \"PA\")]\n\ndf_final_20.to_csv(\"data/ADVAN/df_20.csv\", index=False)\ndf_philly_20.to_csv(\"data/ADVAN/df_philly_20.csv\", index=False)\n\nC:\\Users\\chenf\\AppData\\Local\\Temp\\ipykernel_5080\\1814135670.py:8: DtypeWarning: Columns (18,34,36,37,38,39,42,43,44,45,46,47) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(file_path)\nC:\\Users\\chenf\\AppData\\Local\\Temp\\ipykernel_5080\\1814135670.py:8: DtypeWarning: Columns (18) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(file_path)\nC:\\Users\\chenf\\AppData\\Local\\Temp\\ipykernel_5080\\1814135670.py:8: DtypeWarning: Columns (18) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(file_path)\nC:\\Users\\chenf\\AppData\\Local\\Temp\\ipykernel_5080\\1814135670.py:8: DtypeWarning: Columns (18) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(file_path)\nC:\\Users\\chenf\\AppData\\Local\\Temp\\ipykernel_5080\\1814135670.py:8: DtypeWarning: Columns (18) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(file_path)\nC:\\Users\\chenf\\AppData\\Local\\Temp\\ipykernel_5080\\1814135670.py:8: DtypeWarning: Columns (18,34,36,37,38,39,42,43,44,45,46,47) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(file_path)\nC:\\Users\\chenf\\AppData\\Local\\Temp\\ipykernel_5080\\1814135670.py:8: DtypeWarning: Columns (18) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(file_path)\nC:\\Users\\chenf\\AppData\\Local\\Temp\\ipykernel_5080\\1814135670.py:8: DtypeWarning: Columns (18) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(file_path)\nC:\\Users\\chenf\\AppData\\Local\\Temp\\ipykernel_5080\\1814135670.py:8: DtypeWarning: Columns (18) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(file_path)\nC:\\Users\\chenf\\AppData\\Local\\Temp\\ipykernel_5080\\1814135670.py:8: DtypeWarning: Columns (18) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(file_path)\nC:\\Users\\chenf\\AppData\\Local\\Temp\\ipykernel_5080\\1814135670.py:8: DtypeWarning: Columns (18) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(file_path)\nC:\\Users\\chenf\\AppData\\Local\\Temp\\ipykernel_5080\\1814135670.py:8: DtypeWarning: Columns (18) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(file_path)\nC:\\Users\\chenf\\AppData\\Local\\Temp\\ipykernel_5080\\1814135670.py:8: DtypeWarning: Columns (18) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(file_path)\nC:\\Users\\chenf\\AppData\\Local\\Temp\\ipykernel_5080\\1814135670.py:8: DtypeWarning: Columns (18) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(file_path)\nC:\\Users\\chenf\\AppData\\Local\\Temp\\ipykernel_5080\\1814135670.py:8: DtypeWarning: Columns (18) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(file_path)\nC:\\Users\\chenf\\AppData\\Local\\Temp\\ipykernel_5080\\1814135670.py:8: DtypeWarning: Columns (18) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(file_path)\nC:\\Users\\chenf\\AppData\\Local\\Temp\\ipykernel_5080\\1814135670.py:8: DtypeWarning: Columns (18) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(file_path)\nC:\\Users\\chenf\\AppData\\Local\\Temp\\ipykernel_5080\\1814135670.py:8: DtypeWarning: Columns (18,34,36,37,38,39,42,43,44,45,46,47) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(file_path)\nC:\\Users\\chenf\\AppData\\Local\\Temp\\ipykernel_5080\\1814135670.py:8: DtypeWarning: Columns (18) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(file_path)\nC:\\Users\\chenf\\AppData\\Local\\Temp\\ipykernel_5080\\1814135670.py:8: DtypeWarning: Columns (18) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(file_path)\nC:\\Users\\chenf\\AppData\\Local\\Temp\\ipykernel_5080\\1814135670.py:8: DtypeWarning: Columns (18) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(file_path)\nC:\\Users\\chenf\\AppData\\Local\\Temp\\ipykernel_5080\\1814135670.py:8: DtypeWarning: Columns (18) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(file_path)\nC:\\Users\\chenf\\AppData\\Local\\Temp\\ipykernel_5080\\1814135670.py:8: DtypeWarning: Columns (18,34,36,37,38,39,42,43,44,45,46,47) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(file_path)\nC:\\Users\\chenf\\AppData\\Local\\Temp\\ipykernel_5080\\1814135670.py:8: DtypeWarning: Columns (18,34,36,37,38,39,42,43,44,45,46,47) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(file_path)\nC:\\Users\\chenf\\AppData\\Local\\Temp\\ipykernel_5080\\1814135670.py:8: DtypeWarning: Columns (18) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(file_path)\nC:\\Users\\chenf\\AppData\\Local\\Temp\\ipykernel_5080\\1814135670.py:8: DtypeWarning: Columns (18) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(file_path)\nC:\\Users\\chenf\\AppData\\Local\\Temp\\ipykernel_5080\\1814135670.py:8: DtypeWarning: Columns (18) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(file_path)\nC:\\Users\\chenf\\AppData\\Local\\Temp\\ipykernel_5080\\1814135670.py:8: DtypeWarning: Columns (18) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(file_path)\nC:\\Users\\chenf\\AppData\\Local\\Temp\\ipykernel_5080\\1814135670.py:8: DtypeWarning: Columns (18) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(file_path)\nC:\\Users\\chenf\\AppData\\Local\\Temp\\ipykernel_5080\\1814135670.py:8: DtypeWarning: Columns (18) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(file_path)\nC:\\Users\\chenf\\AppData\\Local\\Temp\\ipykernel_5080\\1814135670.py:8: DtypeWarning: Columns (18) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(file_path)\nC:\\Users\\chenf\\AppData\\Local\\Temp\\ipykernel_5080\\1814135670.py:8: DtypeWarning: Columns (18) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(file_path)\nC:\\Users\\chenf\\AppData\\Local\\Temp\\ipykernel_5080\\1814135670.py:8: DtypeWarning: Columns (18,36,37,38,39,47) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(file_path)\nC:\\Users\\chenf\\AppData\\Local\\Temp\\ipykernel_5080\\1814135670.py:8: DtypeWarning: Columns (18) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(file_path)\nC:\\Users\\chenf\\AppData\\Local\\Temp\\ipykernel_5080\\1814135670.py:8: DtypeWarning: Columns (18) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(file_path)\nC:\\Users\\chenf\\AppData\\Local\\Temp\\ipykernel_5080\\1814135670.py:8: DtypeWarning: Columns (18,34,36,37,38,39,42,43,44,45,46,47) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(file_path)\nC:\\Users\\chenf\\AppData\\Local\\Temp\\ipykernel_5080\\1814135670.py:8: DtypeWarning: Columns (18) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(file_path)\nC:\\Users\\chenf\\AppData\\Local\\Temp\\ipykernel_5080\\1814135670.py:8: DtypeWarning: Columns (18) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(file_path)\nC:\\Users\\chenf\\AppData\\Local\\Temp\\ipykernel_5080\\1814135670.py:8: DtypeWarning: Columns (18) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(file_path)\nC:\\Users\\chenf\\AppData\\Local\\Temp\\ipykernel_5080\\1814135670.py:8: DtypeWarning: Columns (18,36,37,38,39,47) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(file_path)\nC:\\Users\\chenf\\AppData\\Local\\Temp\\ipykernel_5080\\1814135670.py:8: DtypeWarning: Columns (18) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(file_path)\nC:\\Users\\chenf\\AppData\\Local\\Temp\\ipykernel_5080\\1814135670.py:8: DtypeWarning: Columns (18) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(file_path)\nC:\\Users\\chenf\\AppData\\Local\\Temp\\ipykernel_5080\\1814135670.py:8: DtypeWarning: Columns (18) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(file_path)\nC:\\Users\\chenf\\AppData\\Local\\Temp\\ipykernel_5080\\1814135670.py:8: DtypeWarning: Columns (18) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(file_path)\nC:\\Users\\chenf\\AppData\\Local\\Temp\\ipykernel_5080\\1814135670.py:8: DtypeWarning: Columns (18) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(file_path)\nC:\\Users\\chenf\\AppData\\Local\\Temp\\ipykernel_5080\\1814135670.py:8: DtypeWarning: Columns (18) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(file_path)\nC:\\Users\\chenf\\AppData\\Local\\Temp\\ipykernel_5080\\1814135670.py:8: DtypeWarning: Columns (18) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(file_path)"
  },
  {
    "objectID": "analysis/get_data_ADVAN.html#section-2",
    "href": "analysis/get_data_ADVAN.html#section-2",
    "title": "ADVAN Foot Traffic Data",
    "section": "2021",
    "text": "2021\n\n#2021\nfolder_path = \"data/ADVAN/data_21/\"\nfiles = [file for file in os.listdir(folder_path)]\ndfs = []\n\nfor file in files:\n    file_path = os.path.join(folder_path, file)\n    df = pd.read_csv(file_path)\n    dfs.append(df)\n\ndf_final_21 = pd.concat(dfs, ignore_index=True)\n\ndf_philly_21 = df_final_21.loc[(df_final_21[\"CITY\"] == \"Philadelphia\") & (df_final_21[\"REGION\"] == \"PA\")]\n\ndf_final_21.to_csv(\"data/ADVAN/df_21.csv\", index=False)\ndf_philly_21.to_csv(\"data/ADVAN/df_philly_21.csv\", index=False)\n\nC:\\Users\\chenf\\AppData\\Local\\Temp\\ipykernel_21288\\1648787826.py:8: DtypeWarning: Columns (18,34,36,37,38,39,42,43,44,45,46,47) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(file_path)\nC:\\Users\\chenf\\AppData\\Local\\Temp\\ipykernel_21288\\1648787826.py:8: DtypeWarning: Columns (18) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(file_path)\nC:\\Users\\chenf\\AppData\\Local\\Temp\\ipykernel_21288\\1648787826.py:8: DtypeWarning: Columns (18,36,37,38,39,47) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(file_path)\nC:\\Users\\chenf\\AppData\\Local\\Temp\\ipykernel_21288\\1648787826.py:8: DtypeWarning: Columns (18) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(file_path)\nC:\\Users\\chenf\\AppData\\Local\\Temp\\ipykernel_21288\\1648787826.py:8: DtypeWarning: Columns (18,34,36,37,38,39,42,43,44,45,46,47) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(file_path)\nC:\\Users\\chenf\\AppData\\Local\\Temp\\ipykernel_21288\\1648787826.py:8: DtypeWarning: Columns (18) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(file_path)\nC:\\Users\\chenf\\AppData\\Local\\Temp\\ipykernel_21288\\1648787826.py:8: DtypeWarning: Columns (18) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(file_path)\nC:\\Users\\chenf\\AppData\\Local\\Temp\\ipykernel_21288\\1648787826.py:8: DtypeWarning: Columns (18) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(file_path)\nC:\\Users\\chenf\\AppData\\Local\\Temp\\ipykernel_21288\\1648787826.py:8: DtypeWarning: Columns (18,36,37,38,39,47) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(file_path)\nC:\\Users\\chenf\\AppData\\Local\\Temp\\ipykernel_21288\\1648787826.py:8: DtypeWarning: Columns (18) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(file_path)\nC:\\Users\\chenf\\AppData\\Local\\Temp\\ipykernel_21288\\1648787826.py:8: DtypeWarning: Columns (18) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(file_path)\nC:\\Users\\chenf\\AppData\\Local\\Temp\\ipykernel_21288\\1648787826.py:8: DtypeWarning: Columns (18) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(file_path)\nC:\\Users\\chenf\\AppData\\Local\\Temp\\ipykernel_21288\\1648787826.py:8: DtypeWarning: Columns (18) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(file_path)\nC:\\Users\\chenf\\AppData\\Local\\Temp\\ipykernel_21288\\1648787826.py:8: DtypeWarning: Columns (18) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(file_path)\nC:\\Users\\chenf\\AppData\\Local\\Temp\\ipykernel_21288\\1648787826.py:8: DtypeWarning: Columns (18) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(file_path)\nC:\\Users\\chenf\\AppData\\Local\\Temp\\ipykernel_21288\\1648787826.py:8: DtypeWarning: Columns (18) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(file_path)\nC:\\Users\\chenf\\AppData\\Local\\Temp\\ipykernel_21288\\1648787826.py:8: DtypeWarning: Columns (18) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(file_path)\nC:\\Users\\chenf\\AppData\\Local\\Temp\\ipykernel_21288\\1648787826.py:8: DtypeWarning: Columns (18) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(file_path)\nC:\\Users\\chenf\\AppData\\Local\\Temp\\ipykernel_21288\\1648787826.py:8: DtypeWarning: Columns (18) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(file_path)\nC:\\Users\\chenf\\AppData\\Local\\Temp\\ipykernel_21288\\1648787826.py:8: DtypeWarning: Columns (18) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(file_path)\nC:\\Users\\chenf\\AppData\\Local\\Temp\\ipykernel_21288\\1648787826.py:8: DtypeWarning: Columns (18) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(file_path)\nC:\\Users\\chenf\\AppData\\Local\\Temp\\ipykernel_21288\\1648787826.py:8: DtypeWarning: Columns (18) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(file_path)\nC:\\Users\\chenf\\AppData\\Local\\Temp\\ipykernel_21288\\1648787826.py:8: DtypeWarning: Columns (18) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(file_path)\nC:\\Users\\chenf\\AppData\\Local\\Temp\\ipykernel_21288\\1648787826.py:8: DtypeWarning: Columns (18,34,36,37,38,39,42,43,44,45,46,47) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(file_path)\nC:\\Users\\chenf\\AppData\\Local\\Temp\\ipykernel_21288\\1648787826.py:8: DtypeWarning: Columns (18) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(file_path)\nC:\\Users\\chenf\\AppData\\Local\\Temp\\ipykernel_21288\\1648787826.py:8: DtypeWarning: Columns (18) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(file_path)\nC:\\Users\\chenf\\AppData\\Local\\Temp\\ipykernel_21288\\1648787826.py:8: DtypeWarning: Columns (1,18,34,36,37,38,39,42,43,44,45,46,47) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(file_path)\nC:\\Users\\chenf\\AppData\\Local\\Temp\\ipykernel_21288\\1648787826.py:8: DtypeWarning: Columns (18) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(file_path)\nC:\\Users\\chenf\\AppData\\Local\\Temp\\ipykernel_21288\\1648787826.py:8: DtypeWarning: Columns (18) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(file_path)\nC:\\Users\\chenf\\AppData\\Local\\Temp\\ipykernel_21288\\1648787826.py:8: DtypeWarning: Columns (18) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(file_path)\nC:\\Users\\chenf\\AppData\\Local\\Temp\\ipykernel_21288\\1648787826.py:8: DtypeWarning: Columns (18) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(file_path)\nC:\\Users\\chenf\\AppData\\Local\\Temp\\ipykernel_21288\\1648787826.py:8: DtypeWarning: Columns (18,34,36,37,38,39,42,43,44,45,46,47) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(file_path)\nC:\\Users\\chenf\\AppData\\Local\\Temp\\ipykernel_21288\\1648787826.py:8: DtypeWarning: Columns (18) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(file_path)\nC:\\Users\\chenf\\AppData\\Local\\Temp\\ipykernel_21288\\1648787826.py:8: DtypeWarning: Columns (18) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(file_path)\nC:\\Users\\chenf\\AppData\\Local\\Temp\\ipykernel_21288\\1648787826.py:8: DtypeWarning: Columns (18) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(file_path)\nC:\\Users\\chenf\\AppData\\Local\\Temp\\ipykernel_21288\\1648787826.py:8: DtypeWarning: Columns (18) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(file_path)\nC:\\Users\\chenf\\AppData\\Local\\Temp\\ipykernel_21288\\1648787826.py:8: DtypeWarning: Columns (18) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(file_path)\nC:\\Users\\chenf\\AppData\\Local\\Temp\\ipykernel_21288\\1648787826.py:8: DtypeWarning: Columns (18) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(file_path)\nC:\\Users\\chenf\\AppData\\Local\\Temp\\ipykernel_21288\\1648787826.py:8: DtypeWarning: Columns (18) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(file_path)\nC:\\Users\\chenf\\AppData\\Local\\Temp\\ipykernel_21288\\1648787826.py:8: DtypeWarning: Columns (18) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(file_path)\nC:\\Users\\chenf\\AppData\\Local\\Temp\\ipykernel_21288\\1648787826.py:8: DtypeWarning: Columns (18) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(file_path)\nC:\\Users\\chenf\\AppData\\Local\\Temp\\ipykernel_21288\\1648787826.py:8: DtypeWarning: Columns (18) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(file_path)\nC:\\Users\\chenf\\AppData\\Local\\Temp\\ipykernel_21288\\1648787826.py:8: DtypeWarning: Columns (18) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(file_path)\nC:\\Users\\chenf\\AppData\\Local\\Temp\\ipykernel_21288\\1648787826.py:8: DtypeWarning: Columns (18) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(file_path)\nC:\\Users\\chenf\\AppData\\Local\\Temp\\ipykernel_21288\\1648787826.py:8: DtypeWarning: Columns (18) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(file_path)\nC:\\Users\\chenf\\AppData\\Local\\Temp\\ipykernel_21288\\1648787826.py:8: DtypeWarning: Columns (18) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(file_path)\nC:\\Users\\chenf\\AppData\\Local\\Temp\\ipykernel_21288\\1648787826.py:8: DtypeWarning: Columns (18,34,36,37,38,39,42,43,44,45,46,47) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(file_path)"
  },
  {
    "objectID": "analysis/get_data_ADVAN.html#section-3",
    "href": "analysis/get_data_ADVAN.html#section-3",
    "title": "ADVAN Foot Traffic Data",
    "section": "2022",
    "text": "2022\n\n#2022\nfolder_path = \"data/ADVAN/data_22/\"\nfiles = [file for file in os.listdir(folder_path)]\ndfs = []\n\nfor file in files:\n    file_path = os.path.join(folder_path, file)\n    df = pd.read_csv(file_path)\n    dfs.append(df)\n\ndf_final_22 = pd.concat(dfs, ignore_index=True)\n\ndf_philly_22 = df_final_22.loc[(df_final_22[\"CITY\"] == \"Philadelphia\") & (df_final_22[\"REGION\"] == \"PA\")]\n\ndf_final_22.to_csv(\"data/ADVAN/df_22.csv\", index=False)\ndf_philly_22.to_csv(\"data/ADVAN/df_philly_22.csv\", index=False)\n\nC:\\Users\\chenf\\AppData\\Local\\Temp\\ipykernel_14392\\4188310060.py:8: DtypeWarning: Columns (18) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(file_path)\nC:\\Users\\chenf\\AppData\\Local\\Temp\\ipykernel_14392\\4188310060.py:8: DtypeWarning: Columns (18) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(file_path)\nC:\\Users\\chenf\\AppData\\Local\\Temp\\ipykernel_14392\\4188310060.py:8: DtypeWarning: Columns (18) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(file_path)\nC:\\Users\\chenf\\AppData\\Local\\Temp\\ipykernel_14392\\4188310060.py:8: DtypeWarning: Columns (18) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(file_path)\nC:\\Users\\chenf\\AppData\\Local\\Temp\\ipykernel_14392\\4188310060.py:8: DtypeWarning: Columns (18) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(file_path)\nC:\\Users\\chenf\\AppData\\Local\\Temp\\ipykernel_14392\\4188310060.py:8: DtypeWarning: Columns (18) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(file_path)\nC:\\Users\\chenf\\AppData\\Local\\Temp\\ipykernel_14392\\4188310060.py:8: DtypeWarning: Columns (18) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(file_path)\nC:\\Users\\chenf\\AppData\\Local\\Temp\\ipykernel_14392\\4188310060.py:8: DtypeWarning: Columns (18) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(file_path)\nC:\\Users\\chenf\\AppData\\Local\\Temp\\ipykernel_14392\\4188310060.py:8: DtypeWarning: Columns (18) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(file_path)\nC:\\Users\\chenf\\AppData\\Local\\Temp\\ipykernel_14392\\4188310060.py:8: DtypeWarning: Columns (18) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(file_path)\nC:\\Users\\chenf\\AppData\\Local\\Temp\\ipykernel_14392\\4188310060.py:8: DtypeWarning: Columns (18) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(file_path)\nC:\\Users\\chenf\\AppData\\Local\\Temp\\ipykernel_14392\\4188310060.py:8: DtypeWarning: Columns (18) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(file_path)\nC:\\Users\\chenf\\AppData\\Local\\Temp\\ipykernel_14392\\4188310060.py:8: DtypeWarning: Columns (18) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(file_path)\nC:\\Users\\chenf\\AppData\\Local\\Temp\\ipykernel_14392\\4188310060.py:8: DtypeWarning: Columns (18) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(file_path)\nC:\\Users\\chenf\\AppData\\Local\\Temp\\ipykernel_14392\\4188310060.py:8: DtypeWarning: Columns (18) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(file_path)\nC:\\Users\\chenf\\AppData\\Local\\Temp\\ipykernel_14392\\4188310060.py:8: DtypeWarning: Columns (18) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(file_path)\nC:\\Users\\chenf\\AppData\\Local\\Temp\\ipykernel_14392\\4188310060.py:8: DtypeWarning: Columns (18) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(file_path)\nC:\\Users\\chenf\\AppData\\Local\\Temp\\ipykernel_14392\\4188310060.py:8: DtypeWarning: Columns (18) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(file_path)\nC:\\Users\\chenf\\AppData\\Local\\Temp\\ipykernel_14392\\4188310060.py:8: DtypeWarning: Columns (18) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(file_path)\nC:\\Users\\chenf\\AppData\\Local\\Temp\\ipykernel_14392\\4188310060.py:8: DtypeWarning: Columns (18) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(file_path)\nC:\\Users\\chenf\\AppData\\Local\\Temp\\ipykernel_14392\\4188310060.py:8: DtypeWarning: Columns (18) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(file_path)\nC:\\Users\\chenf\\AppData\\Local\\Temp\\ipykernel_14392\\4188310060.py:8: DtypeWarning: Columns (18) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(file_path)\nC:\\Users\\chenf\\AppData\\Local\\Temp\\ipykernel_14392\\4188310060.py:8: DtypeWarning: Columns (18,34,36,37,38,39,42,43,44,45,46,47) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(file_path)\nC:\\Users\\chenf\\AppData\\Local\\Temp\\ipykernel_14392\\4188310060.py:8: DtypeWarning: Columns (18) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(file_path)\nC:\\Users\\chenf\\AppData\\Local\\Temp\\ipykernel_14392\\4188310060.py:8: DtypeWarning: Columns (18) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(file_path)\nC:\\Users\\chenf\\AppData\\Local\\Temp\\ipykernel_14392\\4188310060.py:8: DtypeWarning: Columns (18) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(file_path)\nC:\\Users\\chenf\\AppData\\Local\\Temp\\ipykernel_14392\\4188310060.py:8: DtypeWarning: Columns (18) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(file_path)\nC:\\Users\\chenf\\AppData\\Local\\Temp\\ipykernel_14392\\4188310060.py:8: DtypeWarning: Columns (18) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(file_path)\nC:\\Users\\chenf\\AppData\\Local\\Temp\\ipykernel_14392\\4188310060.py:8: DtypeWarning: Columns (18) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(file_path)\nC:\\Users\\chenf\\AppData\\Local\\Temp\\ipykernel_14392\\4188310060.py:8: DtypeWarning: Columns (18,36,37,38,39,47) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(file_path)\nC:\\Users\\chenf\\AppData\\Local\\Temp\\ipykernel_14392\\4188310060.py:8: DtypeWarning: Columns (18) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(file_path)\nC:\\Users\\chenf\\AppData\\Local\\Temp\\ipykernel_14392\\4188310060.py:8: DtypeWarning: Columns (18,34,36,37,38,39,42,43,44,45,46,47) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(file_path)\nC:\\Users\\chenf\\AppData\\Local\\Temp\\ipykernel_14392\\4188310060.py:8: DtypeWarning: Columns (18) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(file_path)\nC:\\Users\\chenf\\AppData\\Local\\Temp\\ipykernel_14392\\4188310060.py:8: DtypeWarning: Columns (18) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(file_path)\nC:\\Users\\chenf\\AppData\\Local\\Temp\\ipykernel_14392\\4188310060.py:8: DtypeWarning: Columns (18) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(file_path)\nC:\\Users\\chenf\\AppData\\Local\\Temp\\ipykernel_14392\\4188310060.py:8: DtypeWarning: Columns (18) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(file_path)\nC:\\Users\\chenf\\AppData\\Local\\Temp\\ipykernel_14392\\4188310060.py:8: DtypeWarning: Columns (18) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(file_path)\nC:\\Users\\chenf\\AppData\\Local\\Temp\\ipykernel_14392\\4188310060.py:8: DtypeWarning: Columns (18) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(file_path)\nC:\\Users\\chenf\\AppData\\Local\\Temp\\ipykernel_14392\\4188310060.py:8: DtypeWarning: Columns (18) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(file_path)\nC:\\Users\\chenf\\AppData\\Local\\Temp\\ipykernel_14392\\4188310060.py:8: DtypeWarning: Columns (18) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(file_path)\nC:\\Users\\chenf\\AppData\\Local\\Temp\\ipykernel_14392\\4188310060.py:8: DtypeWarning: Columns (18) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(file_path)\nC:\\Users\\chenf\\AppData\\Local\\Temp\\ipykernel_14392\\4188310060.py:8: DtypeWarning: Columns (18) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(file_path)\nC:\\Users\\chenf\\AppData\\Local\\Temp\\ipykernel_14392\\4188310060.py:8: DtypeWarning: Columns (18) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(file_path)\nC:\\Users\\chenf\\AppData\\Local\\Temp\\ipykernel_14392\\4188310060.py:8: DtypeWarning: Columns (18) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(file_path)\nC:\\Users\\chenf\\AppData\\Local\\Temp\\ipykernel_14392\\4188310060.py:8: DtypeWarning: Columns (18) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(file_path)\nC:\\Users\\chenf\\AppData\\Local\\Temp\\ipykernel_14392\\4188310060.py:8: DtypeWarning: Columns (18) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(file_path)\nC:\\Users\\chenf\\AppData\\Local\\Temp\\ipykernel_14392\\4188310060.py:8: DtypeWarning: Columns (18) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(file_path)\nC:\\Users\\chenf\\AppData\\Local\\Temp\\ipykernel_14392\\4188310060.py:8: DtypeWarning: Columns (18,34,36,37,38,39,42,43,44,45,46,47) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(file_path)\nC:\\Users\\chenf\\AppData\\Local\\Temp\\ipykernel_14392\\4188310060.py:8: DtypeWarning: Columns (18) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(file_path)\nC:\\Users\\chenf\\AppData\\Local\\Temp\\ipykernel_14392\\4188310060.py:8: DtypeWarning: Columns (18) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(file_path)\nC:\\Users\\chenf\\AppData\\Local\\Temp\\ipykernel_14392\\4188310060.py:8: DtypeWarning: Columns (18) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(file_path)\nC:\\Users\\chenf\\AppData\\Local\\Temp\\ipykernel_14392\\4188310060.py:8: DtypeWarning: Columns (18) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(file_path)"
  },
  {
    "objectID": "analysis/get_data_ADVAN.html#section-4",
    "href": "analysis/get_data_ADVAN.html#section-4",
    "title": "ADVAN Foot Traffic Data",
    "section": "2023",
    "text": "2023\n\n#2023\nfolder_path = \"data/ADVAN/data_23/\"\nfiles = [file for file in os.listdir(folder_path)]\ndfs = []\n\nfor file in files:\n    file_path = os.path.join(folder_path, file)\n    df = pd.read_csv(file_path)\n    dfs.append(df)\n\ndf_final_23 = pd.concat(dfs, ignore_index=True)\n\ndf_philly_23 = df_final_23.loc[(df_final_23[\"CITY\"] == \"Philadelphia\") & (df_final_23[\"REGION\"] == \"PA\")]\n\ndf_final_23.to_csv(\"data/ADVAN/df_23.csv\", index=False)\ndf_philly_23.to_csv(\"data/ADVAN/df_philly_23.csv\", index=False)\n\nC:\\Users\\chenf\\AppData\\Local\\Temp\\ipykernel_22324\\2293737600.py:8: DtypeWarning: Columns (34,36,37,38,39,42,43,44,45,46,47) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(file_path)"
  },
  {
    "objectID": "analysis/get_data_ADVAN.html#section-5",
    "href": "analysis/get_data_ADVAN.html#section-5",
    "title": "ADVAN Foot Traffic Data",
    "section": "2024",
    "text": "2024\n\n#2024\nfolder_path = \"data/ADVAN/data_24/\"\nfiles = [file for file in os.listdir(folder_path)]\ndfs = []\n\nfor file in files:\n    file_path = os.path.join(folder_path, file)\n    df = pd.read_csv(file_path)\n    dfs.append(df)\n\ndf_final_24 = pd.concat(dfs, ignore_index=True)\n\ndf_philly_24 = df_final_24.loc[(df_final_24[\"CITY\"] == \"Philadelphia\") & (df_final_24[\"REGION\"] == \"PA\")]\n\ndf_final_24.to_csv(\"data/ADVAN/df_24.csv\", index=False)\ndf_philly_24.to_csv(\"data/ADVAN/df_philly_24.csv\", index=False)"
  },
  {
    "objectID": "analysis/data_process.html",
    "href": "analysis/data_process.html",
    "title": "Data Processing",
    "section": "",
    "text": "import pandas as pd\nimport geopandas as gpd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport altair as alt\nimport holoviews as hv\nimport hvplot.pandas\n\n\n\n\n\n\n\n\n\n\nFirst, we will load the data and take a look at it.\n\n# Load the ADVAN data\nadvan_raw = pd.read_csv('data/ADVAN/df_philly_19_24.csv')\n\n# Load the SafeGraph data\nsg_raw = pd.read_csv('data/SafeGraph/df_philly_19_24.csv')\n\nC:\\Users\\chenf\\AppData\\Local\\Temp\\ipykernel_1524\\2411687974.py:2: DtypeWarning: Columns (17,18,19) have mixed types. Specify dtype option on import or set low_memory=False.\n  advan_raw = pd.read_csv('data/ADVAN/df_philly_19_24.csv')\n\n\n\nprint(f\"Advan foot traffic data has {advan_raw.shape[0]} rows and SafeGraph spend pattern data has {sg_raw.shape[0]} rows.\")\n\nAdvan foot traffic data has 373583 rows and SafeGraph spend pattern data has 24083 rows.\n\n\n\nadvan_raw['NAICS_CODE'] = advan_raw['NAICS_CODE'].astype(str)\n\nadvan = advan_raw.loc[\n    ~advan_raw['POPULARITY_BY_HOUR'].isnull()\n    ].loc[\n    advan_raw['NAICS_CODE'].str.startswith('42') |  # Wholesale Trade\n    advan_raw['NAICS_CODE'].str.startswith('44') |  # Retail Trade\n    advan_raw['NAICS_CODE'].str.startswith('45') |  # Retail Trade\n    advan_raw['NAICS_CODE'].str.startswith('72')    # Accommodation and Food Services\n]\n\nadvan_gdf = gpd.GeoDataFrame(\n    advan, \n    geometry=gpd.points_from_xy(advan['LONGITUDE'], advan['LATITUDE']),\n    crs='EPSG:4326'\n)\n\n\nadvan_sg = advan_gdf.merge(\n    sg_raw,\n    left_on=['PLACEKEY', 'DATE_RANGE_START'],\n    right_on=['PLACEKEY', 'SPEND_DATE_RANGE_START'],\n    how='left'\n)\n\nLet’s reproject the GeoDataFrames and map them.\n\nphilly_boundary = gpd.read_file('data/City_Limits.geojson')\n\n\n# Convert to the same CRS\nadvan_gdf = advan_gdf.to_crs(2272)\nadvan_sg = advan_sg.to_crs(2272)\nphilly_boundary = philly_boundary.to_crs(2272)\n\n\nadvan_gdf['RAW_VISIT_COUNTS'].describe()\n\ncount     70419.000000\nmean       1181.235476\nstd        8402.935647\nmin           1.000000\n25%          35.000000\n50%         135.000000\n75%         430.000000\nmax      273108.000000\nName: RAW_VISIT_COUNTS, dtype: float64\n\n\n\nadvan_gdf.hvplot(\n    geo=True,\n    c='RAW_VISIT_COUNTS', \n    cmap='viridis', \n    hover_cols=['LOCATION_NAME', 'RAW_VISIT_COUNTS', 'RAW_VISITOR_COUNTS'], \n    groupby='DATE_RANGE_START',\n    dynamic=False,\n    width=800,\n    height=600,\n    crs=2272,\n    title=\"Number of Visits by Place\",\n    xlabel=\"Longitude\",\n    ylabel=\"Latitude\",\n    line_color='white',\n    line_width=0.3\n)\n\n\n\n\n\n  \n\n\n\n\n\n# Ensure NAICS_CODE is a string and handle null values\nadvan_sg['NAICS_CODE'] = advan_sg['NAICS_CODE'].astype(str)\n\n# Filter the DataFrame\nsg_gdf = advan_sg.loc[\n    ~advan_sg['RAW_TOTAL_SPEND'].isnull()  # Ensure RAW_TOTAL_SPEND is not null\n]\n\n\nsg_gdf['RAW_TOTAL_SPEND'].describe()\n\ncount    1.685100e+04\nmean     6.399669e+03\nstd      2.724914e+04\nmin      3.000000e+00\n25%      4.280100e+02\n50%      1.483730e+03\n75%      4.475825e+03\nmax      1.207083e+06\nName: RAW_TOTAL_SPEND, dtype: float64\n\n\n\nsg_gdf.to_file('data/advan_sg.geojson', driver='GeoJSON')\n\n\nsg_gdf.hvplot(\n    geo=True,\n    c='RAW_TOTAL_SPEND', \n    cmap='viridis', \n    hover_cols=['LOCATION_NAME', 'RAW_VISIT_COUNTS', 'RAW_VISITOR_COUNTS', 'RAW_TOTAL_SPEND', 'RAW_NUM_TRANSACTIONS', 'RAW_NUM_CUSTOMERS'], \n    groupby='DATE_RANGE_START',\n    dynamic=False,\n    width=800,\n    height=600,\n    crs=2272,\n    title=\"Amount of Income by Place\",\n    xlabel=\"Longitude\",\n    ylabel=\"Latitude\",\n    line_color='white',\n    line_width=0.3\n)\n\n\n\n\n\n  \n\n\n\n\n\nsg_gdf.loc[sg_gdf['ONLINE_SPEND'] &gt; 0].hvplot(\n    geo=True,\n    c='ONLINE_SPEND', \n    cmap='viridis', \n    hover_cols=['LOCATION_NAME', 'RAW_VISIT_COUNTS', 'RAW_VISITOR_COUNTS', 'RAW_TOTAL_SPEND', 'RAW_NUM_TRANSACTIONS', 'ONLINE_TRANSACTIONS', 'ONLINE_SPEND'], \n    groupby='DATE_RANGE_START',\n    dynamic=False,\n    width=800,\n    height=600,\n    crs=2272,\n    title=\"Amount of Online Income by Place\",\n    xlabel=\"Longitude\",\n    ylabel=\"Latitude\",\n    line_color='white',\n    line_width=0.3\n)\n\n\n\n\n\n  \n\n\n\n\n\nsg_gdf.loc[sg_gdf['ONLINE_SPEND'] &gt; 0, 'ONLINE_SPEND'].describe()\n\ncount      2924.000000\nmean       1360.562237\nstd       15387.100782\nmin           0.890000\n25%          26.957500\n50%         103.230000\n75%         438.690000\nmax      542717.460000\nName: ONLINE_SPEND, dtype: float64\n\n\n\nadvan_sg = gpd.read_file('data/advan_sg.geojson')\n\n\n# advan_sg = advan_sg[['PLACEKEY', 'LOCATION_NAME', 'TOP_CATEGORY', 'LATITUDE', 'LONGITUDE', 'DATE_RANGE_START', 'RAW_VISIT_COUNTS', 'RAW_VISITOR_COUNTS', 'POI_CBG', 'MEDIAN_DWELL', 'BUCKETED_DWELL_TIMES', 'POPULARITY_BY_DAY', 'NORMALIZED_VISITS_BY_STATE_SCALING', 'RAW_TOTAL_SPEND', 'RAW_NUM_TRANSACTIONS', 'RAW_NUM_CUSTOMERS', 'MEDIAN_SPEND_PER_TRANSACTION', 'MEDIAN_SPEND_PER_CUSTOMER', 'BUCKETED_CUSTOMER_FREQUENCY', 'MEAN_SPEND_PER_CUSTOMER_BY_FREQUENCY']]\n\nadvan_sg = advan_sg[['PLACEKEY', 'LOCATION_NAME', 'TOP_CATEGORY', 'STREET_ADDRESS', 'LATITUDE', 'LONGITUDE', 'DATE_RANGE_START', 'RAW_VISIT_COUNTS', 'RAW_VISITOR_COUNTS', 'POI_CBG', 'MEDIAN_DWELL', 'NORMALIZED_VISITS_BY_STATE_SCALING', 'RAW_TOTAL_SPEND', 'RAW_NUM_TRANSACTIONS', 'RAW_NUM_CUSTOMERS', 'MEDIAN_SPEND_PER_TRANSACTION', 'MEDIAN_SPEND_PER_CUSTOMER']]\n\n\nadvan_sg['rate'] = advan_sg['NORMALIZED_VISITS_BY_STATE_SCALING'] / advan_sg['RAW_VISIT_COUNTS']\n\n\nadvan_sg['DATE_RANGE_START'] = advan_sg['DATE_RANGE_START'].dt.year\n\n\na = advan_sg.groupby('DATE_RANGE_START').agg({'rate': 'mean'}).reset_index()\n\na['state'] = [12807060, 12801989, 12883022, 12964056, 12972008, 12961683]\na['county'] = [1584138, 1584064, 1580157, 1576251, 1567258, 1550542]\na['normal'] = a['rate'] / a['state'] * a['county']\n\n\nb = advan_sg.merge(a, on='DATE_RANGE_START')\nb['Total Visits'] = round(b['normal'] * b['RAW_VISIT_COUNTS'])\nb['Total Visitors'] = round(b['normal'] * b['RAW_VISITOR_COUNTS'])\nb['Total Spend'] = round(b['normal'] * b['RAW_TOTAL_SPEND'])\nb['Total Transactions'] = round(b['normal'] * b['RAW_NUM_TRANSACTIONS'])\nb['Total Customers'] = round(b['normal'] * b['RAW_NUM_CUSTOMERS'])\n\nb = b[['PLACEKEY', 'LOCATION_NAME', 'TOP_CATEGORY', 'STREET_ADDRESS', 'LATITUDE', 'LONGITUDE', 'DATE_RANGE_START', 'Total Visits', 'Total Visitors', 'POI_CBG', 'MEDIAN_DWELL', 'Total Spend', 'Total Transactions', 'Total Customers', 'MEDIAN_SPEND_PER_TRANSACTION', 'MEDIAN_SPEND_PER_CUSTOMER']]\n\nb = b.rename(columns={'LOCATION_NAME': 'Name', 'STREET_ADDRESS': 'Address', 'TOP_CATEGORY': 'Category', 'MEDIAN_DWELL': 'Median Dwell Time', 'MEDIAN_SPEND_PER_TRANSACTION': 'Median Spend per Transaction', 'MEDIAN_SPEND_PER_CUSTOMER': 'Median Spend per Customer'})\n\n\nadvan_sg_2019 = b.loc[b['DATE_RANGE_START']==2019]\nadvan_sg_2020 = b.loc[b['DATE_RANGE_START']==2020]\nadvan_sg_2021 = b.loc[b['DATE_RANGE_START']==2021]\nadvan_sg_2022 = b.loc[b['DATE_RANGE_START']==2022]\nadvan_sg_2023 = b.loc[b['DATE_RANGE_START']==2023]\nadvan_sg_2024 = b.loc[b['DATE_RANGE_START']==2024]\n\n\nb['Total Spend'].describe()\n\ncount    1.685100e+04\nmean     1.086580e+04\nstd      4.359155e+04\nmin      5.000000e+00\n25%      7.170000e+02\n50%      2.477000e+03\n75%      7.644500e+03\nmax      1.589128e+06\nName: Total Spend, dtype: float64\n\n\n\nadvan_sg_2019 = gpd.GeoDataFrame(advan_sg_2019, geometry=gpd.points_from_xy(advan_sg_2019['LONGITUDE'], advan_sg_2019['LATITUDE']), crs='EPSG:4326')\nadvan_sg_2020 = gpd.GeoDataFrame(advan_sg_2020, geometry=gpd.points_from_xy(advan_sg_2020['LONGITUDE'], advan_sg_2020['LATITUDE']), crs='EPSG:4326')\nadvan_sg_2021 = gpd.GeoDataFrame(advan_sg_2021, geometry=gpd.points_from_xy(advan_sg_2021['LONGITUDE'], advan_sg_2021['LATITUDE']), crs='EPSG:4326')\nadvan_sg_2022 = gpd.GeoDataFrame(advan_sg_2022, geometry=gpd.points_from_xy(advan_sg_2022['LONGITUDE'], advan_sg_2022['LATITUDE']), crs='EPSG:4326')\nadvan_sg_2023 = gpd.GeoDataFrame(advan_sg_2023, geometry=gpd.points_from_xy(advan_sg_2023['LONGITUDE'], advan_sg_2023['LATITUDE']), crs='EPSG:4326')\nadvan_sg_2024 = gpd.GeoDataFrame(advan_sg_2024, geometry=gpd.points_from_xy(advan_sg_2024['LONGITUDE'], advan_sg_2024['LATITUDE']), crs='EPSG:4326')\n\n\ndata = gpd.GeoDataFrame(b, geometry=gpd.points_from_xy(b['LONGITUDE'], b['LATITUDE']), crs='EPSG:4326')\n\n\nadvan_sg_2019.to_file('E:/MUSA 6110/engagement-project/assets/data/data_2019.geojson', driver='GeoJSON')\nadvan_sg_2020.to_file('E:/MUSA 6110/engagement-project/assets/data/data_2020.geojson', driver='GeoJSON')\nadvan_sg_2021.to_file('E:/MUSA 6110/engagement-project/assets/data/data_2021.geojson', driver='GeoJSON')\nadvan_sg_2022.to_file('E:/MUSA 6110/engagement-project/assets/data/data_2022.geojson', driver='GeoJSON')\nadvan_sg_2023.to_file('E:/MUSA 6110/engagement-project/assets/data/data_2023.geojson', driver='GeoJSON')\nadvan_sg_2024.to_file('E:/MUSA 6110/engagement-project/assets/data/data_2024.geojson', driver='GeoJSON')\n\n\ndata.to_file('E:/MUSA 6110/engagement-project/assets/data/data.geojson', driver='GeoJSON')\n\n\nadvan_sg_2019.to_file('data/advan_sg_2019.geojson', driver='GeoJSON')\nadvan_sg_2020.to_file('data/advan_sg_2020.geojson', driver='GeoJSON')   \nadvan_sg_2021.to_file('data/advan_sg_2021.geojson', driver='GeoJSON')\nadvan_sg_2022.to_file('data/advan_sg_2022.geojson', driver='GeoJSON')\nadvan_sg_2023.to_file('data/advan_sg_2023.geojson', driver='GeoJSON')\nadvan_sg_2024.to_file('data/advan_sg_2024.geojson', driver='GeoJSON')"
  },
  {
    "objectID": "analysis/exploratory.html",
    "href": "analysis/exploratory.html",
    "title": "Exploratory Analysis",
    "section": "",
    "text": "In this section, I will explore the foot traffic data and the spend patterns data, and try to have more insights about the changes over time by visualizing them.\nCode\nimport pandas as pd\nimport geopandas as gpd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.ticker import FuncFormatter\nimport seaborn as sns\nimport altair as alt\nimport holoviews as hv\nimport hvplot.pandas\nhv.extension('bokeh')\npd.set_option(\"display.float_format\", \"{:.2f}\".format)\nCode\n# Load data\nadvan_sg = gpd.read_file('../data/advan_sg.geojson')\ncity = gpd.read_file('../data/City_Limits.geojson')\n\n# City plot\ncity_plot = city.hvplot(geo=True, alpha=0.5, line_color='black', line_width=1, color='white', hover=False, crs=4326)\nFirst, let’s load the data and have an overview of the average foot traffic and average spend over time in the table below.\nCode\nsummary_table = advan_sg.groupby('DATE_RANGE_START').sum(numeric_only=True).drop(columns=['LATITUDE', 'LONGITUDE', 'RAW_NUM_CUSTOMERS'])\n\nsummary_table = summary_table.reset_index()\n\n# Rename columns if needed\nsummary_table.columns = ['Year', 'Total Visits', 'Total Visitors', 'Total Spend', 'Total Transactions']\n\nsummary_table\n\n\n\n\n\n\n\n\n\nYear\nTotal Visits\nTotal Visitors\nTotal Spend\nTotal Transactions\n\n\n\n\n0\n2019\n5591954.00\n3299734.00\n18602269.20\n730129.00\n\n\n1\n2020\n1484566.00\n921549.00\n13145216.02\n407680.00\n\n\n2\n2021\n2524335.00\n1589399.00\n16743947.70\n544171.00\n\n\n3\n2022\n3953924.00\n2468341.00\n21524058.02\n683882.00\n\n\n4\n2023\n1919268.00\n1275651.00\n19350828.55\n650189.00\n\n\n5\n2024\n1548388.00\n1033117.00\n18474501.11\n610346.00",
    "crumbs": [
      "Analysis",
      "Exploratory Analysis"
    ]
  },
  {
    "objectID": "analysis/prediction.html",
    "href": "analysis/prediction.html",
    "title": "Modeling",
    "section": "",
    "text": "In this section, I will construct the model using Random Forest Regressor from the scikit-learn library. The model will be split into 70% training data and 30% test data, and I will also use grid search to find the best hyperparameters for the model.\nThere will be 4 models in total, each with different features, and I will compare the performance of each model using the median absolute error and the R-squared score. The four models are:\nIn summary, the steps to build a model are:",
    "crumbs": [
      "Analysis",
      "Modeling"
    ]
  },
  {
    "objectID": "analysis/prediction.html#preparation",
    "href": "analysis/prediction.html#preparation",
    "title": "Modeling",
    "section": "Preparation",
    "text": "Preparation\nBefore constructing the model, I will first preprocess the data. Basically, I will use data of 2022 and 2023 to predict the data of 2024. The selected features are:\n\nvisits: the number of visits to a place in 2023\nvisitors: the number of visitors to a place in 2023\ntransactions: the number of transactions in a place in 2023\nspend: the mean amount of money spent in a place of year 2022 and 2023\n\nMoreover, I will also add the spatial features of 1, 3, and 5 nearest neighbours to the data. The spatial features are:\n\nvisits_neighbour: the mean value of number of visits of the k nearest neighbours of a place in 2023\nvisitors_neighbour: the mean value of the number of visitors of the k nearest neighbours of a place in 2023\ntransactions_neighbour: the mean value of the number of transactions of the k nearest neighbours of a place in 2023\n\n\n\nCode\nimport pandas as pd\nimport geopandas as gpd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport altair as alt\nimport holoviews as hv\nimport hvplot.pandas\nhv.extension('bokeh')\n\nnp.random.seed(42)\npd.options.display.max_columns = 999\npd.set_option(\"display.float_format\", \"{:.2f}\".format)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n  \n\n\n\n\n\n\n\nFirst, I will load the data and select the wanted columns.\n\n\nCode\nraw_data = gpd.read_file('../data/advan_sg.geojson')\n\n# filter 2022, 2023, 2024 data only for the model\nraw_data_2022 = raw_data.loc[raw_data['DATE_RANGE_START']==2022]\nraw_data_2023 = raw_data.loc[raw_data['DATE_RANGE_START']==2023]\nraw_data_2024 = raw_data.loc[raw_data['DATE_RANGE_START']==2024]\n\n# merge into one dataframe\ndata = raw_data_2023.merge(raw_data_2022, on='PLACEKEY', suffixes=('_2023', '_2022'))\ndata = data.merge(raw_data_2024, on='PLACEKEY', how='right')\ndata = data.dropna()\ndata = data.drop(columns=['LOCATION_NAME_2023', 'TOP_CATEGORY_2023', 'LATITUDE_2023', 'LONGITUDE_2023', 'DATE_RANGE_START_2023', 'RAW_NUM_CUSTOMERS_2023', 'geometry_2023', 'LOCATION_NAME_2022', 'TOP_CATEGORY_2022', 'LATITUDE_2022', 'LONGITUDE_2022', 'DATE_RANGE_START_2022', 'RAW_VISIT_COUNTS_2022', 'RAW_VISITOR_COUNTS_2022', 'RAW_NUM_TRANSACTIONS_2022', 'RAW_NUM_CUSTOMERS_2022', 'geometry_2022', 'DATE_RANGE_START', 'RAW_VISIT_COUNTS', 'RAW_VISITOR_COUNTS', 'RAW_NUM_TRANSACTIONS', 'RAW_NUM_CUSTOMERS'])\n\ndata = data.rename(columns={'RAW_VISIT_COUNTS_2023': 'visits', 'RAW_VISITOR_COUNTS_2023': 'visitors', 'RAW_TOTAL_SPEND_2023': 'spend_2023', 'RAW_NUM_TRANSACTIONS_2023': 'transactions', 'RAW_TOTAL_SPEND_2022': 'spend_2022', 'RAW_TOTAL_SPEND': 'spend_2024'})\n\ndata['spend'] = (data['spend_2023'] + data['spend_2022']) / 2\n\ndata = gpd.GeoDataFrame(data, geometry='geometry')\n\ndata.head()\n\n\n\n\n\n\n\n\n\nPLACEKEY\nvisits\nvisitors\nspend_2023\ntransactions\nspend_2022\nLOCATION_NAME\nTOP_CATEGORY\nLATITUDE\nLONGITUDE\nspend_2024\ngeometry\nspend\n\n\n\n\n0\nzzw-224@628-pmf-skf\n227.0\n202.0\n5442.35\n152.0\n17446.21\nMisconduct Tavern\nRestaurants and Other Eating Places\n39.948709\n-75.166626\n6196.76\nPOINT (-75.16663 39.94871)\n11444.280\n\n\n1\nzzw-223@628-p7x-w6k\n178.0\n100.0\n2782.23\n180.0\n4617.35\nPizza Roma\nRestaurants and Other Eating Places\n40.049594\n-75.059819\n3495.90\nPOINT (-75.05982 40.04959)\n3699.790\n\n\n2\n22x-223@628-pmb-fpv\n273.0\n171.0\n1802.47\n43.0\n2955.04\nShaking Seafood\nRestaurants and Other Eating Places\n39.946982\n-75.157365\n2734.49\nPOINT (-75.15736 39.94698)\n2378.755\n\n\n3\n22d-222@628-pmb-6hq\n186.0\n174.0\n1425.15\n68.0\n4052.00\nDaMo Pasta Lab\nRestaurants and Other Eating Places\n39.949786\n-75.160216\n1146.45\nPOINT (-75.16022 39.94979)\n2738.575\n\n\n6\n225-222@628-pft-zpv\n12.0\n10.0\n324.75\n23.0\n451.65\nWendy's\nRestaurants and Other Eating Places\n40.030377\n-75.211174\n261.88\nPOINT (-75.21117 40.03038)\n388.200\n\n\n\n\n\n\n\nThen, I will calculate the spatial features of 1, 3, and 5 nearest neighbours using the NearestNeighbors class from the scikit-learn library.\n\n\nCode\ndef get_xy_from_geometry(df):\n    \"\"\"\n    Return a numpy array with two columns, where the \n    first holds the `x` geometry coordinate and the second \n    column holds the `y` geometry coordinate\n    \"\"\"\n    x = df.geometry.x\n    y = df.geometry.y\n    \n    return np.column_stack((x, y)) # stack as columns\n\n# Extract x/y for data\ndataXY = get_xy_from_geometry(data)\n\ndataXY.shape\n\n\n(1890, 2)\n\n\n\n\nCode\nfrom sklearn.neighbors import NearestNeighbors\n\n# STEP 1: Define k values\nk_values = [1, 3, 5]\n\n# STEP 2: Initialize the NearestNeighbors algorithm with the maximum k\nmax_k = max(k_values)\nnbrs = NearestNeighbors(n_neighbors=max_k)\nnbrs.fit(dataXY)\n\n# STEP 3: Get distances for sale to neighbors\ndataDists, dataIndices = nbrs.kneighbors(dataXY)\n\n\n\n\nCode\n# Define columns for which to calculate mean\ncolumns = [\"visits\", \"visitors\", \"transactions\"]\n\n# Loop through each k value\nfor k in k_values:\n    for col in columns:\n        # Calculate the mean for each point based on the k nearest neighbors\n        data[f\"{col}_k{k}\"] = [\n            data.iloc[indices[:k]][col].mean() for indices in dataIndices\n        ]\n\ndata.head()\n\n\n\n\n\n\n\n\n\nPLACEKEY\nvisits\nvisitors\nspend_2023\ntransactions\nspend_2022\nLOCATION_NAME\nTOP_CATEGORY\nLATITUDE\nLONGITUDE\nspend_2024\ngeometry\nspend\nvisits_k1\nvisitors_k1\ntransactions_k1\nvisits_k3\nvisitors_k3\ntransactions_k3\nvisits_k5\nvisitors_k5\ntransactions_k5\n\n\n\n\n0\nzzw-224@628-pmf-skf\n227.0\n202.0\n5442.35\n152.0\n17446.21\nMisconduct Tavern\nRestaurants and Other Eating Places\n39.948709\n-75.166626\n6196.76\nPOINT (-75.16663 39.94871)\n11444.280\n227.0\n202.0\n152.0\n1231.666667\n892.000000\n78.000000\n988.6\n692.8\n329.2\n\n\n1\nzzw-223@628-p7x-w6k\n178.0\n100.0\n2782.23\n180.0\n4617.35\nPizza Roma\nRestaurants and Other Eating Places\n40.049594\n-75.059819\n3495.90\nPOINT (-75.05982 40.04959)\n3699.790\n178.0\n100.0\n180.0\n195.333333\n158.333333\n183.000000\n859.4\n620.0\n796.6\n\n\n2\n22x-223@628-pmb-fpv\n273.0\n171.0\n1802.47\n43.0\n2955.04\nShaking Seafood\nRestaurants and Other Eating Places\n39.946982\n-75.157365\n2734.49\nPOINT (-75.15736 39.94698)\n2378.755\n273.0\n171.0\n43.0\n135.333333\n67.000000\n28.333333\n144.8\n88.8\n30.8\n\n\n3\n22d-222@628-pmb-6hq\n186.0\n174.0\n1425.15\n68.0\n4052.00\nDaMo Pasta Lab\nRestaurants and Other Eating Places\n39.949786\n-75.160216\n1146.45\nPOINT (-75.16022 39.94979)\n2738.575\n186.0\n174.0\n68.0\n1014.000000\n758.333333\n84.000000\n1022.0\n774.8\n78.2\n\n\n6\n225-222@628-pft-zpv\n12.0\n10.0\n324.75\n23.0\n451.65\nWendy's\nRestaurants and Other Eating Places\n40.030377\n-75.211174\n261.88\nPOINT (-75.21117 40.03038)\n388.200\n12.0\n10.0\n23.0\n281.333333\n237.000000\n222.333333\n172.2\n144.2\n210.6\n\n\n\n\n\n\n\nFinally, I will split the data into training and test data.\n\n\nCode\n# Models\nfrom sklearn.ensemble import RandomForestRegressor\n\n# Model selection\nfrom sklearn.model_selection import train_test_split, GridSearchCV\n\n# Pipelines\nfrom sklearn.pipeline import make_pipeline\n\n# Preprocessing\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder\n\n\n\n\nCode\n# Split the data 70/30\ntrain_set, test_set = train_test_split(data, test_size=0.3, random_state=7)\n\n# the target labels\ny_train = train_set[\"spend_2024\"]\ny_test = test_set[\"spend_2024\"]",
    "crumbs": [
      "Analysis",
      "Modeling"
    ]
  },
  {
    "objectID": "analysis/prediction.html#building-the-model",
    "href": "analysis/prediction.html#building-the-model",
    "title": "Modeling",
    "section": "Building the Model",
    "text": "Building the Model\nIn this section, I will build four models using Random Forest Regressor from the scikit-learn library, and give the detailed steps of the model construction.\n\nModel 1: No Spatial Features\nFor the first model, I will use the data without the spatial features. There will be both numerical and categorical features in the data, so I will use the ColumnTransformer class from the scikit-learn library to preprocess the data.\n\n\nCode\n# Numerical columns\nnum_cols = [\n    \"visits\",\n    \"visitors\",\n    \"transactions\",\n    \"spend\"\n]\n\n# Categorical columns\ncat_cols = [\n    \"TOP_CATEGORY\"\n]\n\n# Set up the column transformer with two transformers\n# ----&gt; Scale the numerical columns\n# ----&gt; One-hot encode the categorical columns\n\ntransformer = ColumnTransformer(\n    transformers=[\n        (\"num\", StandardScaler(), num_cols),\n        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), cat_cols),\n    ]\n)\n\n\nThen, I will build a pipeline to process the data and build the model. The pipeline will consist of the ColumnTransformer class and the RandomForestRegressor class.\n\n\nCode\n# Initialize the pipeline\npipe = make_pipeline(\n    transformer, RandomForestRegressor(random_state=7)\n)\n\n\nFinally, I will use grid search to find the best hyperparameters for the model. Three hyperparameters will be tuned: n_estimators, max_depth, and min_samples_split.\n\n\nCode\n# Use GridSearchCV to find the best hyperparameters\n# Set up GridSearchCV\nparam_grid = {\n    'randomforestregressor__n_estimators': [50, 100, 200],\n    'randomforestregressor__max_depth': [20, 50, None],\n    'randomforestregressor__min_samples_split': [2, 4, 6]\n}\n\n# Set up the grid search\ngrid_search = GridSearchCV(\n    pipe, \n    param_grid, \n    cv=5, # 5-fold cross-validation\n    scoring='r2', # Use R^2 as the scoring metric\n    n_jobs=-1 # Use all available cpu cores\n    )\n\n\n\n\nCode\n# Fit the model\ngrid_search.fit(train_set, y_train)\n\n\nGridSearchCV(cv=5,\n             estimator=Pipeline(steps=[('columntransformer',\n                                        ColumnTransformer(transformers=[('num',\n                                                                         StandardScaler(),\n                                                                         ['visits',\n                                                                          'visitors',\n                                                                          'transactions',\n                                                                          'spend']),\n                                                                        ('cat',\n                                                                         OneHotEncoder(handle_unknown='ignore'),\n                                                                         ['TOP_CATEGORY'])])),\n                                       ('randomforestregressor',\n                                        RandomForestRegressor(random_state=7))]),\n             n_jobs=-1,\n             param_grid={'randomforestregressor__max_depth': [20, 50, None],\n                         'randomforestregressor__min_samples_split': [2, 4, 6],\n                         'randomforestregressor__n_estimators': [50, 100, 200]},\n             scoring='r2')In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GridSearchCVGridSearchCV(cv=5,\n             estimator=Pipeline(steps=[('columntransformer',\n                                        ColumnTransformer(transformers=[('num',\n                                                                         StandardScaler(),\n                                                                         ['visits',\n                                                                          'visitors',\n                                                                          'transactions',\n                                                                          'spend']),\n                                                                        ('cat',\n                                                                         OneHotEncoder(handle_unknown='ignore'),\n                                                                         ['TOP_CATEGORY'])])),\n                                       ('randomforestregressor',\n                                        RandomForestRegressor(random_state=7))]),\n             n_jobs=-1,\n             param_grid={'randomforestregressor__max_depth': [20, 50, None],\n                         'randomforestregressor__min_samples_split': [2, 4, 6],\n                         'randomforestregressor__n_estimators': [50, 100, 200]},\n             scoring='r2')estimator: PipelinePipeline(steps=[('columntransformer',\n                 ColumnTransformer(transformers=[('num', StandardScaler(),\n                                                  ['visits', 'visitors',\n                                                   'transactions', 'spend']),\n                                                 ('cat',\n                                                  OneHotEncoder(handle_unknown='ignore'),\n                                                  ['TOP_CATEGORY'])])),\n                ('randomforestregressor',\n                 RandomForestRegressor(random_state=7))])columntransformer: ColumnTransformerColumnTransformer(transformers=[('num', StandardScaler(),\n                                 ['visits', 'visitors', 'transactions',\n                                  'spend']),\n                                ('cat', OneHotEncoder(handle_unknown='ignore'),\n                                 ['TOP_CATEGORY'])])num['visits', 'visitors', 'transactions', 'spend']StandardScalerStandardScaler()cat['TOP_CATEGORY']OneHotEncoderOneHotEncoder(handle_unknown='ignore')RandomForestRegressorRandomForestRegressor(random_state=7)\n\n\n\n\nCode\n# Get the best hyperparameters\nbest_hyperparameters_1 = grid_search.best_params_\n\n# Evaluate on test set\nmodel_1 = grid_search.best_estimator_\ntest_score_1 = model_1.score(test_set, y_test)\n\n\n\n\nModel 2: with Spatial Features of 1 Nearest Neighbour\nFor the second model, I will use the data with spatial features of the nearest neighbor. The rest of the steps are the same as the first model.\n\n\nCode\n# Numerical columns\nnum_cols = [\n    \"visits\",\n    \"visitors\",\n    \"transactions\",\n    \"spend\",\n    \"visits_k1\",\n    \"visitors_k1\",\n    \"transactions_k1\",\n]\n\n# Categorical columns\ncat_cols = [\n    \"TOP_CATEGORY\"\n]\n\n# Set up the column transformer with two transformers\n# ----&gt; Scale the numerical columns\n# ----&gt; One-hot encode the categorical columns\n\ntransformer = ColumnTransformer(\n    transformers=[\n        (\"num\", StandardScaler(), num_cols),\n        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), cat_cols),\n    ]\n)\n\n# Initialize the pipeline\npipe = make_pipeline(\n    transformer, RandomForestRegressor(random_state=7)\n)\n\n# Use GridSearchCV to find the best hyperparameters\n# Set up GridSearchCV\nparam_grid = {\n    'randomforestregressor__n_estimators': [50, 100, 200],\n    'randomforestregressor__max_depth': [20, 50, None],\n    'randomforestregressor__min_samples_split': [2, 4, 6]\n}\n\n# Set up the grid search\ngrid_search = GridSearchCV(\n    pipe, \n    param_grid, \n    cv=5, # 5-fold cross-validation\n    scoring='r2', # Use R^2 as the scoring metric\n    n_jobs=-1 # Use all available cpu cores\n    )\n\n# Fit the model\ngrid_search.fit(train_set, y_train)\n\n\nGridSearchCV(cv=5,\n             estimator=Pipeline(steps=[('columntransformer',\n                                        ColumnTransformer(transformers=[('num',\n                                                                         StandardScaler(),\n                                                                         ['visits',\n                                                                          'visitors',\n                                                                          'transactions',\n                                                                          'spend',\n                                                                          'visits_k1',\n                                                                          'visitors_k1',\n                                                                          'transactions_k1']),\n                                                                        ('cat',\n                                                                         OneHotEncoder(handle_unknown='ignore'),\n                                                                         ['TOP_CATEGORY'])])),\n                                       ('randomforestregressor',\n                                        RandomForestRegressor(random_state=7))]),\n             n_jobs=-1,\n             param_grid={'randomforestregressor__max_depth': [20, 50, None],\n                         'randomforestregressor__min_samples_split': [2, 4, 6],\n                         'randomforestregressor__n_estimators': [50, 100, 200]},\n             scoring='r2')In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GridSearchCVGridSearchCV(cv=5,\n             estimator=Pipeline(steps=[('columntransformer',\n                                        ColumnTransformer(transformers=[('num',\n                                                                         StandardScaler(),\n                                                                         ['visits',\n                                                                          'visitors',\n                                                                          'transactions',\n                                                                          'spend',\n                                                                          'visits_k1',\n                                                                          'visitors_k1',\n                                                                          'transactions_k1']),\n                                                                        ('cat',\n                                                                         OneHotEncoder(handle_unknown='ignore'),\n                                                                         ['TOP_CATEGORY'])])),\n                                       ('randomforestregressor',\n                                        RandomForestRegressor(random_state=7))]),\n             n_jobs=-1,\n             param_grid={'randomforestregressor__max_depth': [20, 50, None],\n                         'randomforestregressor__min_samples_split': [2, 4, 6],\n                         'randomforestregressor__n_estimators': [50, 100, 200]},\n             scoring='r2')estimator: PipelinePipeline(steps=[('columntransformer',\n                 ColumnTransformer(transformers=[('num', StandardScaler(),\n                                                  ['visits', 'visitors',\n                                                   'transactions', 'spend',\n                                                   'visits_k1', 'visitors_k1',\n                                                   'transactions_k1']),\n                                                 ('cat',\n                                                  OneHotEncoder(handle_unknown='ignore'),\n                                                  ['TOP_CATEGORY'])])),\n                ('randomforestregressor',\n                 RandomForestRegressor(random_state=7))])columntransformer: ColumnTransformerColumnTransformer(transformers=[('num', StandardScaler(),\n                                 ['visits', 'visitors', 'transactions', 'spend',\n                                  'visits_k1', 'visitors_k1',\n                                  'transactions_k1']),\n                                ('cat', OneHotEncoder(handle_unknown='ignore'),\n                                 ['TOP_CATEGORY'])])num['visits', 'visitors', 'transactions', 'spend', 'visits_k1', 'visitors_k1', 'transactions_k1']StandardScalerStandardScaler()cat['TOP_CATEGORY']OneHotEncoderOneHotEncoder(handle_unknown='ignore')RandomForestRegressorRandomForestRegressor(random_state=7)\n\n\n\n\nCode\n# Get the best hyperparameters\nbest_hyperparameters_2 = grid_search.best_params_\n\n# Evaluate on test set\nmodel_2 = grid_search.best_estimator_\ntest_score_2 = model_2.score(test_set, y_test)\n\n\n\n\nModel 3: with Spatial Features of 3 Nearest Neighbours\nFor the third model, I will use the data with spatial features of 3 nearest neighbor. The rest of the steps are the same as the previous models.\n\n\nCode\n# Numerical columns\nnum_cols = [\n    \"visits\",\n    \"visitors\",\n    \"transactions\",\n    \"spend\",\n    \"visits_k3\",\n    \"visitors_k3\",\n    \"transactions_k3\",\n]\n\n# Categorical columns\ncat_cols = [\n    \"TOP_CATEGORY\"\n]\n\n# Set up the column transformer with two transformers\n# ----&gt; Scale the numerical columns\n# ----&gt; One-hot encode the categorical columns\n\ntransformer = ColumnTransformer(\n    transformers=[\n        (\"num\", StandardScaler(), num_cols),\n        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), cat_cols),\n    ]\n)\n\n# Initialize the pipeline\npipe = make_pipeline(\n    transformer, RandomForestRegressor(random_state=7)\n)\n\n# Use GridSearchCV to find the best hyperparameters\n# Set up GridSearchCV\nparam_grid = {\n    'randomforestregressor__n_estimators': [50, 100, 200],\n    'randomforestregressor__max_depth': [20, 50, None],\n    'randomforestregressor__min_samples_split': [2, 4, 6]\n}\n\n# Set up the grid search\ngrid_search = GridSearchCV(\n    pipe, \n    param_grid, \n    cv=5, # 5-fold cross-validation\n    scoring='r2', # Use R^2 as the scoring metric\n    n_jobs=-1 # Use all available cpu cores\n    )\n\n# Fit the model\ngrid_search.fit(train_set, y_train)\n\n\nGridSearchCV(cv=5,\n             estimator=Pipeline(steps=[('columntransformer',\n                                        ColumnTransformer(transformers=[('num',\n                                                                         StandardScaler(),\n                                                                         ['visits',\n                                                                          'visitors',\n                                                                          'transactions',\n                                                                          'spend',\n                                                                          'visits_k3',\n                                                                          'visitors_k3',\n                                                                          'transactions_k3']),\n                                                                        ('cat',\n                                                                         OneHotEncoder(handle_unknown='ignore'),\n                                                                         ['TOP_CATEGORY'])])),\n                                       ('randomforestregressor',\n                                        RandomForestRegressor(random_state=7))]),\n             n_jobs=-1,\n             param_grid={'randomforestregressor__max_depth': [20, 50, None],\n                         'randomforestregressor__min_samples_split': [2, 4, 6],\n                         'randomforestregressor__n_estimators': [50, 100, 200]},\n             scoring='r2')In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GridSearchCVGridSearchCV(cv=5,\n             estimator=Pipeline(steps=[('columntransformer',\n                                        ColumnTransformer(transformers=[('num',\n                                                                         StandardScaler(),\n                                                                         ['visits',\n                                                                          'visitors',\n                                                                          'transactions',\n                                                                          'spend',\n                                                                          'visits_k3',\n                                                                          'visitors_k3',\n                                                                          'transactions_k3']),\n                                                                        ('cat',\n                                                                         OneHotEncoder(handle_unknown='ignore'),\n                                                                         ['TOP_CATEGORY'])])),\n                                       ('randomforestregressor',\n                                        RandomForestRegressor(random_state=7))]),\n             n_jobs=-1,\n             param_grid={'randomforestregressor__max_depth': [20, 50, None],\n                         'randomforestregressor__min_samples_split': [2, 4, 6],\n                         'randomforestregressor__n_estimators': [50, 100, 200]},\n             scoring='r2')estimator: PipelinePipeline(steps=[('columntransformer',\n                 ColumnTransformer(transformers=[('num', StandardScaler(),\n                                                  ['visits', 'visitors',\n                                                   'transactions', 'spend',\n                                                   'visits_k3', 'visitors_k3',\n                                                   'transactions_k3']),\n                                                 ('cat',\n                                                  OneHotEncoder(handle_unknown='ignore'),\n                                                  ['TOP_CATEGORY'])])),\n                ('randomforestregressor',\n                 RandomForestRegressor(random_state=7))])columntransformer: ColumnTransformerColumnTransformer(transformers=[('num', StandardScaler(),\n                                 ['visits', 'visitors', 'transactions', 'spend',\n                                  'visits_k3', 'visitors_k3',\n                                  'transactions_k3']),\n                                ('cat', OneHotEncoder(handle_unknown='ignore'),\n                                 ['TOP_CATEGORY'])])num['visits', 'visitors', 'transactions', 'spend', 'visits_k3', 'visitors_k3', 'transactions_k3']StandardScalerStandardScaler()cat['TOP_CATEGORY']OneHotEncoderOneHotEncoder(handle_unknown='ignore')RandomForestRegressorRandomForestRegressor(random_state=7)\n\n\n\n\nCode\n# Get the best hyperparameters\nbest_hyperparameters_3 = grid_search.best_params_\n\n# Evaluate on test set\nmodel_3 = grid_search.best_estimator_\ntest_score_3 = model_3.score(test_set, y_test)\n\n\n\n\nModel 4: with Spatial Features of 5 Nearest Neighbours\nFor the fourth model, I will use the data with spatial features of 5 nearest neighbor. The rest of the steps are the same as the previous models.\n\n\nCode\n# Numerical columns\nnum_cols = [\n    \"visits\",\n    \"visitors\",\n    \"transactions\",\n    \"spend\",\n    \"visits_k5\",\n    \"visitors_k5\",\n    \"transactions_k5\",\n]\n\n# Categorical columns\ncat_cols = [\n    \"TOP_CATEGORY\"\n]\n\n# Set up the column transformer with two transformers\n# ----&gt; Scale the numerical columns\n# ----&gt; One-hot encode the categorical columns\n\ntransformer = ColumnTransformer(\n    transformers=[\n        (\"num\", StandardScaler(), num_cols),\n        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), cat_cols),\n    ]\n)\n\n# Initialize the pipeline\npipe = make_pipeline(\n    transformer, RandomForestRegressor(random_state=7)\n)\n\n# Use GridSearchCV to find the best hyperparameters\n# Set up GridSearchCV\nparam_grid = {\n    'randomforestregressor__n_estimators': [50, 100, 200],\n    'randomforestregressor__max_depth': [20, 50, None],\n    'randomforestregressor__min_samples_split': [2, 4, 6]\n}\n\n# Set up the grid search\ngrid_search = GridSearchCV(\n    pipe, \n    param_grid, \n    cv=5, # 5-fold cross-validation\n    scoring='r2', # Use R^2 as the scoring metric\n    n_jobs=-1 # Use all available cpu cores\n    )\n\n# Fit the model\ngrid_search.fit(train_set, y_train)\n\n\nGridSearchCV(cv=5,\n             estimator=Pipeline(steps=[('columntransformer',\n                                        ColumnTransformer(transformers=[('num',\n                                                                         StandardScaler(),\n                                                                         ['visits',\n                                                                          'visitors',\n                                                                          'transactions',\n                                                                          'spend',\n                                                                          'visits_k5',\n                                                                          'visitors_k5',\n                                                                          'transactions_k5']),\n                                                                        ('cat',\n                                                                         OneHotEncoder(handle_unknown='ignore'),\n                                                                         ['TOP_CATEGORY'])])),\n                                       ('randomforestregressor',\n                                        RandomForestRegressor(random_state=7))]),\n             n_jobs=-1,\n             param_grid={'randomforestregressor__max_depth': [20, 50, None],\n                         'randomforestregressor__min_samples_split': [2, 4, 6],\n                         'randomforestregressor__n_estimators': [50, 100, 200]},\n             scoring='r2')In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GridSearchCVGridSearchCV(cv=5,\n             estimator=Pipeline(steps=[('columntransformer',\n                                        ColumnTransformer(transformers=[('num',\n                                                                         StandardScaler(),\n                                                                         ['visits',\n                                                                          'visitors',\n                                                                          'transactions',\n                                                                          'spend',\n                                                                          'visits_k5',\n                                                                          'visitors_k5',\n                                                                          'transactions_k5']),\n                                                                        ('cat',\n                                                                         OneHotEncoder(handle_unknown='ignore'),\n                                                                         ['TOP_CATEGORY'])])),\n                                       ('randomforestregressor',\n                                        RandomForestRegressor(random_state=7))]),\n             n_jobs=-1,\n             param_grid={'randomforestregressor__max_depth': [20, 50, None],\n                         'randomforestregressor__min_samples_split': [2, 4, 6],\n                         'randomforestregressor__n_estimators': [50, 100, 200]},\n             scoring='r2')estimator: PipelinePipeline(steps=[('columntransformer',\n                 ColumnTransformer(transformers=[('num', StandardScaler(),\n                                                  ['visits', 'visitors',\n                                                   'transactions', 'spend',\n                                                   'visits_k5', 'visitors_k5',\n                                                   'transactions_k5']),\n                                                 ('cat',\n                                                  OneHotEncoder(handle_unknown='ignore'),\n                                                  ['TOP_CATEGORY'])])),\n                ('randomforestregressor',\n                 RandomForestRegressor(random_state=7))])columntransformer: ColumnTransformerColumnTransformer(transformers=[('num', StandardScaler(),\n                                 ['visits', 'visitors', 'transactions', 'spend',\n                                  'visits_k5', 'visitors_k5',\n                                  'transactions_k5']),\n                                ('cat', OneHotEncoder(handle_unknown='ignore'),\n                                 ['TOP_CATEGORY'])])num['visits', 'visitors', 'transactions', 'spend', 'visits_k5', 'visitors_k5', 'transactions_k5']StandardScalerStandardScaler()cat['TOP_CATEGORY']OneHotEncoderOneHotEncoder(handle_unknown='ignore')RandomForestRegressorRandomForestRegressor(random_state=7)\n\n\n\n\nCode\n# Get the best hyperparameters\nbest_hyperparameters_4 = grid_search.best_params_\n\n# Evaluate on test set\nmodel_4 = grid_search.best_estimator_\ntest_score_4 = model_4.score(test_set, y_test)",
    "crumbs": [
      "Analysis",
      "Modeling"
    ]
  },
  {
    "objectID": "analysis/exercise.html",
    "href": "analysis/exercise.html",
    "title": "Exercise on Predictive modeling: Housing price modeling",
    "section": "",
    "text": "from matplotlib import pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport geopandas as gpd\nimport requests\nimport hvplot.pandas\n\nnp.random.seed(42)\npd.options.display.max_columns = 999"
  },
  {
    "objectID": "analysis/exercise.html#how-to-handle-categorical-data",
    "href": "analysis/exercise.html#how-to-handle-categorical-data",
    "title": "Exercise on Predictive modeling: Housing price modeling",
    "section": "How to handle categorical data?",
    "text": "How to handle categorical data?\nWe can use a technique called one-hot encoding  example here: ZIP code\nSteps: - Create a new column for each category - Represent each category as a vector of 1s and 0s"
  },
  {
    "objectID": "analysis/exercise.html#one-hot-encoding-in-scikit-learn",
    "href": "analysis/exercise.html#one-hot-encoding-in-scikit-learn",
    "title": "Exercise on Predictive modeling: Housing price modeling",
    "section": "One-hot encoding in scikit learn",
    "text": "One-hot encoding in scikit learn\n\nThe OneHotEncoder object is a preprocessor that will perform the vectorization step\nThe ColumnTransformer object will help us apply different transformers to numerical and categorical columns\n\n\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder\n\nLet’s try out the example data of colors:\n\n# Example data of colors\ncolors = np.array([\"red\", \"green\", \"blue\", \"red\"])\ncolors = colors[:, np.newaxis]\n\n\ncolors.shape\n\n(4, 1)\n\n\n\ncolors\n\narray([['red'],\n       ['green'],\n       ['blue'],\n       ['red']], dtype='&lt;U5')\n\n\n\n# Initialize the OHE transformer\nohe = OneHotEncoder()\n\n# Fit the transformer and then transform the colors \nohe.fit_transform(colors).toarray()\n\narray([[0., 0., 1.],\n       [0., 1., 0.],\n       [1., 0., 0.],\n       [0., 0., 1.]])\n\n\n\n# The corresponding category for each column\nohe.categories_\n\n[array(['blue', 'green', 'red'], dtype='&lt;U5')]\n\n\nLet’s apply separate transformers for our numerical and categorical columns:\n\n# Numerical columns\nnum_cols = [\n    \"total_livable_area\",\n    \"total_area\",\n    \"garage_spaces\",\n    \"fireplaces\",\n    \"number_of_bathrooms\",\n    \"number_of_bedrooms\",\n    \"number_stories\",\n]\n\n# Categorical columns\ncat_cols = [\"exterior_condition\", \"zip_code\"]\n\n\n# Set up the column transformer with two transformers\n# ----&gt; Scale the numerical columns\n# ----&gt; One-hot encode the categorical columns\n\ntransformer = ColumnTransformer(\n    transformers=[\n        (\"num\", StandardScaler(), num_cols),\n        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), cat_cols),\n    ]\n)\n\nNote: the handle_unknown='ignore' parameter ensures that if categories show up in our training set, but not our test set, no error will be raised.\nInitialize the pipeline object, using the column transformer and the random forest regressor\n\n# Initialize the pipeline\n# NOTE: only use 10 estimators here so it will run in a reasonable time\npipe = make_pipeline(\n    transformer, RandomForestRegressor(n_estimators=30, \n                                       random_state=42)\n)\n\nNow, let’s fit the model.\n\nImportant!\n\nYou must pass in the full training set and test set DataFrames: train_set and test_set\nNo need to create the X_train and X_test numpy arrays.\nWe told scikit learn which column strings to extract in the ColumnTransformer, so it needs the DataFrame with named columns.\n\n\n# Fit the training set\npipe.fit(train_set, y_train);\n\n\n# What's the test score?\npipe.score(test_set, y_test)\n\n0.5687023191225087\n\n\n\n\nSubstantial improvement on test set when including ZIP codes\nR-squared of ~0.30 improved to R-squared of ~0.54!\nTakeaway: neighborhood based effects play a crucial role in determining housing prices.\nSide Note: to fully validate the model we should run k-fold cross validation and optimize hyperparameters of the model as well…\n\n\nBut how crucial? Let’s plot the importances\nBut first, we need to know the column names! The one-hot encoder created a column for each category type…\n\n# The column transformer...\ntransformer\n\nColumnTransformer(transformers=[('num', StandardScaler(),\n                                 ['total_livable_area', 'total_area',\n                                  'garage_spaces', 'fireplaces',\n                                  'number_of_bathrooms', 'number_of_bedrooms',\n                                  'number_stories']),\n                                ('cat', OneHotEncoder(handle_unknown='ignore'),\n                                 ['exterior_condition', 'zip_code'])])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.ColumnTransformerColumnTransformer(transformers=[('num', StandardScaler(),\n                                 ['total_livable_area', 'total_area',\n                                  'garage_spaces', 'fireplaces',\n                                  'number_of_bathrooms', 'number_of_bedrooms',\n                                  'number_stories']),\n                                ('cat', OneHotEncoder(handle_unknown='ignore'),\n                                 ['exterior_condition', 'zip_code'])])num['total_livable_area', 'total_area', 'garage_spaces', 'fireplaces', 'number_of_bathrooms', 'number_of_bedrooms', 'number_stories']StandardScalerStandardScaler()cat['exterior_condition', 'zip_code']OneHotEncoderOneHotEncoder(handle_unknown='ignore')\n\n\n\n# The steps in the column transformer\ntransformer.named_transformers_\n\n{'num': StandardScaler(),\n 'cat': OneHotEncoder(handle_unknown='ignore'),\n 'remainder': 'drop'}\n\n\n\n# The one-hot step\nohe = transformer.named_transformers_['cat']\n\nohe\n\nOneHotEncoder(handle_unknown='ignore')In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  OneHotEncoder?Documentation for OneHotEncoderiFittedOneHotEncoder(handle_unknown='ignore') \n\n\n\n# One column for each category type!\nohe_cols = ohe.get_feature_names_out()\n\nohe_cols\n\narray(['exterior_condition_0', 'exterior_condition_1',\n       'exterior_condition_2', 'exterior_condition_3',\n       'exterior_condition_4', 'exterior_condition_5',\n       'exterior_condition_6', 'exterior_condition_7', 'zip_code_19102',\n       'zip_code_19103', 'zip_code_19104', 'zip_code_19106',\n       'zip_code_19107', 'zip_code_19111', 'zip_code_19114',\n       'zip_code_19115', 'zip_code_19116', 'zip_code_19118',\n       'zip_code_19119', 'zip_code_19120', 'zip_code_19121',\n       'zip_code_19122', 'zip_code_19123', 'zip_code_19124',\n       'zip_code_19125', 'zip_code_19126', 'zip_code_19127',\n       'zip_code_19128', 'zip_code_19129', 'zip_code_19130',\n       'zip_code_19131', 'zip_code_19132', 'zip_code_19133',\n       'zip_code_19134', 'zip_code_19135', 'zip_code_19136',\n       'zip_code_19137', 'zip_code_19138', 'zip_code_19139',\n       'zip_code_19140', 'zip_code_19141', 'zip_code_19142',\n       'zip_code_19143', 'zip_code_19144', 'zip_code_19145',\n       'zip_code_19146', 'zip_code_19147', 'zip_code_19148',\n       'zip_code_19149', 'zip_code_19150', 'zip_code_19151',\n       'zip_code_19152', 'zip_code_19153', 'zip_code_19154'], dtype=object)\n\n\n\n# Full list of columns is numerical + one-hot \nfeatures = num_cols + list(ohe_cols)\n\nfeatures\n\n['total_livable_area',\n 'total_area',\n 'garage_spaces',\n 'fireplaces',\n 'number_of_bathrooms',\n 'number_of_bedrooms',\n 'number_stories',\n 'exterior_condition_0',\n 'exterior_condition_1',\n 'exterior_condition_2',\n 'exterior_condition_3',\n 'exterior_condition_4',\n 'exterior_condition_5',\n 'exterior_condition_6',\n 'exterior_condition_7',\n 'zip_code_19102',\n 'zip_code_19103',\n 'zip_code_19104',\n 'zip_code_19106',\n 'zip_code_19107',\n 'zip_code_19111',\n 'zip_code_19114',\n 'zip_code_19115',\n 'zip_code_19116',\n 'zip_code_19118',\n 'zip_code_19119',\n 'zip_code_19120',\n 'zip_code_19121',\n 'zip_code_19122',\n 'zip_code_19123',\n 'zip_code_19124',\n 'zip_code_19125',\n 'zip_code_19126',\n 'zip_code_19127',\n 'zip_code_19128',\n 'zip_code_19129',\n 'zip_code_19130',\n 'zip_code_19131',\n 'zip_code_19132',\n 'zip_code_19133',\n 'zip_code_19134',\n 'zip_code_19135',\n 'zip_code_19136',\n 'zip_code_19137',\n 'zip_code_19138',\n 'zip_code_19139',\n 'zip_code_19140',\n 'zip_code_19141',\n 'zip_code_19142',\n 'zip_code_19143',\n 'zip_code_19144',\n 'zip_code_19145',\n 'zip_code_19146',\n 'zip_code_19147',\n 'zip_code_19148',\n 'zip_code_19149',\n 'zip_code_19150',\n 'zip_code_19151',\n 'zip_code_19152',\n 'zip_code_19153',\n 'zip_code_19154']\n\n\n\nrandom_forest = pipe[\"randomforestregressor\"]\n\n# Create the dataframe with importances\nimportance = pd.DataFrame(\n    {\"Feature\": features, \"Importance\": random_forest.feature_importances_}\n)\n\n\nimportance.head(n=20)\n\n\n\n\n\n\n\n\nFeature\nImportance\n\n\n\n\n0\ntotal_livable_area\n0.177882\n\n\n1\ntotal_area\n0.231340\n\n\n2\ngarage_spaces\n0.010044\n\n\n3\nfireplaces\n0.001861\n\n\n4\nnumber_of_bathrooms\n0.161817\n\n\n5\nnumber_of_bedrooms\n0.052586\n\n\n6\nnumber_stories\n0.022008\n\n\n7\nexterior_condition_0\n0.000013\n\n\n8\nexterior_condition_1\n0.001369\n\n\n9\nexterior_condition_2\n0.004490\n\n\n10\nexterior_condition_3\n0.009052\n\n\n11\nexterior_condition_4\n0.011657\n\n\n12\nexterior_condition_5\n0.014794\n\n\n13\nexterior_condition_6\n0.005667\n\n\n14\nexterior_condition_7\n0.013110\n\n\n15\nzip_code_19102\n0.000312\n\n\n16\nzip_code_19103\n0.002195\n\n\n17\nzip_code_19104\n0.005277\n\n\n18\nzip_code_19106\n0.001260\n\n\n19\nzip_code_19107\n0.000755\n\n\n\n\n\n\n\n\n# Sort by importance and get the top 30\n# SORT IN DESCENDING ORDER\nimportance = importance.sort_values(\"Importance\", ascending=False).iloc[:30]\n\n# Plot\nimportance.hvplot.barh(x=\"Feature\", y=\"Importance\", height=700, flip_yaxis=True)\n\n\n\n\n\n  \n\n\n\n\n\n\nTakeaways\n\nNumber of bathrooms and area-based features still important\nZIP codes in North Philadelphia also important: 19140, 19132, 19134\n\nInterpretation\nThese North Philadelphia ZIP codes have some of the lowest valued homes in the city, which are inherently the most difficult to model accurately. It makes sense when included ZIP code information that these areas would be the most to improve."
  },
  {
    "objectID": "analysis/exercise.html#why-is-feature-engineering-so-important",
    "href": "analysis/exercise.html#why-is-feature-engineering-so-important",
    "title": "Exercise on Predictive modeling: Housing price modeling",
    "section": "Why is feature engineering so important?",
    "text": "Why is feature engineering so important?\nGarbage in, garbage out\n\nWhat we’re trying to do is build the best possible model for a particular thing we care about, e.g., housing price, bikeshare trips, etc\nOur machine learning models try to translate from some set of input features to the thing we care about\nYou should think of the input features as having all of the same information as the predicted quantity — they are just a different representation\n\nTakeway: If your input features are poorly designed (for example, completely unrelated to thing you want to predict), then no matter how good your machine learning model is or how well you “train” it, then the model will never be able to do the translation from features to predicted value."
  },
  {
    "objectID": "analysis/exercise.html#adding-spatial-features-to-the-housing-price-model",
    "href": "analysis/exercise.html#adding-spatial-features-to-the-housing-price-model",
    "title": "Exercise on Predictive modeling: Housing price modeling",
    "section": "Adding spatial features to the housing price model",
    "text": "Adding spatial features to the housing price model\n\nAdding in ZIP code information captures a lot of the neighborhood-based amenity/disamenity properties\nCan we explicitly add new features that also try to capture some of those features?\n\nYes, let’s add distance-based features"
  },
  {
    "objectID": "analysis/exercise.html#spatial-amenitydisamenity-features",
    "href": "analysis/exercise.html#spatial-amenitydisamenity-features",
    "title": "Exercise on Predictive modeling: Housing price modeling",
    "section": "Spatial amenity/disamenity features",
    "text": "Spatial amenity/disamenity features\nThe strategy\n\nGet the data for a certain type of amenity, e.g., restaurants, bars, or disamenity, e.g., crimes\n\nData sources: 311 requests, crime incidents, Open Street Map\n\nUse scikit learn’s nearest neighbor algorithm to calculate the distance from each sale to its nearest neighbor in the amenity/disamenity datasets"
  },
  {
    "objectID": "analysis/exercise.html#examples-of-new-possible-features",
    "href": "analysis/exercise.html#examples-of-new-possible-features",
    "title": "Exercise on Predictive modeling: Housing price modeling",
    "section": "Examples of new possible features…",
    "text": "Examples of new possible features…\nDistance from each sale to:\n\nUniversities\nParks\nCity Hall\nSubway Stops\nNew Construction Permits\nAggravated Assaults\nGraffiti 311 Calls\nAbandoned Vehicle 311 Calls\n\n\nExample #1: 311 Graffiti Calls\nSource: https://www.opendataphilly.org/dataset/311-service-and-information-requests\nMetadata\n\nStep 1: Download the data from the CARTO database\nWe’ll only pull data from 2022.\nLet’s make a utility function to download data for a specific table and where statement from CARTO:\n\ndef get_carto_data(table_name, where=None, limit=None):\n    \"\"\"\n    Download data from CARTO given a specific table name and\n    optionally a where statement or limit.\n    \"\"\"\n\n    # the CARTO API url\n    carto_url = \"https://phl.carto.com/api/v2/sql\"\n\n    # Create the query\n    query = f\"SELECT * FROM {table_name}\"\n\n    # Add a where\n    if where is not None:\n        query = query + f\" WHERE {where}\"\n\n    # Add a limit\n    if limit is not None:\n        query = query + f\" LIMIT {limit}\"\n\n    # Make the request\n    params = {\"q\": query, \"format\": \"geojson\"}\n    response = requests.get(carto_url, params=params)\n\n    # Make the GeoDataFrame\n    return gpd.GeoDataFrame.from_features(response.json(), crs=\"EPSG:4326\")\n\nLet’s take a peak at the first row:\n\n# the 311 table\ntable_name = \"public_cases_fc\"\n\nget_carto_data(table_name, limit=1)\n\n\n\n\n\n\n\n\ngeometry\ncartodb_id\nobjectid\nservice_request_id\nsubject\nstatus\nstatus_notes\nservice_name\nservice_code\nagency_responsible\nservice_notice\nrequested_datetime\nupdated_datetime\nexpected_datetime\nclosed_datetime\naddress\nzipcode\nmedia_url\nlat\nlon\n\n\n\n\n0\nPOINT (-75.15416 39.96156)\n1\n1\n1960\nStreet Light Outage\nClosed\nOther\nStreet Light Outage\nSR-ST04\nStreets Department\nNone\n2014-05-23T13:23:57Z\n2024-05-10T18:43:50Z\n2014-06-09T00:00:00Z\n2017-06-07T18:10:04Z\n990 SPRING GARDEN ST\n19123\nNone\n39.961564\n-75.154156\n\n\n\n\n\n\n\nLet’s build our where clause based on the ‘requested_datetime’\n\n# Select only those for grafitti and in 2022\nwhere = \"requested_datetime &gt;= '01-01-2022' and requested_datetime &lt; '01-01-2023'\"\nwhere = where + \" and service_name = 'Graffiti Removal'\"\n\n# Pull the subset we want\ngraffiti = get_carto_data(table_name, where=where)\n\n\n# Remove rows with empty or NaN geometries\nnot_missing = (~graffiti.geometry.is_empty) & (graffiti.geometry.notna())\ngraffiti = graffiti.loc[not_missing]\n\n\nlen(graffiti)\n\n16281\n\n\n\ngraffiti.head()\n\n\n\n\n\n\n\n\ngeometry\ncartodb_id\nobjectid\nservice_request_id\nsubject\nstatus\nstatus_notes\nservice_name\nservice_code\nagency_responsible\nservice_notice\nrequested_datetime\nupdated_datetime\nexpected_datetime\nclosed_datetime\naddress\nzipcode\nmedia_url\nlat\nlon\n\n\n\n\n0\nPOINT (-75.14570 39.95761)\n4038081\n4044086\n14650458\nGraffiti Removal\nClosed\nIssue Resolved\nGraffiti Removal\nSR-CL01\nCommunity Life Improvement Program\n7 Business Days\n2022-01-03T23:03:07Z\n2024-05-11T00:44:42Z\n2022-01-13T00:00:00Z\n2022-01-07T13:30:02Z\nCALLOWHILL ST & N 4TH ST\n19123\nhttps://d17aqltn7cihbm.cloudfront.net/uploads/...\n39.957610\n-75.145700\n\n\n1\nPOINT (-75.22123 39.97351)\n4038092\n4044098\n14650473\nGraffiti Removal\nClosed\nIssue Resolved\nGraffiti Removal\nSR-CL01\nCommunity Life Improvement Program\n7 Business Days\n2022-01-03T23:19:26Z\n2023-02-16T03:01:28Z\n2022-01-13T00:00:00Z\n2022-01-06T14:10:58Z\n4945 W THOMPSON ST\n19131\nNone\n39.973507\n-75.221235\n\n\n2\nPOINT (-75.16333 39.94261)\n4038224\n4044230\n14650631\nGraffiti Removal\nClosed\nIssue Resolved\nGraffiti Removal\nSR-CL01\nCommunity Life Improvement Program\n7 Business Days\n2022-01-04T02:39:53Z\n2023-02-16T02:54:49Z\n2022-01-13T00:00:00Z\n2022-01-06T13:48:15Z\n629 S 13TH ST\n19147\nhttps://d17aqltn7cihbm.cloudfront.net/uploads/...\n39.942605\n-75.163325\n\n\n3\nPOINT (-75.15956 39.94604)\n4038227\n4044233\n14650638\nGraffiti Removal\nClosed\nIssue Resolved\nGraffiti Removal\nSR-CL01\nCommunity Life Improvement Program\n7 Business Days\n2022-01-04T02:58:24Z\n2023-02-16T03:02:18Z\n2022-01-13T00:00:00Z\n2022-01-06T14:05:01Z\n304 S 10TH ST\n19107\nhttps://d17aqltn7cihbm.cloudfront.net/uploads/...\n39.946044\n-75.159564\n\n\n4\nPOINT (-75.13272 39.99284)\n4038246\n4044252\n14650677\nGraffiti Removal\nClosed\nOther\nGraffiti Removal\nSR-CL01\nCommunity Life Improvement Program\n7 Business Days\n2022-01-04T09:32:56Z\n2023-02-16T03:00:40Z\n2022-01-13T00:00:00Z\n2022-01-04T14:06:36Z\n2800 MASCHER ST\n19133\nNone\n39.992840\n-75.132723\n\n\n\n\n\n\n\n\n\nStep 2: Get the x/y coordinates of both datasets\nWe will need to:\n\nWe’ll want distances in meters (rather than degrees), so we’ll convert the CRS to EPSG=3857\nExtract out the x/y coordinates of the geometry column of each dataset (sales and grafitti calls)\n\n\n# Do the CRS conversion\nsales_3857 = sales.to_crs(epsg=3857)\ngraffiti_3857 = graffiti.to_crs(epsg=3857)\n\n\ndef get_xy_from_geometry(df):\n    \"\"\"\n    Return a numpy array with two columns, where the \n    first holds the `x` geometry coordinate and the second \n    column holds the `y` geometry coordinate\n    \"\"\"\n    x = df.geometry.x\n    y = df.geometry.y\n    \n    return np.column_stack((x, y)) # stack as columns\n\n\n# Extract x/y for sales\nsalesXY = get_xy_from_geometry(sales_3857)\n\n# Extract x/y for grafitti calls\ngraffitiXY = get_xy_from_geometry(graffiti_3857)\n\n\nsalesXY.shape\n\n(17763, 2)\n\n\n\ngraffitiXY.shape\n\n(16281, 2)\n\n\n\n\nStep 3: Calculate the nearest neighbor distances\nFor this, we will use the k-nearest neighbors algorithm from scikit learn.\nFor each sale: - Find the k-nearest neighbors in the second dataset (graffiti calls, crimes, etc) - Calculate the average distance from the sale to those k-neighbors\n\nfrom sklearn.neighbors import NearestNeighbors\n\n\n# STEP 1: Initialize the algorithm\nk = 5\nnbrs = NearestNeighbors(n_neighbors=k)\n\n# STEP 2: Fit the algorithm on the \"neighbors\" dataset\nnbrs.fit(graffitiXY)\n\n# STEP 3: Get distances for sale to neighbors\ngrafDists, grafIndices = nbrs.kneighbors(salesXY) \n\nNote: I am using k=5 here without any real justification. In practice, you would want to try a few different k values to try to identify the best value to use.\n\n\nWhat did we just calculate?\n\ngrafDists: For each sale, the distances to the 5 nearest graffiti calls\n\nThis should have 5 columns and the same length as the sales dataset\n\ngrafIndices: For each sale, the index of each of the 5 neighbors in the original dataset\n\nThis allows you to access the original 311 graffiti data\n\n\n\nprint(\"length of sales = \", len(salesXY))\nprint(\"shape of grafDists = \", grafDists.shape)\nprint(\"shape of grafIndices = \", grafIndices.shape)\n\nlength of sales =  17763\nshape of grafDists =  (17763, 5)\nshape of grafIndices =  (17763, 5)\n\n\n\n# The distances from the first sale to the 5 nearest neighbors\ngrafDists[0]\n\narray([ 74.69196533, 109.96264648, 132.79982755, 134.17262561,\n       174.53660354])\n\n\n\ngrafIndices[0]\n\narray([ 3816,  5779, 13408,  1139,  7146])\n\n\n\n\nCan we reproduce these distances?\n\nsalesXY[0]\n\narray([-8365504.33110536,  4855986.06521187])\n\n\n\n# The coordinates for the first sale\nx0, y0 = salesXY[0]\nx0, y0\n\n(-8365504.331105363, 4855986.065211866)\n\n\n\n# The indices for the 5 nearest graffiti calls\ngrafIndices[0]\n\narray([ 3816,  5779, 13408,  1139,  7146])\n\n\n\n# the graffiti neighbors\nsale0_neighbors = graffitiXY[grafIndices[0]]\nsale0_neighbors\n\narray([[-8365555.31543214,  4856040.64989935],\n       [-8365425.62822537,  4856062.86130748],\n       [-8365594.27725392,  4856083.76620754],\n       [-8365636.57866042,  4856008.71201389],\n       [-8365406.14731448,  4856130.36687223]])\n\n\n\n# Access the first and second column for x/y values\nneighbors_x = sale0_neighbors[:,0]\nneighbors_y = sale0_neighbors[:,1]\n\n# The x/y differences between neighbors and first sale coordinates\ndx = (neighbors_x - x0)\ndy = (neighbors_y - y0)\n\n# The Euclidean dist\nmanual_dists = (dx**2 + dy**2) ** 0.5\n\n\nmanual_dists\n\narray([ 74.69196533, 109.96264648, 132.79982755, 134.17262561,\n       174.53660354])\n\n\n\ngrafDists[0]\n\narray([ 74.69196533, 109.96264648, 132.79982755, 134.17262561,\n       174.53660354])\n\n\n\n\nUse the log of the average distance as the new feature\nWe’ll average over the column axis: axis=1\n\ngrafDists.mean(axis=1)\n\narray([125.2327337 , 164.21662169, 156.49823723, ..., 614.26510679,\n       614.26510679, 614.26510679])\n\n\n\n# Average distance to neighbors\navgGrafDist = grafDists.mean(axis=1)\n\n# Set zero distances to be small, but nonzero\n# IMPORTANT: THIS WILL AVOID INF DISTANCES WHEN DOING THE LOG\navgGrafDist[avgGrafDist==0] = 1e-5\n\n# Calculate log of distances\nsales['logDistGraffiti'] = np.log10(avgGrafDist)\n\n\nsales.head()\n\n\n\n\n\n\n\n\nsale_price\ntotal_livable_area\ntotal_area\ngarage_spaces\nfireplaces\nnumber_of_bathrooms\nnumber_of_bedrooms\nnumber_stories\nexterior_condition\nzip_code\ngeometry\nlogDistGraffiti\n\n\n\n\n2587\n450000.0\n1785.0\n1625.0\n1.0\n0.0\n2.0\n3.0\n3.0\n4\n19147\nPOINT (-75.14860 39.93145)\n2.097718\n\n\n11836\n670000.0\n2244.0\n1224.0\n0.0\n0.0\n3.0\n4.0\n3.0\n3\n19147\nPOINT (-75.14817 39.93101)\n2.215417\n\n\n9533\n790000.0\n2514.0\n1400.0\n1.0\n0.0\n0.0\n3.0\n2.0\n3\n19147\nPOINT (-75.14781 39.93010)\n2.194509\n\n\n2192\n195000.0\n1358.0\n840.0\n0.0\n0.0\n2.0\n3.0\n3.0\n4\n19147\nPOINT (-75.14887 39.93026)\n2.212881\n\n\n13305\n331000.0\n868.0\n546.0\n0.0\n0.0\n2.0\n2.0\n2.0\n4\n19147\nPOINT (-75.14881 39.93012)\n2.206706\n\n\n\n\n\n\n\n\n\nLet’s plot a hex map of the new feature!\n\n# Load the City Limits from OpenDataPhilly's page\nurl = \"https://opendata.arcgis.com/datasets/405ec3da942d4e20869d4e1449a2be48_0.geojson\"\ncity_limits = gpd.read_file(url).to_crs(epsg=3857)\n\n\nimport matplotlib.pyplot as plt\n\nfig, ax = plt.subplots(figsize=(7, 7), facecolor=plt.get_cmap(\"viridis\")(30))\n\n# Plot the log of the Graffiti distance\nx = salesXY[:, 0]\ny = salesXY[:, 1]\nhb = ax.hexbin(x, y, C=sales[\"logDistGraffiti\"].values, gridsize=60, cmap=\"viridis\")\n\n# Add color bar for the hexbin plot\ncb = plt.colorbar(hb, ax=ax, orientation=\"vertical\")\ncb.set_label(\"Log of Graffiti Distance\", color=\"white\")  # Set colorbar label to white\n\n# Change color of the colorbar ticks\ncb.ax.yaxis.set_tick_params(color=\"white\")  # Color of tick marks\ncb.ax.yaxis.set_tick_params(labelcolor=\"white\")  # Color of tick labels\n\n# Change the spine color of the colorbar to white\ncb.outline.set_edgecolor(\"white\")\n\n# Plot the city limits\ncity_limits.plot(ax=ax, facecolor=\"none\", edgecolor=\"white\", linewidth=4)\n\n# Set axis off\nax.set_axis_off()\nax.set_aspect(\"equal\")\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nExample #2: Subway stops\nUse the osmnx package to get subway stops in Philly — we can use the ox.geometries_from_polygon() function.\n\nTo select subway stations, we can use station=subway: see the OSM Wikipedia\nSee Lecture 5A for a reminder on osmnx!\n\n\nimport osmnx as ox\n\n\ncity_limits.to_crs(epsg=4326).squeeze().geometry\n\n\n\n\n\n\n\n\n\n# Get the geometry from the city limits\ncity_limits_outline = city_limits.to_crs(epsg=4326).squeeze().geometry\n\ncity_limits_outline\n\n\n\n\n\n\n\n\n\n# Get the subway stops within the city limits\nsubway = ox.features_from_polygon(city_limits_outline, tags={\"station\": \"subway\"})\n\n# Convert to 3857 (meters)\nsubway = subway.to_crs(epsg=3857)\n\nsubway.head(n=10)\n\n\n\n\n\n\n\n\n\naddr:city\naddr:state\nname\nnetwork\noperator\nplatforms\npublic_transport\nrailway\nstation\nsubway\nwheelchair\nwikidata\nwikipedia\ngeometry\naddr:postcode\nnetwork:wikidata\ntrain\naddr:housenumber\naddr:street\nrailway:position\ninternet_access\nname:en\nold_name\nshort_name\noperator:wikidata\nelevator\ntram\n\n\nelement_type\nosmid\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nnode\n469917297\nPhiladelphia\nPA\n15th-16th & Locust\nPATCO\nPATCO\n1\nstation\nstation\nsubway\nyes\nyes\nQ4551078\nen:15–16th & Locust station\nPOINT (-8367552.610 4858465.747)\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n469917298\nPhiladelphia\nNaN\n9th-10th & Locust\nPATCO\nPATCO\n1\nstation\nstation\nsubway\nyes\nyes\nQ4646737\nen:9–10th & Locust station\nPOINT (-8366424.042 4858281.683)\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n471026103\nPhiladelphia\nNaN\n12th-13th & Locust\nPATCO\nPATCO\n1\nstation\nstation\nsubway\nyes\nno\nQ4548965\nen:12–13th & Locust station\nPOINT (-8366949.703 4858366.817)\n19107\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n650938316\nNaN\nNaN\n63rd Street\nSEPTA\nSEPTA\nNaN\nstation\nstation\nsubway\nyes\nNaN\nNaN\nNaN\nPOINT (-8376424.717 4860524.238)\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n650959043\nNaN\nNaN\n56th Street\nSEPTA\nSEPTA\nNaN\nstation\nstation\nsubway\nyes\nNaN\nQ4640769\nNaN\nPOINT (-8374883.844 4860274.795)\nNaN\nQ2037863\nyes\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n650960111\nNaN\nNaN\n46th Street\nNaN\nSEPTA\nNaN\nstation\nstation\nsubway\nyes\nyes\nNaN\nNaN\nPOINT (-8372784.826 4859935.838)\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n775915931\nPhiladelphia\nNaN\nLombard-South\nSEPTA\nSEPTA\n1\nstation\nstation\nsubway\nyes\nno\nQ6669414\nen:Lombard–South station\nPOINT (-8367373.508 4857829.684)\nNaN\nNaN\nNaN\n500\nSouth Broad Street\nmi:7.40\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n775915932\nNaN\nNaN\nEllsworth-Federal\nSEPTA\nSEPTA\n1\nstation\nstation\nsubway\nyes\nno\nQ11681426\nen:Ellsworth–Federal station\nPOINT (-8367565.011 4856679.778)\nNaN\nNaN\nNaN\n1200\nSouth Broad Street\nmi:7.95\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n775915933\nPhiladelphia\nNaN\nTasker-Morris\nSEPTA\nSEPTA\n1\nstation\nstation\nsubway\nyes\nno\nQ7687362\nen:Tasker–Morris station\nPOINT (-8367718.220 4855760.268)\nNaN\nNaN\nNaN\nNaN\nNaN\nmi:8.40\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n775915935\nPhiladelphia\nNaN\nSnyder\nSEPTA\nSEPTA\n1\nstation\nstation\nsubway\nyes\nNaN\nQ7548885\nen:Snyder station\nPOINT (-8367839.558 4854864.750)\nNaN\nNaN\nNaN\nNaN\nNaN\nmi:8.78\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n\n\n\n\n\n\nfig, ax = plt.subplots(figsize=(6,6))\n\n# Plot the subway locations\nsubway.plot(ax=ax, markersize=10, color='crimson')\n\n# City limits, too\ncity_limits.plot(ax=ax, facecolor='none', edgecolor='black', linewidth=4)\n\nax.set_axis_off()\n\n\n\n\n\n\n\n\nThe stops on the Market-Frankford and Broad St. subway lines!\n\nNow, get the distances to the nearest subway stop\nWe’ll use k=1 to get the distance to the nearest stop.\n\n# STEP 1: x/y coordinates of subway stops (in EPGS=3857)\nsubwayXY = get_xy_from_geometry(subway.to_crs(epsg=3857))\n\n# STEP 2: Initialize the algorithm\nnbrs = NearestNeighbors(n_neighbors=1)\n\n# STEP 3: Fit the algorithm on the \"neighbors\" dataset\nnbrs.fit(subwayXY)\n\n# STEP 4: Get distances for sale to neighbors\nsubwayDists, subwayIndices = nbrs.kneighbors(salesXY)\n\n# STEP 5: add back to the original dataset\nsales[\"logDistSubway\"] = np.log10(subwayDists.mean(axis=1))\n\n\n\nLet’s plot a hex map again!\n\n\n\nfig, ax = plt.subplots(figsize=(6,6), facecolor=plt.get_cmap('viridis')(30))\n\n# Plot the log of the subway distance\nx = salesXY[:,0]\ny = salesXY[:,1]\nax.hexbin(x, y, C=sales['logDistSubway'].values, gridsize=60)\n\n\n# Add color bar for the hexbin plot\ncb = plt.colorbar(hb, ax=ax, orientation=\"vertical\")\ncb.set_label(\"Distance to bus stops\", color=\"white\")  # Set colorbar label to white\n\n# Change color of the colorbar ticks\ncb.ax.yaxis.set_tick_params(color=\"white\")  # Color of tick marks\ncb.ax.yaxis.set_tick_params(labelcolor=\"white\")  # Color of tick labels\n\n# Change the spine color of the colorbar to white\ncb.outline.set_edgecolor(\"white\")\n\n# Plot the city limits\ncity_limits.plot(ax=ax, facecolor=\"none\", edgecolor=\"white\", linewidth=4)\n\n# Set axis off\nax.set_axis_off()\nax.set_aspect(\"equal\")\n\nplt.show()\n\n\n\n\n\n\n\n\nLooks like it worked!"
  },
  {
    "objectID": "analysis/exercise.html#what-about-correlations",
    "href": "analysis/exercise.html#what-about-correlations",
    "title": "Exercise on Predictive modeling: Housing price modeling",
    "section": "What about correlations?",
    "text": "What about correlations?\nLet’s have a look at the correlations of numerical columns:\n\nimport seaborn as sns\n\n\ncols = [\n    \"total_livable_area\",\n    \"total_area\",\n    \"garage_spaces\",\n    \"fireplaces\",\n    \"number_of_bathrooms\",\n    \"number_of_bedrooms\",\n    \"number_stories\",\n    \"logDistGraffiti\", # NEW\n    \"logDistSubway\",  # NEW\n    \"sale_price\"\n]\nsns.heatmap(sales[cols].corr(), cmap='coolwarm', annot=True, vmin=-1, vmax=1);"
  },
  {
    "objectID": "analysis/exercise.html#now-lets-re-run-our-modeldid-it-help",
    "href": "analysis/exercise.html#now-lets-re-run-our-modeldid-it-help",
    "title": "Exercise on Predictive modeling: Housing price modeling",
    "section": "Now, let’s re-run our model…did it help?",
    "text": "Now, let’s re-run our model…did it help?\n\n# Numerical columns\nnum_cols = [\n    \"total_livable_area\",\n    \"total_area\",\n    \"garage_spaces\",\n    \"fireplaces\",\n    \"number_of_bathrooms\",\n    \"number_of_bedrooms\",\n    \"number_stories\",\n    \"logDistGraffiti\", # NEW\n    \"logDistSubway\" # NEW\n]\n\n# Categorical columns\ncat_cols = [\"exterior_condition\", \"zip_code\"]\n\n\n# Set up the column transformer with two transformers\ntransformer = ColumnTransformer(\n    transformers=[\n        (\"num\", StandardScaler(), num_cols),\n        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), cat_cols),\n    ]\n)\n\n\n# Initialize the pipeline\n# NOTE: only use 20 estimators here so it will run in a reasonable time\npipe = make_pipeline(\n    transformer, RandomForestRegressor(n_estimators=20, random_state=42)\n)\n\n\n# Split the data 70/30\ntrain_set, test_set = train_test_split(sales, test_size=0.3, random_state=42)\n\n# the target labels\ny_train = np.log(train_set[\"sale_price\"])\ny_test = np.log(test_set[\"sale_price\"])\n\n\n# Fit the training set\n# REMINDER: use the training dataframe objects here rather than numpy array\npipe.fit(train_set, y_train);\n\n\n# What's the test score?\n# REMINDER: use the test dataframe rather than numpy array\npipe.score(test_set, y_test)\n\n0.6083692791290457\n\n\n\nA small improvement!\nR-squared of ~0.54 improved to R-squared of ~0.61"
  },
  {
    "objectID": "analysis/exercise.html#how-about-the-top-30-feature-importances-now",
    "href": "analysis/exercise.html#how-about-the-top-30-feature-importances-now",
    "title": "Exercise on Predictive modeling: Housing price modeling",
    "section": "How about the top 30 feature importances now?",
    "text": "How about the top 30 feature importances now?\n\ndef plot_feature_importances(pipeline, num_cols, transformer, top=20, **kwargs):\n    \"\"\"\n    Utility function to plot the feature importances from the input\n    random forest regressor.\n\n    Parameters\n    ----------\n    pipeline :\n        the pipeline object\n    num_cols :\n        list of the numerical columns\n    transformer :\n        the transformer preprocessing step\n    top : optional\n        the number of importances to plot\n    **kwargs : optional\n        extra keywords passed to the hvplot function\n    \"\"\"\n    # The one-hot step\n    ohe = transformer.named_transformers_[\"cat\"]\n\n    # One column for each category type!\n    ohe_cols = ohe.get_feature_names_out()\n\n    # Full list of columns is numerical + one-hot\n    features = num_cols + list(ohe_cols)\n\n    # The regressor\n    regressor = pipeline[\"randomforestregressor\"]\n\n    # Create the dataframe with importances\n    importance = pd.DataFrame(\n        {\"Feature\": features, \"Importance\": regressor.feature_importances_}\n    )\n\n    # Sort importance in descending order and get the top\n    importance = importance.sort_values(\"Importance\", ascending=False).iloc[:top]\n\n    # Plot\n    return importance.hvplot.barh(\n        x=\"Feature\", y=\"Importance\", flip_yaxis=True, **kwargs\n    )\n\n\nplot_feature_importances(pipe, num_cols, transformer, top=30, height=500)\n\n/Users/delmelle/miniforge3/lib/python3.10/site-packages/hvplot/converter.py:578: ParamDeprecationWarning: 'params' has been deprecated and will be removed in a future version. Use instead `.param.values()` or `.param['param']`\n  if k in OverlayPlot.param.params()}\n\n\n\n\n\n\n  \n\n\n\n\n\nBoth new spatial features are in the top 5 in terms of importance!"
  },
  {
    "objectID": "analysis/exercise.html#exercise-how-about-other-spatial-features",
    "href": "analysis/exercise.html#exercise-how-about-other-spatial-features",
    "title": "Exercise on Predictive modeling: Housing price modeling",
    "section": "Exercise: How about other spatial features?",
    "text": "Exercise: How about other spatial features?\n\nI’ve listed out several other types of potential sources of new distance-based features from OpenDataPhilly\nChoose a few and add new features\nRe-fit the model and evalute the performance on the test set and feature importances\n\nModify the get_xy_from_geometry() function to use the “centroid” of the geometry column.\nNote: you can take the centroid of a Point() or Polygon() object. For a Point(), you just get the x/y coordinates back.\n\ndef get_xy_from_geometry(df):\n    \"\"\"\n    Return a numpy array with two columns, where the \n    first holds the `x` geometry coordinate and the second \n    column holds the `y` geometry coordinate\n    \"\"\"\n    # NEW: use the centroid.x and centroid.y to support Polygon() and Point() geometries \n    x = df.geometry.centroid.x\n    y = df.geometry.centroid.y\n    \n    return np.column_stack((x, y)) # stack as columns\n\n\n1. Universities\nNew feature: Distance to the nearest university/college\n\nSource: OpenDataPhilly\nGeoJSON URL\n\n\n# Get the data\nurl = \"https://opendata.arcgis.com/api/v3/datasets/8ad76bc179cf44bd9b1c23d6f66f57d1_0/downloads/data?format=geojson&spatialRefId=4326\"\nunivs = gpd.read_file(url)\n\nunivs = univs.to_crs(epsg=3857)  # Ensure the CRS is correct\nunivs[\"geometry\"] = univs.centroid\n\n# Get the X/Y\nunivXY = get_xy_from_geometry(univs.to_crs(epsg=3857))\n\n# Run the k nearest algorithm\nnbrs = NearestNeighbors(n_neighbors=1)\nnbrs.fit(univXY)\nunivDists, _ = nbrs.kneighbors(salesXY)\n\n# Add the new feature\nsales['logDistUniv'] = np.log10(univDists.mean(axis=1))\n\n\nprint(univs.geometry.geom_type.value_counts())\n\nPoint    629\nName: count, dtype: int64\n\n\n\nfig, ax = plt.subplots(figsize=(5,5), facecolor=plt.get_cmap('viridis')(0))\n\nx = salesXY[:,0]\ny = salesXY[:,1]\nax.hexbin(x, y, C=np.log10(univDists.mean(axis=1)), gridsize=60)\n\n# Plot the city limits\ncity_limits.plot(ax=ax, facecolor='none', edgecolor='white', linewidth=4)\n\nax.set_axis_off()\nax.set_aspect(\"equal\")\nax.set_title(\"Distance to Nearest University/College\", fontsize=16, color='white');\n\n\n\n\n\n\n\n\n\n\n2. Parks\nNew feature: Distance to the nearest park centroid\n\nSource: OpenDataPhilly\nGeoJSON URL\n\nNotes - The park geometries are polygons, so you’ll need to get the x and y coordinates of the park centroids and calculate the distance to these centroids. - You can use the geometry.centroid.x and geometry.centroid.y values to access these coordinates.\n\n# Get the data\nurl = \"https://opendata.arcgis.com/datasets/d52445160ab14380a673e5849203eb64_0.geojson\"\nparks = gpd.read_file(url)\n\nparks = parks.to_crs(epsg=3857)  # Ensure the CRS is correct\nparks[\"geometry\"] = parks.centroid\n\n# Get the X/Y\nparksXY = get_xy_from_geometry(parks.to_crs(epsg=3857))\n\n# Run the k nearest algorithm\nnbrs = NearestNeighbors(n_neighbors=1)\nnbrs.fit(parksXY)\nparksDists, _ = nbrs.kneighbors(salesXY)\n\n# Add the new feature\nsales[\"logDistParks\"] = np.log10(parksDists.mean(axis=1))\n\n\nfig, ax = plt.subplots(figsize=(5, 5), facecolor=plt.get_cmap(\"viridis\")(0))\n\nx = salesXY[:, 0]\ny = salesXY[:, 1]\nax.hexbin(x, y, C=np.log10(parksDists.mean(axis=1)), gridsize=60)\n\n# Plot the city limits\ncity_limits.plot(ax=ax, facecolor=\"none\", edgecolor=\"white\", linewidth=4)\n\nax.set_axis_off()\nax.set_aspect(\"equal\")\nax.set_title(\"Distance to Nearest Park\", fontsize=16, color=\"white\");\n\n\n\n\n\n\n\n\n\n\n3. Residential Construction Permits\nNew feature: Distance to the 5 nearest residential construction permits from 2022\n\nSource: OpenDataPhilly\nCARTO table name: “permits”\n\nNotes\n\nYou can pull new construction permits only by selecting where permitdescription equals ‘RESIDENTRIAL CONSTRUCTION PERMIT’\nYou can select permits from only 2022 using the permitissuedate column\n\n\n# Table name\ntable_name = \"permits\"\n\n# Where clause\nwhere = \"permitissuedate &gt;= '2022-01-01' AND permitissuedate &lt; '2023-01-01'\"\nwhere = where + \" AND permitdescription='RESIDENTIAL BUILDING PERMIT'\"\n\n# Query\npermits = get_carto_data(table_name, where=where)\n\n# Remove missing\nnot_missing = ~permits.geometry.is_empty & permits.geometry.notna()\npermits = permits.loc[not_missing]\n\n# Get the X/Y\npermitsXY = get_xy_from_geometry(permits.to_crs(epsg=3857))\n\n# Run the k nearest algorithm\nnbrs = NearestNeighbors(n_neighbors=5)\nnbrs.fit(permitsXY)\npermitsDist, _ = nbrs.kneighbors(salesXY)\n\n# Add the new feature\nsales[\"logDistPermits\"] = np.log10(permitsDist.mean(axis=1))\n\n/var/folders/lv/8xb4hscx2hs0c0fwhd2bnjy80000gn/T/ipykernel_5039/3972340687.py:12: UserWarning: GeoSeries.notna() previously returned False for both missing (None) and empty geometries. Now, it only returns False for missing values. Since the calling GeoSeries contains empty geometries, the result has changed compared to previous versions of GeoPandas.\nGiven a GeoSeries 's', you can use '~s.is_empty & s.notna()' to get back the old behaviour.\n\nTo further ignore this warning, you can do: \nimport warnings; warnings.filterwarnings('ignore', 'GeoSeries.notna', UserWarning)\n  not_missing = ~permits.geometry.is_empty & permits.geometry.notna()\n\n\n\nfig, ax = plt.subplots(figsize=(10, 10), facecolor=plt.get_cmap(\"viridis\")(0))\n\nx = salesXY[:, 0]\ny = salesXY[:, 1]\nax.hexbin(x, y, C=np.log10(permitsDist.mean(axis=1)), gridsize=60)\n\n# Plot the city limits\ncity_limits.plot(ax=ax, facecolor=\"none\", edgecolor=\"white\", linewidth=4)\n\nax.set_axis_off()\nax.set_aspect(\"equal\")\nax.set_title(\"Distance to 5 Closest Building Permits\", fontsize=16, color=\"white\");\n\n\n\n\n\n\n\n\n\n\n4. Aggravated Assaults\nNew feature: Distance to the 5 nearest aggravated assaults in 2022\n\nSource: OpenDataPhilly\nCARTO table name: “incidents_part1_part2”\n\nNotes\n\nYou can pull aggravated assaults only by selecting where Text_General_Code equals ‘Aggravated Assault No Firearm’ or ‘Aggravated Assault Firearm’\nYou can select crimes from only 2022 using the dispatch_date column\n\n\n# Table name\ntable_name = \"incidents_part1_part2\"\n\n# Where selection\nwhere = \"dispatch_date &gt;= '2022-01-01' AND dispatch_date &lt; '2023-01-01'\"\nwhere = where + \" AND Text_General_Code IN ('Aggravated Assault No Firearm', 'Aggravated Assault Firearm')\"\n\n# Query\nassaults = get_carto_data(table_name, where=where)\n\n# Remove missing \nnot_missing = ~assaults.geometry.is_empty & assaults.geometry.notna()\nassaults = assaults.loc[not_missing]\n    \n# Get the X/Y\nassaultsXY = get_xy_from_geometry(assaults.to_crs(epsg=3857))\n\n# Run the k nearest algorithm\nnbrs = NearestNeighbors(n_neighbors=5)\nnbrs.fit(assaultsXY)\nassaultDists, _ = nbrs.kneighbors(salesXY)\n\n# Add the new feature\nsales['logDistAssaults'] = np.log10(assaultDists.mean(axis=1))\n\n\nfig, ax = plt.subplots(figsize=(10, 10), facecolor=plt.get_cmap(\"viridis\")(0))\n\nx = salesXY[:, 0]\ny = salesXY[:, 1]\nax.hexbin(x, y, C=np.log10(assaultDists.mean(axis=1)), gridsize=60)\n\n# Plot the city limits\ncity_limits.plot(ax=ax, facecolor=\"none\", edgecolor=\"white\", linewidth=4)\n\nax.set_axis_off()\nax.set_aspect(\"equal\")\nax.set_title(\"Distance to 5 Closest Assaults\", fontsize=16, color=\"white\");\n\n\n\n\n\n\n\n\n\n\n5. Abandonded Vehicle 311 Calls\nNew feature: Distance to the 5 nearest abandoned vehicle 311 calls in 2022\n\nSource: OpenDataPhilly\nCARTO table name: “public_cases_fc”\n\nNotes\n\nYou can pull abandonded vehicle calls only by selecting where service_name equals ‘Abandoned Vehicle’\nYou can select crimes from only 2022 using the requested_datetime column\n\n\n# Table name\ntable_name = \"public_cases_fc\"\n\n# Where selection\nwhere = \"requested_datetime &gt;= '2022-01-01' AND requested_datetime &lt; '2023-01-01'\"\nwhere = \"service_name = 'Abandoned Vehicle'\"\n\n# Query\ncars = get_carto_data(table_name, where=where)\n\n# Remove missing\nnot_missing = ~cars.geometry.is_empty & cars.geometry.notna()\ncars = cars.loc[not_missing]\n\n# Get the X/Y\ncarsXY = get_xy_from_geometry(cars.to_crs(epsg=3857))\n\n# Run the k nearest algorithm\nnbrs = NearestNeighbors(n_neighbors=5)\nnbrs.fit(carsXY)\ncarDists, _ = nbrs.kneighbors(salesXY)\n\n# Handle any sales that have 0 distances\ncarDists[carDists == 0] = 1e-5  # a small, arbitrary value\n\n# Add the new feature\nsales[\"logDistCars\"] = np.log10(carDists.mean(axis=1))\n\n\nfig, ax = plt.subplots(figsize=(10, 10), facecolor=plt.get_cmap(\"viridis\")(0))\n\nx = salesXY[:, 0]\ny = salesXY[:, 1]\nax.hexbin(x, y, C=np.log10(carDists.mean(axis=1)), gridsize=60)\n\n# Plot the city limits\ncity_limits.plot(ax=ax, facecolor=\"none\", edgecolor=\"white\", linewidth=4)\n\nax.set_axis_off()\nax.set_aspect(\"equal\")\nax.set_title(\n    \"Distance to 5 Closest Abandoned Vehicle 311 Calls\", fontsize=16, color=\"white\"\n);\n\n\n\n\n\n\n\n\n\n\n6. City Hall\nNew feature: Distance to City Hall.\n\nSource: OpenDataPhilly\nGeoJSON URL\n\nNotes\n\nTo identify City Hall, you’ll need to pull data where “NAME=‘City Hall’” and “FEAT_TYPE=‘Municipal Building’”\nAs with the parks, the geometry will be a polygon, so you should calculate the distance to the centroid of the City Hall polygon\nurl = “http://data-phl.opendata.arcgis.com/datasets/5146960d4d014f2396cb82f31cd82dfe_0.geojson” landmarks = gpd.read_file(url)\n\n\n\n7. SEPTA train stations\n\nidentify if a house is close to a septa station (maybe here we don’t need to get the average distance to the 5 nearest)\n\n\n\n8. Bus stops\n\nIs the price of the house going to increase if you have more bustops nearby?"
  },
  {
    "objectID": "analysis/assignment-6.html",
    "href": "analysis/assignment-6.html",
    "title": "Assignment 6: Predictive Modeling of Housing Prices in Philadelphia",
    "section": "",
    "text": "Due date: Wednesday, 12/6 by the end of the day\nLectures 12B and 13A will cover predictive modeling of housing prices in Philadelphia. We’ll extend that analysis in this section by:"
  },
  {
    "objectID": "analysis/assignment-6.html#part-2-modeling-philadelphias-housing-prices-and-algorithmic-fairness",
    "href": "analysis/assignment-6.html#part-2-modeling-philadelphias-housing-prices-and-algorithmic-fairness",
    "title": "Assignment 6: Predictive Modeling of Housing Prices in Philadelphia",
    "section": "Part 2: Modeling Philadelphia’s Housing Prices and Algorithmic Fairness",
    "text": "Part 2: Modeling Philadelphia’s Housing Prices and Algorithmic Fairness\n\n2.1 Load data from the Office of Property Assessment\nUse the requests package to query the CARTO API for single-family property assessment data in Philadelphia for properties that had their last sale during 2022.\nSources: - OpenDataPhilly - Metadata\n\nimport geopandas as gpd\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nimport requests\n\n\n# The API endpoint\ncarto_url = \"https://phl.carto.com/api/v2/sql\"\n\n# Only pull 2022 sales for single family residential properties\nwhere = \"sale_date &gt;= '2022-01-01' and sale_date &lt;= '2022-12-31'\"\nwhere = where + \" and category_code_description IN ('SINGLE FAMILY', 'Single Family')\"\n\n# Create the query\nquery = f\"SELECT * FROM opa_properties_public WHERE {where}\"\n\n# Make the request\nparams = {\"q\": query, \"format\": \"geojson\", \"where\": where}\nresponse = requests.get(carto_url, params=params)\n\n# Make the GeoDataFrame\ndata = gpd.GeoDataFrame.from_features(response.json(), crs=\"EPSG:4326\")\ndata.head()\n\n\n\n\n\n\n\n\ngeometry\ncartodb_id\nassessment_date\nbasements\nbeginning_point\nbook_and_page\nbuilding_code\nbuilding_code_description\ncategory_code\ncategory_code_description\n...\nutility\nview_type\nyear_built\nyear_built_estimate\nzip_code\nzoning\npin\nbuilding_code_new\nbuilding_code_description_new\nobjectid\n\n\n\n\n0\nPOINT (-75.11476 39.99888)\n1282\n2024-06-06T16:08:59Z\nD\n15' N CORNWALL ST\n54341670\nO30\nROW 2 STY MASONRY\n1\nSINGLE FAMILY\n...\nNone\nI\n1920\nNone\n19134\nRSA5\n1001231917\n24\nROW PORCH FRONT\n557204973\n\n\n1\nPOINT (-75.12513 39.99317)\n1766\n2024-06-06T16:05:55Z\nNone\n470' N SOMERSET ST\n54349556\nO30\nROW 2 STY MASONRY\n1\nSINGLE FAMILY\n...\nNone\nI\n1930\nY\n19134\nICMX\n1001463580\n22\nROW TYPICAL\n557203818\n\n\n2\nPOINT (-75.18250 39.99616)\n3266\n2024-06-06T16:04:59Z\nNone\n57' N HUNTINGDON ST\n54347360\nO30\nROW 2 STY MASONRY\n1\nSINGLE FAMILY\n...\nNone\nI\n1915\nY\n19132\nRSA5\n1001384821\n22\nROW TYPICAL\n557204158\n\n\n3\nPOINT (-75.22952 40.03209)\n6233\n2024-06-06T16:08:50Z\nC\n85'3 7/8\"E SMICK\n54324208\nH50\nSEMI/DET 3 STY MASONRY\n1\nSINGLE FAMILY\n...\nNone\nI\n1900\nY\n19127\nRSA5\n1001273076\n32\nTWIN CONVENTIONAL\n557208015\n\n\n4\nPOINT (-75.22352 39.95894)\n6279\n2024-06-06T16:11:17Z\nNone\n90' W 51ST ST\n54325718\nO30\nROW 2 STY MASONRY\n1\nSINGLE FAMILY\n...\nNone\nI\n1925\nY\n19139\nRSA6\n1001439821\n24\nROW PORCH FRONT\n557207794\n\n\n\n\n5 rows × 80 columns\n\n\n\n\n\n2.2 Load data for census tracts and neighborhoods\nLoad various Philadelphia-based regions that we will use in our analysis.\n\nCensus tracts can be downloaded from: https://opendata.arcgis.com/datasets/8bc0786524a4486bb3cf0f9862ad0fbf_0.geojson\nNeighborhoods can be downloaded from: https://raw.githubusercontent.com/azavea/geo-data/master/Neighborhoods_Philadelphia/Neighborhoods_Philadelphia.geojson\n\n\ntracts = gpd.read_file(\"https://opendata.arcgis.com/datasets/8bc0786524a4486bb3cf0f9862ad0fbf_0.geojson\")\n\nneighborhoods = gpd.read_file(\"https://raw.githubusercontent.com/opendataphilly/open-geo-data/refs/heads/master/philadelphia-neighborhoods/philadelphia-neighborhoods.geojson\")\n\n\n\n2.3 Spatially join the sales data and neighborhoods/census tracts.\nPerform a spatial join, such that each sale has an associated neighborhood and census tract.\nNote: After performing the first spatial join, you will need to use the drop() function to remove the index_right column; otherwise an error will be raised on the second spatial join about duplicate columns.\n\n# Spatial join the data with the tracts\nsp_data = gpd.sjoin(data, tracts, how=\"left\", op=\"within\").drop(columns=\"index_right\")\n\n# Spatial join the data with the neighborhoods\nsp_data = gpd.sjoin(sp_data, neighborhoods, how=\"left\", op=\"within\")\n\ne:\\miniforge3\\envs\\musa-550\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3448: FutureWarning: The `op` parameter is deprecated and will be removed in a future release. Please use the `predicate` parameter instead.\n  if await self.run_code(code, result, async_=asy):\ne:\\miniforge3\\envs\\musa-550\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3448: FutureWarning: The `op` parameter is deprecated and will be removed in a future release. Please use the `predicate` parameter instead.\n  if await self.run_code(code, result, async_=asy):\n\n\n\n\n2.4 Train a Random Forest on the sales data\nIn this step, you should follow the steps outlined in lecture to preprocess and train your model. We’ll extend our analysis to do a hyperparameter grid search to find the best model configuration. As you train your model, follow the following steps:\nPreprocessing Requirements - Trim the sales data to those sales with prices between $3,000 and $1 million - Set up a pipeline that includes both numerical columns and categorical columns - Include one-hot encoded variable for the neighborhood of the sale, instead of ZIP code. We don’t want to include multiple location based categories, since they encode much of the same information.\nTraining requirements - Use a 70/30% training/test split and predict the log of the sales price. - Use GridSearchCV to perform a k-fold cross validation that optimize at least 2 hyperparameters of the RandomForestRegressor - After fitting your model and finding the optimal hyperparameters, you should evaluate the score (R-squared) on the test set (the original 30% sample withheld)\nNote: You don’t need to include additional features (such as spatial distance features) or perform any extra feature engineering beyond what is required above to receive full credit. Of course, you are always welcome to experiment!\n\n# Models\nfrom sklearn.ensemble import RandomForestRegressor\n\n# Model selection\nfrom sklearn.model_selection import train_test_split, GridSearchCV\n\n# Pipelines\nfrom sklearn.pipeline import make_pipeline\n\n# Preprocessing\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder\n\n\n# Filter the data to include only sales between $3,000 and $1,000,000   \nsales = sp_data.loc[(sp_data[\"sale_price\"] &gt;= 3000) & (sp_data[\"sale_price\"] &lt;= 1e6)]\n\n# Wanted columns\ncols = [\n    \"sale_price\",\n    \"total_livable_area\",\n    \"total_area\",\n    \"garage_spaces\",\n    \"fireplaces\",\n    \"number_of_bathrooms\",\n    \"number_of_bedrooms\",\n    \"number_stories\",\n    \"exterior_condition\",\n    'interior_condition', # Added\n    'NAME10', # Census tract name\n    'GEOID10', # Census tract\n    'LISTNAME' # Neighborhood\n]\n\n# Trim to these columns and remove NaNs\nsales = sales[cols + [\"geometry\"]].dropna()\n\nlen(sales)\n\n17724\n\n\n\n# Numerical columns\nnum_cols = [\n    \"total_livable_area\",\n    \"total_area\",\n    \"garage_spaces\",\n    \"fireplaces\",\n    \"number_of_bathrooms\",\n    \"number_of_bedrooms\",\n    \"number_stories\",\n]\n\n# Categorical columns\ncat_cols = [\n    \"exterior_condition\", \n    \"interior_condition\", \n    \"LISTNAME\"\n]\n\n# Set up the column transformer with two transformers\n# ----&gt; Scale the numerical columns\n# ----&gt; One-hot encode the categorical columns\n\ntransformer = ColumnTransformer(\n    transformers=[\n        (\"num\", StandardScaler(), num_cols),\n        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), cat_cols),\n    ]\n)\n\n\n# Initialize the pipeline\npipe = make_pipeline(\n    transformer, RandomForestRegressor(random_state=7)\n)\n\n\n# Split the data 70/30\ntrain_set, test_set = train_test_split(sales, test_size=0.3, random_state=7)\n\n# the target labels: log of sale price\ny_train = np.log(train_set[\"sale_price\"])\ny_test = np.log(test_set[\"sale_price\"])\n\n\n# Use GridSearchCV to find the best hyperparameters\n# Set up GridSearchCV\nparam_grid = {\n    'randomforestregressor__n_estimators': [50, 100, 200],\n    'randomforestregressor__max_depth': [20, 50, None],\n    'randomforestregressor__min_samples_split': [2, 4, 6]\n}\n\n# Set up the grid search\ngrid_search = GridSearchCV(\n    pipe, \n    param_grid, \n    cv=5, # 5-fold cross-validation\n    scoring='r2', # Use R^2 as the scoring metric\n    n_jobs=-1 # Use all available cpu cores\n    )\n\n\n# Fit the model\ngrid_search.fit(train_set, y_train)\n\nGridSearchCV(cv=5,\n             estimator=Pipeline(steps=[('columntransformer',\n                                        ColumnTransformer(transformers=[('num',\n                                                                         StandardScaler(),\n                                                                         ['total_livable_area',\n                                                                          'total_area',\n                                                                          'garage_spaces',\n                                                                          'fireplaces',\n                                                                          'number_of_bathrooms',\n                                                                          'number_of_bedrooms',\n                                                                          'number_stories']),\n                                                                        ('cat',\n                                                                         OneHotEncoder(handle_unknown='ignore'),\n                                                                         ['exterior_condition',\n                                                                          'interior_condition',\n                                                                          'LISTNAME'])])),\n                                       ('randomforestregressor',\n                                        RandomForestRegressor(random_state=7))]),\n             n_jobs=-1,\n             param_grid={'randomforestregressor__max_depth': [20, 50, None],\n                         'randomforestregressor__min_samples_split': [2, 4, 6],\n                         'randomforestregressor__n_estimators': [50, 100, 200]},\n             scoring='r2')In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GridSearchCVGridSearchCV(cv=5,\n             estimator=Pipeline(steps=[('columntransformer',\n                                        ColumnTransformer(transformers=[('num',\n                                                                         StandardScaler(),\n                                                                         ['total_livable_area',\n                                                                          'total_area',\n                                                                          'garage_spaces',\n                                                                          'fireplaces',\n                                                                          'number_of_bathrooms',\n                                                                          'number_of_bedrooms',\n                                                                          'number_stories']),\n                                                                        ('cat',\n                                                                         OneHotEncoder(handle_unknown='ignore'),\n                                                                         ['exterior_condition',\n                                                                          'interior_condition',\n                                                                          'LISTNAME'])])),\n                                       ('randomforestregressor',\n                                        RandomForestRegressor(random_state=7))]),\n             n_jobs=-1,\n             param_grid={'randomforestregressor__max_depth': [20, 50, None],\n                         'randomforestregressor__min_samples_split': [2, 4, 6],\n                         'randomforestregressor__n_estimators': [50, 100, 200]},\n             scoring='r2')estimator: PipelinePipeline(steps=[('columntransformer',\n                 ColumnTransformer(transformers=[('num', StandardScaler(),\n                                                  ['total_livable_area',\n                                                   'total_area',\n                                                   'garage_spaces',\n                                                   'fireplaces',\n                                                   'number_of_bathrooms',\n                                                   'number_of_bedrooms',\n                                                   'number_stories']),\n                                                 ('cat',\n                                                  OneHotEncoder(handle_unknown='ignore'),\n                                                  ['exterior_condition',\n                                                   'interior_condition',\n                                                   'LISTNAME'])])),\n                ('randomforestregressor',\n                 RandomForestRegressor(random_state=7))])columntransformer: ColumnTransformerColumnTransformer(transformers=[('num', StandardScaler(),\n                                 ['total_livable_area', 'total_area',\n                                  'garage_spaces', 'fireplaces',\n                                  'number_of_bathrooms', 'number_of_bedrooms',\n                                  'number_stories']),\n                                ('cat', OneHotEncoder(handle_unknown='ignore'),\n                                 ['exterior_condition', 'interior_condition',\n                                  'LISTNAME'])])num['total_livable_area', 'total_area', 'garage_spaces', 'fireplaces', 'number_of_bathrooms', 'number_of_bedrooms', 'number_stories']StandardScalerStandardScaler()cat['exterior_condition', 'interior_condition', 'LISTNAME']OneHotEncoderOneHotEncoder(handle_unknown='ignore')RandomForestRegressorRandomForestRegressor(random_state=7)\n\n\n\n# Get the best hyperparameters\nbest_hyperparameters = grid_search.best_params_\nprint(best_hyperparameters)\n\n{'randomforestregressor__max_depth': 50, 'randomforestregressor__min_samples_split': 6, 'randomforestregressor__n_estimators': 200}\n\n\n\n# Evaluate on test set\nbest_model = grid_search.best_estimator_\ntest_score = best_model.score(test_set, y_test)\nprint(f\"Best model R^2 on test set: {test_score:.4f}\")\n\nBest model R^2 on test set: 0.6009\n\n\n\n\n2.5 Calculate the percent error of your model predictions for each sale in the test set\nFit your best model and use it to make predictions on the test set.\nNote: This should be the percent error in terms of sale price. You’ll need to convert if your model predicted the log of sales price!\n\n# Predict the values for the test set using the best model\ny_pred = best_model.predict(test_set)\n\n# Add exponantial predicted values to the DataFrame\ntest_set_results = test_set.copy() \ntest_set_results['predicted_sale_price'] = np.exp(y_pred)\n\n# Calculate percent error\ntest_set_results['percent_error'] = 100 * (test_set_results['predicted_sale_price'] - test_set_results['sale_price']) / test_set_results['sale_price']\ntest_set_results['abs_percent_error'] = 100 * abs((test_set_results['predicted_sale_price'] - test_set_results['sale_price'])) / test_set_results['sale_price']\n\n\n\n2.6 Make a data frame with percent errors and census tract info for each sale in the test set\nCreate a data frame that has the property geometries, census tract data, and percent errors for all of the sales in the test set.\nNotes\n\nWhen using the “train_test_split()” function, the index of the test data frame includes the labels from the original sales data frame\nYou can use this index to slice out the test data from the original sales data frame, which should include the census tract info and geometries\nAdd a new column to this data frame holding the percent error data\nMake sure to use the percent error and not the absolute percent error\n\n\n# Slice the sales data to only include test_set indexes\nresult = sales.loc[test_set.index]\n\n# Add the predicted values to the sales data\nresult['percent_error'] = test_set_results['percent_error']\n\n\n\n2.8 Plot a map of the median percent error by census tract\n\nYou’ll want to group your data frame of test sales by the GEOID10 column and take the median of you percent error column\nMerge the census tract geometries back in and use geopandas to plot.\n\n\n# Group by GEOID10 and calculate the median percent error\ngrouped = result.groupby(\"GEOID10\")[\"percent_error\"].median().reset_index()\n\n# Merge the map data with the tracts\nmap_data = tracts.merge(grouped, on=\"GEOID10\")\n\n\n# Plot the map\nfig, ax = plt.subplots(1, 1, figsize=(8, 8))\n\nmap_data.plot(\n    ax=ax,\n    column=\"percent_error\",\n    scheme=\"natural_breaks\",\n    k=5,\n    cmap=\"coolwarm\",\n    edgecolor=\"white\",\n    linewidth=0.5,\n    legend=True,\n    legend_kwds={\"title\": \"Median Percent Error\", \"loc\": \"lower right\"},\n)\n\nplt.title(\"Median Percent Error by Census Tract\")\n\ne:\\miniforge3\\envs\\musa-550\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1436: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=2.\n  warnings.warn(\n\n\nText(0.5, 1.0, 'Median Percent Error by Census Tract')\n\n\n\n\n\n\n\n\n\n\n\n2.9 Compare the percent errors in Qualifying Census Tracts and other tracts\nQualifying Census Tracts are a poverty designation that HUD uses to allocate housing tax credits\n\nI’ve included a list of the census tract names that qualify in Philadelphia\nAdd a new column to your dataframe of test set sales that is True/False depending on if the tract is a QCT\nThen, group by this new column and calculate the median percent error\n\nYou should find that the algorithm’s accuracy is significantly worse in these low-income, qualifying census tracts\n\nqct = ['5',\n '20',\n '22',\n '28.01',\n '30.01',\n '30.02',\n '31',\n '32',\n '33',\n '36',\n '37.01',\n '37.02',\n '39.01',\n '41.01',\n '41.02',\n '56',\n '60',\n '61',\n '62',\n '63',\n '64',\n '65',\n '66',\n '67',\n '69',\n '70',\n '71.01',\n '71.02',\n '72',\n '73',\n '74',\n '77',\n '78',\n '80',\n '81.01',\n '81.02',\n '82',\n '83.01',\n '83.02',\n '84',\n '85',\n '86.01',\n '86.02',\n '87.01',\n '87.02',\n '88.01',\n '88.02',\n '90',\n '91',\n '92',\n '93',\n '94',\n '95',\n '96',\n '98.01',\n '100',\n '101',\n '102',\n '103',\n '104',\n '105',\n '106',\n '107',\n '108',\n '109',\n '110',\n '111',\n '112',\n '113',\n '119',\n '121',\n '122.01',\n '122.03',\n '131',\n '132',\n '137',\n '138',\n '139',\n '140',\n '141',\n '144',\n '145',\n '146',\n '147',\n '148',\n '149',\n '151.01',\n '151.02',\n '152',\n '153',\n '156',\n '157',\n '161',\n '162',\n '163',\n '164',\n '165',\n '167.01',\n '167.02',\n '168',\n '169.01',\n '169.02',\n '170',\n '171',\n '172.01',\n '172.02',\n '173',\n '174',\n '175',\n '176.01',\n '176.02',\n '177.01',\n '177.02',\n '178',\n '179',\n '180.02',\n '188',\n '190',\n '191',\n '192',\n '195.01',\n '195.02',\n '197',\n '198',\n '199',\n '200',\n '201.01',\n '201.02',\n '202',\n '203',\n '204',\n '205',\n '206',\n '208',\n '239',\n '240',\n '241',\n '242',\n '243',\n '244',\n '245',\n '246',\n '247',\n '249',\n '252',\n '253',\n '265',\n '267',\n '268',\n '271',\n '274.01',\n '274.02',\n '275',\n '276',\n '277',\n '278',\n '279.01',\n '279.02',\n '280',\n '281',\n '282',\n '283',\n '284',\n '285',\n '286',\n '287',\n '288',\n '289.01',\n '289.02',\n '290',\n '291',\n '293',\n '294',\n '298',\n '299',\n '300',\n '301',\n '302',\n '305.01',\n '305.02',\n '309',\n '311.01',\n '312',\n '313',\n '314.01',\n '314.02',\n '316',\n '318',\n '319',\n '321',\n '325',\n '329',\n '330',\n '337.01',\n '345.01',\n '357.01',\n '376',\n '377',\n '380',\n '381',\n '382',\n '383',\n '389',\n '390']\n\n\n# Add a column to the test set results to indicate if the census tract is a QCT\ntest_set_results['QCT'] = test_set_results['NAME10'].isin(qct)\n\n# Group by QCT and calculate the median percent error\ngrouped_qct = test_set_results.groupby(\"QCT\")[\"percent_error\"].median().reset_index()\n\ngrouped_qct\n\n\n\n\n\n\n\n\nQCT\npercent_error\n\n\n\n\n0\nFalse\n-8.069707\n\n\n1\nTrue\n-0.401733\n\n\n\n\n\n\n\nIt seems that the median percent error is actually lower in my best random forest model, and I think my scripts are correct after checking for many times. So, I decide to make a box plot to see if there is anything wrong using MEDIAN percent error.\n\n# Set a clean aesthetic for the plot\nsns.set_theme(style=\"whitegrid\")\n\n# Create a boxplot\nplt.figure(figsize=(8, 6))\nsns.boxplot(x='QCT', y='percent_error', data=test_set_results, palette=\"coolwarm\", showfliers=False)\n\nsns.pointplot(x='QCT', y='percent_error', data=test_set_results, estimator='median', \n              color='black', linestyles='--', markers='o', ci=None, scale=1.2)\n\nplt.title(\"Percent Error by QCT\", fontsize=16)\nplt.xlabel(\"Qualifying Census Tract (QCT)\", fontsize=14)\nplt.ylabel(\"Percent Error\", fontsize=14)\nplt.xticks([0, 1], ['Non-QCT', 'QCT'], fontsize=12) \nplt.tight_layout()\nplt.show()\n\nC:\\Users\\chenf\\AppData\\Local\\Temp\\ipykernel_30484\\3570526491.py:8: FutureWarning: \n\nThe `ci` parameter is deprecated. Use `errorbar=None` for the same effect.\n\n  sns.pointplot(x='QCT', y='percent_error', data=test_set_results, estimator='median',\n\n\n\n\n\n\n\n\n\nIt turns out that MEDIAN percent error lied to us, and if I turn to use MEDIAN ABSOLUTE percent error, it is much higher in QCT and aligns with the statement that “algorithm’s accuracy is significantly worse in these low-income, qualifying census tracts”.\n\n# Group by QCT and calculate the absolute median percent error\ngrouped_qct_abs = test_set_results.groupby(\"QCT\")[\"abs_percent_error\"].median().reset_index()\n\ngrouped_qct_abs\n\n\n\n\n\n\n\n\nQCT\nabs_percent_error\n\n\n\n\n0\nFalse\n15.556725\n\n\n1\nTrue\n24.750887"
  },
  {
    "objectID": "analysis/prediction.html#model-comparison",
    "href": "analysis/prediction.html#model-comparison",
    "title": "Modeling",
    "section": "Model Comparison",
    "text": "Model Comparison\nIn this section, I will compare the performance of the four models using the median absolute error and the R-squared score.\nFisrt, let’s have a look at the hyperparameters and the R-squared score of each model in the table below.\n\n\nCode\nmodel_performance = pd.DataFrame({\n    \"Name\": [\"Model 1\", \"Model 2\", \"Model 3\", \"Model 4\"],\n    \"Features\": [\"No spatial features\", \"with k=1 nearest neigbor\", \"with k=3 nearest neigbor\", \"with k=5 nearest neigbor\"],\n    \"R-squared\": [test_score_1, test_score_2, test_score_3, test_score_4]\n})\n\nmodel_performance\n\n\n\n\n\n\n\n\n\nName\nFeatures\nR-squared\n\n\n\n\n0\nModel 1\nNo spatial features\n0.725692\n\n\n1\nModel 2\nwith k=1 nearest neigbor\n0.735239\n\n\n2\nModel 3\nwith k=3 nearest neigbor\n0.718237\n\n\n3\nModel 4\nwith k=5 nearest neigbor\n0.738747\n\n\n\n\n\n\n\nWe can see that the best model is Model 4 for now, which has the highest R-squared score. The model has the following hyperparameters:\n\n\nCode\nprint(best_hyperparameters_4)\n\n\n{'randomforestregressor__max_depth': 20, 'randomforestregressor__min_samples_split': 2, 'randomforestregressor__n_estimators': 200}\n\n\nNow, Let’s compare the performance of the four models using the median absolute error. The median absolute error is a robust metric that is less sensitive to outliers compared to the mean squared error. The lower the median absolute error, the better the model. The results are shown in the table below.\n\n\nCode\n# Predict the values for the test set using the best model\ny_pred_1 = model_1.predict(test_set)\ny_pred_2 = model_2.predict(test_set)\ny_pred_3 = model_3.predict(test_set)\ny_pred_4 = model_4.predict(test_set)\n\n# Add predicted values to the DataFrame\ntest_set_results = test_set.copy() \ntest_set_results['predicted_1'] = y_pred_1\ntest_set_results['predicted_2'] = y_pred_2\ntest_set_results['predicted_3'] = y_pred_3\ntest_set_results['predicted_4'] = y_pred_4\n\n# Calculate absolute error\ntest_set_results['abs_error_1'] = abs(test_set_results['predicted_1'] - test_set_results['spend_2024'])\ntest_set_results['abs_error_2'] = abs(test_set_results['predicted_2'] - test_set_results['spend_2024'])\ntest_set_results['abs_error_3'] = abs(test_set_results['predicted_3'] - test_set_results['spend_2024'])\ntest_set_results['abs_error_4'] = abs(test_set_results['predicted_4'] - test_set_results['spend_2024'])\n\n# Calculate median absolute error\nmae_1 = test_set_results['abs_error_1'].median()\nmae_2 = test_set_results['abs_error_2'].median()\nmae_3 = test_set_results['abs_error_3'].median()\nmae_4 = test_set_results['abs_error_4'].median()\n\n# Create a DataFrame with the results\nmodel_results = pd.DataFrame({\n    \"Model\": [\"Model 1\", \"Model 2\", \"Model 3\", \"Model 4\"],\n    \"Median Absolute Error\": [mae_1, mae_2, mae_3, mae_4]\n})\n\nmodel_results\n\n\n\n\n\n\n\n\n\nModel\nMedian Absolute Error\n\n\n\n\n0\nModel 1\n751.250520\n\n\n1\nModel 2\n763.960198\n\n\n2\nModel 3\n776.700924\n\n\n3\nModel 4\n729.354195\n\n\n\n\n\n\n\nStill, Model 4 is the best model with the lowest median absolute error. Thus, we can conclude that Model 4 is the best model among the four models.",
    "crumbs": [
      "Analysis",
      "Modeling"
    ]
  },
  {
    "objectID": "analysis/prediction.html#prediction",
    "href": "analysis/prediction.html#prediction",
    "title": "Modeling",
    "section": "Prediction",
    "text": "Prediction\nIn this section, I will use the best model to predict the spend data of 2025 using the data of 2024. The predicted spend data will be visualized using a map.\nFirst, let’s preprocess the data of 2024 and add the spatial features of 5 nearest neighbours.\n\n\nCode\ndata_2024 = raw_data_2024.copy().merge(raw_data_2023, on='PLACEKEY', suffixes=('', '_2023'))\n\ndata_2024 = data_2024.drop(columns=['DATE_RANGE_START', 'RAW_NUM_CUSTOMERS', 'RAW_VISIT_COUNTS_2023', 'RAW_VISITOR_COUNTS_2023', 'RAW_NUM_TRANSACTIONS_2023', 'RAW_NUM_CUSTOMERS_2023', 'geometry_2023', 'DATE_RANGE_START_2023', 'LOCATION_NAME_2023', 'TOP_CATEGORY_2023', 'LATITUDE_2023', 'LONGITUDE_2023'])\n\ndata_2024 = data_2024.rename(columns={'RAW_VISIT_COUNTS': 'visits', 'RAW_VISITOR_COUNTS': 'visitors', 'RAW_TOTAL_SPEND': 'spend_2024', 'RAW_NUM_TRANSACTIONS': 'transactions', 'RAW_TOTAL_SPEND_2023': 'spend_2023'})\n\ndata_2024['spend'] = (data_2024['spend_2024'] + data_2024['spend_2023']) / 2\n\n\n\n\nCode\ndata_2024XY = get_xy_from_geometry(data_2024)\n\n# STEP 1: Define k values\nk = 5\nnbrs = NearestNeighbors(n_neighbors=k)\n\n# STEP 2: Fit the algorithm on the \"neighbors\" dataset\nnbrs.fit(data_2024XY)\n\n# STEP 3: Get distances for sale to neighbors\ndataDists, dataIndices = nbrs.kneighbors(data_2024XY)\n\n# Define columns for which to calculate mean\ncolumns = [\"visits\", \"visitors\", \"transactions\"]\n\n\nHere, we have prepared the data for the prediction, as shown below. Now, let’s predict the spend data of 2025 using the best model.\n\n\nCode\nfor col in columns:\n    # Calculate the mean for each point based on the k nearest neighbors\n    data_2024[f\"{col}_k5\"] = [\n        data_2024.iloc[indices[:5]][col].mean() for indices in dataIndices\n    ]\n\ndata_2024.head()\n\n\n\n\n\n\n\n\n\nPLACEKEY\nLOCATION_NAME\nTOP_CATEGORY\nLATITUDE\nLONGITUDE\nvisits\nvisitors\nspend_2024\ntransactions\ngeometry\nspend_2023\nspend\nvisits_k5\nvisitors_k5\ntransactions_k5\n\n\n\n\n0\nzzw-224@628-pmf-skf\nMisconduct Tavern\nRestaurants and Other Eating Places\n39.948709\n-75.166626\n247.0\n218.0\n6196.76\n155.0\nPOINT (-75.16663 39.94871)\n5442.35\n5819.555\n905.8\n582.0\n309.6\n\n\n1\nzzw-223@628-p7x-w6k\nPizza Roma\nRestaurants and Other Eating Places\n40.049594\n-75.059819\n155.0\n127.0\n3495.90\n204.0\nPOINT (-75.05982 40.04959)\n2782.23\n3139.065\n5316.2\n2388.0\n547.6\n\n\n2\n22x-223@628-pmb-fpv\nShaking Seafood\nRestaurants and Other Eating Places\n39.946982\n-75.157365\n251.0\n135.0\n2734.49\n74.0\nPOINT (-75.15736 39.94698)\n1802.47\n2268.480\n93.2\n53.0\n30.6\n\n\n3\n22d-222@628-pmb-6hq\nDaMo Pasta Lab\nRestaurants and Other Eating Places\n39.949786\n-75.160216\n178.0\n178.0\n1146.45\n43.0\nPOINT (-75.16022 39.94979)\n1425.15\n1285.800\n902.8\n628.8\n72.8\n\n\n4\nzzw-222@628-p9w-zj9\nLittle Istanbul\nRestaurants and Other Eating Places\n40.119755\n-75.018108\n804.0\n523.0\n3204.47\n51.0\nPOINT (-75.01811 40.11975)\n1758.69\n2481.580\n581.6\n370.4\n303.4\n\n\n\n\n\n\n\n\n\nCode\ndata_2024_results = data_2024.copy()\n\ndata_2024_results['predicted_spend_2025'] = model_4.predict(data_2024_results)\n\n# Sort by visit counts so that the largest points are plotted on top\ndata_2024_results = gpd.GeoDataFrame(data_2024_results, geometry='geometry')\ndata_2024_results = data_2024_results.sort_values(by='predicted_spend_2025')\n\n\nThe result of the prediction is shown in the table below. To better understand the trend of the income data, I have added the result to the previous data. From the table and chart below, it can be found that the predicted total of 2025 will be much lower than the actual income data of 2024, and just slightly higher than the lowest point of 2020. However, when it turns to the average income, the predicted data of 2025 is higher than the actual data of 2021. But there is still a drop compared to the actual data of 2024.\n\n\nCode\nadvan_sg = gpd.read_file('../data/advan_sg.geojson')\n\nsummary_table = advan_sg.groupby('DATE_RANGE_START').sum(numeric_only=True).drop(columns=['LATITUDE', 'LONGITUDE'])\nsummary_table_2 = advan_sg.groupby('DATE_RANGE_START').mean(numeric_only=True).drop(columns=['LATITUDE', 'LONGITUDE'])\n\nsummary_pred = data_2024_results.sum(numeric_only=True).to_frame().T\nsummary_pred_2 = data_2024_results.mean(numeric_only=True).to_frame().T\n\nresult = summary_table['RAW_TOTAL_SPEND']\n\nmean_spend = summary_table_2['RAW_TOTAL_SPEND']\nresult = result.to_frame(name=\"Total Spend\")\nresult['Mean Spend'] = mean_spend\n\nresult.loc[2025] = [summary_pred.loc[0, 'predicted_spend_2025'], summary_pred_2.loc[0, 'predicted_spend_2025']]\n\nresult = result.reset_index()\n\nresult.columns = ['year', 'Total Income', 'Mean Income']\n\nresult\n\n\n\n\n\n\n\n\n\nyear\nTotal Income\nMean Income\n\n\n\n\n0\n2019\n18602269.20\n6508.84\n\n\n1\n2020\n13145216.02\n5067.55\n\n\n2\n2021\n16743947.70\n5852.48\n\n\n3\n2022\n21524058.02\n7004.25\n\n\n4\n2023\n19350828.55\n6801.70\n\n\n5\n2024\n18474501.11\n7051.34\n\n\n6\n2025\n14411266.08\n6365.40\n\n\n\n\n\n\n\n\n\nCode\nimport matplotlib.pyplot as plt\nfrom matplotlib.ticker import FuncFormatter\n\n# Adjusted axis and title font properties\naxis_fontdict = {'fontsize': 12, 'family': 'Cambria', 'color': 'white'}\ntitle_fontdict = {'fontsize': 14, 'fontweight': 'bold', 'family': 'Cambria', 'color': 'white'}\n\nfig, ax = plt.subplots(figsize=(6, 4))\n\n# Plotting\nresult.plot(\n    x='year', \n    y='Total Income',\n    marker='o',\n    color='#ea00d9',\n    legend=False,\n    ax=ax\n)\n\n# Setting axis labels and title\nax.set_xlabel('Year', fontdict=axis_fontdict)\nax.set_ylabel('Total Income', fontdict=axis_fontdict)\nax.set_title('Total Income by Place, Philadelphia', fontdict=title_fontdict, pad=20)\n\n# Customize axis colors\nax.xaxis.label.set_color('white')\nax.yaxis.label.set_color('white')\nax.tick_params(axis='x', colors='white')  # X-axis ticks\nax.tick_params(axis='y', colors='white')  # Y-axis ticks\n\n# Customize spines\nax.spines['bottom'].set_color('white') \nax.spines['left'].set_color('white')\nax.spines['top'].set_visible(False)\nax.spines['right'].set_visible(False)\n\n# Highlight the projected year\nax.axvspan(2024.5, 2025.5, color='#711C91', alpha=0.5, zorder=0)\n\n# Add \"Projected Total Spend\" label on the plot\nax.text(2025, result['Total Income'].max() * 0.9, 'Projected',\n        fontsize=10, color='white', ha='center', va='center', family='Cambria')\n\n# Customize ticks\nax.set_xticks(range(2019, 2026, 1))\nax.set_yticks(range(0, 30000000, 5000000))\n\n# Turn off scientific notation on y-axis\nax.get_yaxis().get_major_formatter().set_scientific(False)\nax.yaxis.set_major_formatter(FuncFormatter(lambda x, _: f\"{x:,.0f}\"))\n\n# Set the background colors\nfig.set_facecolor('#091833')\nax.set_facecolor('#091833')\n\nplt.show()\n\n\n\n\n\n\n\n\n\nIn terms of the spatial distribution of the spend data, the map below shows that the higher points are still mainly located in the city center and spread in other areas, which is consistent with the previous years.\n\n\nCode\ncity = gpd.read_file('../data/City_Limits.geojson')\n\n# City plot\ncity_plot = city.hvplot(geo=True, alpha=0.5, line_color='black', line_width=1, color='white', hover=False, crs=4326)\n\n\nresults_plot = data_2024_results.hvplot(\n    geo=True,\n    c='predicted_spend_2025', \n    cmap='viridis', \n    hover_cols=['LOCATION_NAME', 'predicted_spend_2025'], \n    dynamic=False,\n    width=800,\n    height=600,\n    crs=4326,\n    title=\"Total Income by Place\",\n    xlabel=\"Longitude\",\n    ylabel=\"Latitude\",\n    line_color='white',\n    line_width=0.3,\n    clim=(0, 20000)\n)\n\ncity_plot * results_plot",
    "crumbs": [
      "Analysis",
      "Modeling"
    ]
  },
  {
    "objectID": "analysis/exploratory.html#total-number-of-visits",
    "href": "analysis/exploratory.html#total-number-of-visits",
    "title": "Exploratory Analysis",
    "section": "Total Number of Visits",
    "text": "Total Number of Visits\nThe chart shows the total number of visits over time. The number of visits first decreased vastly in 2020 due to the COVID-19 pandemic, and then started to recover in 2021 and reached a peak in 2022. However, the number of visits started to decrease again in 2022. In 2024, the number of visits is almost the same as in 2020.\n\n\nCode\n# Adjusted axis and title font properties\naxis_fontdict = {'fontsize': 12, 'family': 'Cambria', 'color': 'white'}\ntitle_fontdict = {'fontsize': 14, 'fontweight': 'bold', 'family': 'Cambria', 'color': 'white'}\n\nfig, ax = plt.subplots(figsize=(6, 4))\n\n# Plotting\nsummary_table.plot(\n    x='Year', \n    y='Total Visits',\n    marker='o',\n    color='#ea00d9',\n    legend=False,\n    ax=ax\n)\n\n# Setting axis labels and title\nax.set_xlabel('Year', fontdict=axis_fontdict)\nax.set_ylabel('Total Visits', fontdict=axis_fontdict)\nax.set_title('Total Number of Visits, Philadelphia', fontdict=title_fontdict, pad=20)\n\n# Customize axis colors\nax.xaxis.label.set_color('white')\nax.yaxis.label.set_color('white')\nax.tick_params(axis='x', colors='white')  # X-axis ticks\nax.tick_params(axis='y', colors='white')  # Y-axis ticks\n\n# Customize spines\nax.spines['bottom'].set_color('white') \nax.spines['left'].set_color('white')\nax.spines['top'].set_visible(False)\nax.spines['right'].set_visible(False)\n\n# Customize ticks\nax.set_xticks(range(2019, 2025, 1))\nax.set_yticks(range(0, 7500000, 1500000))\n\n# Turn off scientific notation on y-axis\nax.get_yaxis().get_major_formatter().set_scientific(False)\nax.yaxis.set_major_formatter(FuncFormatter(lambda x, _: f\"{x:,.0f}\"))\n\n# Set the background colors\nfig.set_facecolor('#091833')\nax.set_facecolor('#091833')\n\nplt.show()\n\n\n\n\n\n\n\n\n\nHere I map the total number of visits in Philadelphia from 2019 to 2024 using hvplot, and you can drag the slider to see the changes over time. It can be seen that the most visited places are in the Center City area, and the number of yellow dots (representing a higher number of visits) dropped since 2019, which is aligned with the conclusion from the line chart.\n\n\nCode\n# Sort by visit counts so that the largest points are plotted on top\nvisits = advan_sg.copy().sort_values(by='RAW_VISIT_COUNTS')\n\nvisit_plot = visits.hvplot(\n    geo=True,\n    c='RAW_VISIT_COUNTS', \n    cmap='viridis', \n    hover_cols=['LOCATION_NAME', 'RAW_VISIT_COUNTS'], \n    groupby='DATE_RANGE_START',\n    dynamic=False,\n    width=800,\n    height=600,\n    crs=4326,\n    title=\"Number of Visits by Place\",\n    xlabel=\"Longitude\",\n    ylabel=\"Latitude\",\n    line_color='white',\n    line_width=0.3,\n    clim=(0, 5000)\n)\n\ncity_plot * visit_plot",
    "crumbs": [
      "Analysis",
      "Exploratory Analysis"
    ]
  },
  {
    "objectID": "analysis/exploratory.html#total-number-of-visitors",
    "href": "analysis/exploratory.html#total-number-of-visitors",
    "title": "Exploratory Analysis",
    "section": "Total Number of Visitors",
    "text": "Total Number of Visitors\nThe chart and the map show the total number of visitors over time, and we can found that there is no suprise that the trend and spatial pattern is very similar to the total number of visits.\n\n\nCode\n# Adjusted axis and title font properties\naxis_fontdict = {'fontsize': 12, 'family': 'Cambria', 'color': 'white'}\ntitle_fontdict = {'fontsize': 14, 'fontweight': 'bold', 'family': 'Cambria', 'color': 'white'}\n\nfig, ax = plt.subplots(figsize=(6, 4))\n\n# Plotting\nsummary_table.plot(\n    x='Year', \n    y='Total Visitors',\n    marker='o',\n    color='#ea00d9',\n    legend=False,\n    ax=ax\n)\n\n# Setting axis labels and title\nax.set_xlabel('Year', fontdict=axis_fontdict)\nax.set_ylabel('Total Visitors', fontdict=axis_fontdict)\nax.set_title('Total Number of Visitors, Philadelphia', fontdict=title_fontdict, pad=20)\n\n# Customize axis colors\nax.xaxis.label.set_color('white')\nax.yaxis.label.set_color('white')\nax.tick_params(axis='x', colors='white')  # X-axis ticks\nax.tick_params(axis='y', colors='white')  # Y-axis ticks\n\n# Customize spines\nax.spines['bottom'].set_color('white') \nax.spines['left'].set_color('white')\nax.spines['top'].set_visible(False)\nax.spines['right'].set_visible(False)\n\n# Customize ticks\nax.set_xticks(range(2019, 2025, 1))\nax.set_yticks(range(0, 5000000, 1000000))\n\n# Turn off scientific notation on y-axis\nax.get_yaxis().get_major_formatter().set_scientific(False)\nax.yaxis.set_major_formatter(FuncFormatter(lambda x, _: f\"{x:,.0f}\"))\n\n# Set the background colors\nfig.set_facecolor('#091833')\nax.set_facecolor('#091833')\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Sort by visitor counts so that the largest points are plotted on top\nvisitors = advan_sg.copy().sort_values(by='RAW_VISITOR_COUNTS')\n\nvisitor_plot = visits.hvplot(\n    geo=True,\n    c='RAW_VISITOR_COUNTS', \n    cmap='viridis', \n    hover_cols=['LOCATION_NAME', 'RAW_VISITOR_COUNTS'], \n    groupby='DATE_RANGE_START',\n    dynamic=False,\n    width=800,\n    height=600,\n    crs=4326,\n    title=\"Number of Visitors by Place\",\n    xlabel=\"Longitude\",\n    ylabel=\"Latitude\",\n    line_color='white',\n    line_width=0.3,\n    clim=(0, 5000)\n)\n\ncity_plot * visitor_plot",
    "crumbs": [
      "Analysis",
      "Exploratory Analysis"
    ]
  },
  {
    "objectID": "analysis/exploratory.html#total-income-spend",
    "href": "analysis/exploratory.html#total-income-spend",
    "title": "Exploratory Analysis",
    "section": "Total Income (Spend)",
    "text": "Total Income (Spend)\nThe spend data is aggregated by places, so it can also be seen as the income of the store.\nThe chart shows the total income over time. The trend is kind of different to the total number of visits. The total income first decreased vastly in 2020 and then started to recover in 2021 and reached a peak in 2022. However, the number of visits started to decrease again in 2022 but seemed to stay steady at the same level of pre-pandemic period. Recall that some reports mentioned that there were signs of a turnaround in 2023 in the background section. It seems our data also supports this conclusion. But is this really the case? I will further investigate this in the next section.\n\n\nCode\n# Adjusted axis and title font properties\naxis_fontdict = {'fontsize': 12, 'family': 'Cambria', 'color': 'white'}\ntitle_fontdict = {'fontsize': 14, 'fontweight': 'bold', 'family': 'Cambria', 'color': 'white'}\n\nfig, ax = plt.subplots(figsize=(6, 4))\n\n# Plotting\nsummary_table.plot(\n    x='Year', \n    y='Total Spend',\n    marker='o',\n    color='#ea00d9',\n    legend=False,\n    ax=ax\n)\n\n# Setting axis labels and title\nax.set_xlabel('Year', fontdict=axis_fontdict)\nax.set_ylabel('Total Income', fontdict=axis_fontdict)\nax.set_title('Total Income, Philadelphia', fontdict=title_fontdict, pad=20)\n\n# Customize axis colors\nax.xaxis.label.set_color('white')\nax.yaxis.label.set_color('white')\nax.tick_params(axis='x', colors='white')  # X-axis ticks\nax.tick_params(axis='y', colors='white')  # Y-axis ticks\n\n# Customize spines\nax.spines['bottom'].set_color('white') \nax.spines['left'].set_color('white')\nax.spines['top'].set_visible(False)\nax.spines['right'].set_visible(False)\n\n# Customize ticks\nax.set_xticks(range(2019, 2025, 1))\nax.set_yticks(range(0, 30000000, 5000000))\n\n# Turn off scientific notation on y-axis\nax.get_yaxis().get_major_formatter().set_scientific(False)\nax.yaxis.set_major_formatter(FuncFormatter(lambda x, _: f\"{x:,.0f}\"))\n\n# Set the background colors\nfig.set_facecolor('#091833')\nax.set_facecolor('#091833')\n\nplt.show()\n\n\n\n\n\n\n\n\n\nIn terms of the spatial pattern, the map shows that the total income is higher in the Center City area, which is consistent with the total number of visits. However, it can also be seen that the total income is high in the northwestern and northeastern parts of the city, which may indicate that the average spend or the average number of transactions per visit is higher in these areas.\n\n\nCode\n# Sort by visit counts so that the largest points are plotted on top\nincome = advan_sg.copy().sort_values(by='RAW_TOTAL_SPEND')\n\nincome_plot = visits.hvplot(\n    geo=True,\n    c='RAW_TOTAL_SPEND', \n    cmap='viridis', \n    hover_cols=['LOCATION_NAME', 'RAW_TOTAL_SPEND'], \n    groupby='DATE_RANGE_START',\n    dynamic=False,\n    width=800,\n    height=600,\n    crs=4326,\n    title=\"Total Income by Place\",\n    xlabel=\"Longitude\",\n    ylabel=\"Latitude\",\n    line_color='white',\n    line_width=0.3,\n    clim=(0, 20000)\n)\n\ncity_plot * income_plot",
    "crumbs": [
      "Analysis",
      "Exploratory Analysis"
    ]
  },
  {
    "objectID": "analysis/exploratory.html#total-number-of-transactions",
    "href": "analysis/exploratory.html#total-number-of-transactions",
    "title": "Exploratory Analysis",
    "section": "Total Number of Transactions",
    "text": "Total Number of Transactions\nThe chart shows the total number of transactions over time, and we can found that there is no suprise that the trend and spatial pattern is very similar to the total spend.\n\n\nCode\n# Adjusted axis and title font properties\naxis_fontdict = {'fontsize': 12, 'family': 'Cambria', 'color': 'white'}\ntitle_fontdict = {'fontsize': 14, 'fontweight': 'bold', 'family': 'Cambria', 'color': 'white'}\n\nfig, ax = plt.subplots(figsize=(6, 4))\n\n# Plotting\nsummary_table.plot(\n    x='Year', \n    y='Total Transactions',\n    marker='o',\n    color='#ea00d9',\n    legend=False,\n    ax=ax\n)\n\n# Setting axis labels and title\nax.set_xlabel('Year', fontdict=axis_fontdict)\nax.set_ylabel('Total Transactions', fontdict=axis_fontdict)\nax.set_title('Total Number of Transactions, Philadelphia', fontdict=title_fontdict, pad=20)\n\n# Customize axis colors\nax.xaxis.label.set_color('white')\nax.yaxis.label.set_color('white')\nax.tick_params(axis='x', colors='white')  # X-axis ticks\nax.tick_params(axis='y', colors='white')  # Y-axis ticks\n\n# Customize spines\nax.spines['bottom'].set_color('white') \nax.spines['left'].set_color('white')\nax.spines['top'].set_visible(False)\nax.spines['right'].set_visible(False)\n\n# Customize ticks\nax.set_xticks(range(2019, 2025, 1))\nax.set_yticks(range(0, 1000000, 200000))\n\n# Turn off scientific notation on y-axis\nax.get_yaxis().get_major_formatter().set_scientific(False)\nax.yaxis.set_major_formatter(FuncFormatter(lambda x, _: f\"{x:,.0f}\"))\n\n# Set the background colors\nfig.set_facecolor('#091833')\nax.set_facecolor('#091833')\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Sort by transaction counts so that the largest points are plotted on top\ntransaction = advan_sg.copy().sort_values(by='RAW_NUM_TRANSACTIONS')\n\ntransaction_plot = visits.hvplot(\n    geo=True,\n    c='RAW_NUM_TRANSACTIONS', \n    cmap='viridis', \n    hover_cols=['LOCATION_NAME', 'RAW_NUM_TRANSACTIONS'], \n    groupby='DATE_RANGE_START',\n    dynamic=False,\n    width=800,\n    height=600,\n    crs=4326,\n    title=\"Number of Transactions by Place\",\n    xlabel=\"Longitude\",\n    ylabel=\"Latitude\",\n    line_color='white',\n    line_width=0.3,\n    clim=(0, 1000)\n)\n\ncity_plot * transaction_plot",
    "crumbs": [
      "Analysis",
      "Exploratory Analysis"
    ]
  },
  {
    "objectID": "analysis/conclusion.html",
    "href": "analysis/conclusion.html",
    "title": "Conclusion and Limitation",
    "section": "",
    "text": "This project analyzed the income changes of Philadelphia stores from 2019 to 2024, and forecast the income of 2025 using July data of each year from ADVAN and SafeGraph.\nThe data shows that the total number of visits and the total number of visitors first decreased vastly in 2020 due to the COVID-19 pandemic, and then started to recover in 2021 and reached a peak in 2022, but then decreased again. Spatial wise, the most visited places are in the Center City area and the airport. In terms of total income and total number of transactions, the number of visits started to decrease again in 2022 but seemed to stay steady at the same level of pre-pandemic period. Spatial wise, the total income is higher in the Center City area, which is consistent with the total number of visits. However, it is also high in the northwestern and northeastern parts of the city, which may be related to the higher average spend or the average number of transactions per visit is in these areas.\nThis project also builds a machine learning model to predict the store income of 2025 using Random Forest algorithm. It also only involves some spatial features using K Nearest Neighbor (K=5 in this project) that help reduce the error. Four models using different features are compared and I use the model with best performance in terms of median absolute error and R-squared to make prediction. The result shows that there would be a significant decrease on total income in Philadelphia, the mean income of stores would also decrease but is much more steady compared to the total income.\nThus, this project can inform business owner to strategize cost management, or adapt to changing consumer behaviors. Meanwhile, this project can be a good tool for local governments and urban planners to make decisions about economic development and investment in Philadelphia’s retail sector.\n\n\n\nHowever, the project has a key limitation: the dataset used does not cover all stores in Philadelphia but only a relatively big sample. This limitation suggests the need for more comprehensive data to improve the accuracy of future analyses. Furthermore, more features could be introduced to the model, such as local demographic characteristics.",
    "crumbs": [
      "Analysis",
      "Conclusion and Limitation"
    ]
  },
  {
    "objectID": "analysis/conclusion.html#conclusion",
    "href": "analysis/conclusion.html#conclusion",
    "title": "Conclusion and Limitation",
    "section": "",
    "text": "This project analyzed the income changes of Philadelphia stores from 2019 to 2024, and forecast the income of 2025 using July data of each year from ADVAN and SafeGraph.\nThe data shows that the total number of visits and the total number of visitors first decreased vastly in 2020 due to the COVID-19 pandemic, and then started to recover in 2021 and reached a peak in 2022, but then decreased again. Spatial wise, the most visited places are in the Center City area and the airport. In terms of total income and total number of transactions, the number of visits started to decrease again in 2022 but seemed to stay steady at the same level of pre-pandemic period. Spatial wise, the total income is higher in the Center City area, which is consistent with the total number of visits. However, it is also high in the northwestern and northeastern parts of the city, which may be related to the higher average spend or the average number of transactions per visit is in these areas.\nThis project also builds a machine learning model to predict the store income of 2025 using Random Forest algorithm. It also only involves some spatial features using K Nearest Neighbor (K=5 in this project) that help reduce the error. Four models using different features are compared and I use the model with best performance in terms of median absolute error and R-squared to make prediction. The result shows that there would be a significant decrease on total income in Philadelphia, the mean income of stores would also decrease but is much more steady compared to the total income.\nThus, this project can inform business owner to strategize cost management, or adapt to changing consumer behaviors. Meanwhile, this project can be a good tool for local governments and urban planners to make decisions about economic development and investment in Philadelphia’s retail sector.",
    "crumbs": [
      "Analysis",
      "Conclusion and Limitation"
    ]
  },
  {
    "objectID": "analysis/conclusion.html#limitation",
    "href": "analysis/conclusion.html#limitation",
    "title": "Conclusion and Limitation",
    "section": "",
    "text": "However, the project has a key limitation: the dataset used does not cover all stores in Philadelphia but only a relatively big sample. This limitation suggests the need for more comprehensive data to improve the accuracy of future analyses. Furthermore, more features could be introduced to the model, such as local demographic characteristics.",
    "crumbs": [
      "Analysis",
      "Conclusion and Limitation"
    ]
  }
]