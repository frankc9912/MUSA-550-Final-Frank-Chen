{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "format: \n",
    "  html:\n",
    "    toc: true\n",
    "    code-fold: true\n",
    "execute:\n",
    "  echo: true\n",
    "  eval: false\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Sources\n",
    "\n",
    "In this section, I will show how to get the data for this project. I will give an example of using API to call the **foot traffic data** provided by [Dewey](https://www.deweydata.io/). There is also an official guide on youtube that you can refer to [here](https://www.youtube.com/watch?v=SlJAZ_91UPg). For the **spend pattern data**, the process would be exactly the same except for the `API Key` and `file path` obtained from the website. Here are the details about the data sets:\n",
    "\n",
    "- [ADVAN monthly foot traffic data](https://app.deweydata.io/products/5acc9f39-1ca6-4535-b3ff-38f6b9baf85e/package). It includes aggregated raw counts of visits to POIs from a panel of mobile devices over a given month, detailing how often people visit, how long they stay, where they came from, where else they go, and more.\n",
    "- [SafeGraph monthly spend patterns data](https://app.deweydata.io/products/eb6e748a-0fdd-4bc7-9dd7-bbed0890948d/package). It includes aggregated, anonymized credit and debit transaction data associated to specific stores, including median spend per day, median spend per customer, and other detailed statistics, as well as where else consumers spend money and the breakdown of online/offline spending.\n",
    "\n",
    "The two data sets can be joined by a shared column called PlaceKey, which is the unique and persistent ID tied to a POI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to install the `deweydatapy` package, and then import it to use the API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install deweydatapy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import deweydatapy as ddp\n",
    "import pandas as pd\n",
    "import os\n",
    "import geopandas as gpd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we need to get the `API key` and the `file path` from the Dewey website. After that, we can use the `get_meta` function to have an overview of the wanted data. It suggests that the total size of the data is about 750,000 MB, and is aggregated by month from 1/1/2019 to 10/1/2024."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monthly patterns api key\n",
    "apikey_ = \"Your API Key\"\n",
    "\n",
    "# File path\n",
    "path = \"Your file path\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "Metadata summary ------------------------------------------------\n",
      "Total number of files: 3,607\n",
      "Total files size (MB): 748,164.8\n",
      "Date aggregation: MONTH\n",
      "Date partition column: DATE_RANGE_START\n",
      "Data min available date: 2019-01-01\n",
      "Data max available date: 2024-10-01\n",
      "-----------------------------------------------------------------\n",
      " \n"
     ]
    }
   ],
   "source": [
    "meta = ddp.get_meta(apikey_, path, print_meta=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we can specify the date range for which we want to get the data. If we did not specify a specific date range, the API will return the whole data of a year and the file size would be too large for the computer to handle. So, we will use the `start_date` and `end_date` parameters to get the data of July from 2019 to 2024. The date format should be `YYYY-MM-DD`. For example, to get the data for the month of July 2021, we can set the `start_date` to `2021-07-01` and the `end_date` to `2021-07-31`. \n",
    "\n",
    "Here we can write a for loop to get the data of each year and store them in a list, the data size is 62.5 GB in total.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting files information for page 1/1...\n",
      "Files information collection completed.\n",
      " \n",
      "Files information summary ---------------------------------------\n",
      "Total number of pages: 1\n",
      "Total number of files: 60\n",
      "Total files size (MB): 12,389.41\n",
      "Average single file size (MB): 206.49\n",
      "Date partition column: DATE_RANGE_START\n",
      "Expires at: 2024-11-15T13:58:41.429Z\n",
      "-----------------------------------------------------------------\n",
      "Collecting files information for page 1/1...\n",
      "Files information collection completed.\n",
      " \n",
      "Files information summary ---------------------------------------\n",
      "Total number of pages: 1\n",
      "Total number of files: 47\n",
      "Total files size (MB): 9,768.66\n",
      "Average single file size (MB): 207.84\n",
      "Date partition column: DATE_RANGE_START\n",
      "Expires at: 2024-11-15T13:58:42.285Z\n",
      "-----------------------------------------------------------------\n",
      "Collecting files information for page 1/1...\n",
      "Files information collection completed.\n",
      " \n",
      "Files information summary ---------------------------------------\n",
      "Total number of pages: 1\n",
      "Total number of files: 48\n",
      "Total files size (MB): 10,033.68\n",
      "Average single file size (MB): 209.03\n",
      "Date partition column: DATE_RANGE_START\n",
      "Expires at: 2024-11-15T13:58:43.121Z\n",
      "-----------------------------------------------------------------\n",
      "Collecting files information for page 1/1...\n",
      "Files information collection completed.\n",
      " \n",
      "Files information summary ---------------------------------------\n",
      "Total number of pages: 1\n",
      "Total number of files: 53\n",
      "Total files size (MB): 11,003.53\n",
      "Average single file size (MB): 207.61\n",
      "Date partition column: DATE_RANGE_START\n",
      "Expires at: 2024-11-15T13:58:43.995Z\n",
      "-----------------------------------------------------------------\n",
      "Collecting files information for page 1/1...\n",
      "Files information collection completed.\n",
      " \n",
      "Files information summary ---------------------------------------\n",
      "Total number of pages: 1\n",
      "Total number of files: 61\n",
      "Total files size (MB): 12,659.24\n",
      "Average single file size (MB): 207.53\n",
      "Date partition column: DATE_RANGE_START\n",
      "Expires at: 2024-11-15T13:58:44.852Z\n",
      "-----------------------------------------------------------------\n",
      "Collecting files information for page 1/1...\n",
      "Files information collection completed.\n",
      " \n",
      "Files information summary ---------------------------------------\n",
      "Total number of pages: 1\n",
      "Total number of files: 54\n",
      "Total files size (MB): 11,261.46\n",
      "Average single file size (MB): 208.55\n",
      "Date partition column: DATE_RANGE_START\n",
      "Expires at: 2024-11-15T13:58:45.741Z\n",
      "-----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "year_var = [str(year) for year in range(19, 25)]\n",
    "file_lists = {}\n",
    "\n",
    "for i in year_var:\n",
    "    start_date = \"20\" + i + \"-07-01\"\n",
    "    end_date = \"20\" + i + \"-07-31\"\n",
    "\n",
    "    file_list = ddp.get_file_list(apikey_, path, start_date=start_date, end_date=end_date, print_info=True)\n",
    "\n",
    "    dataframe_name = \"file_df_\" + i\n",
    "    file_lists[dataframe_name] = pd.DataFrame(file_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "62.5066275279969"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "size = []\n",
    "\n",
    "for i in year_var:\n",
    "    file_df = \"file_df_\" + i\n",
    "    size.append(file_lists[file_df][\"file_size_bytes\"].sum())\n",
    "\n",
    "sum(size) / (1024 ** 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can use the `download_files` function to download the data to the specified file path, and use the `CITY` and `REGION` parameters to filter the data to Philadelphia. The data will be saved in a `.csv` file format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in year_var:\n",
    "    save_path = \"data/ADVAN/data_\" + i\n",
    "    file_df = \"file_df_\" + i\n",
    "    df_to_download = file_lists[file_df]\n",
    "\n",
    "    ddp.download_files(df_to_download, save_path, skip_exists=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2019\n",
    "folder_path = \"data/ADVAN/data_19/\"\n",
    "files = [file for file in os.listdir(folder_path)]\n",
    "dfs = []\n",
    "\n",
    "for file in files:\n",
    "    file_path = os.path.join(folder_path, file)\n",
    "    df = pd.read_csv(file_path)\n",
    "    dfs.append(df)\n",
    "\n",
    "df_final_19 = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "df_philly_19 = df_final_19.loc[(df_final_19[\"CITY\"] == \"Philadelphia\") & (df_final_19[\"REGION\"] == \"PA\")]\n",
    "\n",
    "df_final_19.to_csv(\"data/ADVAN/df_19.csv\", index=False)\n",
    "df_philly_19.to_csv(\"data/ADVAN/df_philly_19.csv\", index=False)\n",
    "\n",
    "#2020\n",
    "folder_path = \"data/ADVAN/data_20/\"\n",
    "files = [file for file in os.listdir(folder_path)]\n",
    "dfs = []\n",
    "\n",
    "for file in files:\n",
    "    file_path = os.path.join(folder_path, file)\n",
    "    df = pd.read_csv(file_path)\n",
    "    dfs.append(df)\n",
    "\n",
    "df_final_20 = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "df_philly_20 = df_final_20.loc[(df_final_20[\"CITY\"] == \"Philadelphia\") & (df_final_20[\"REGION\"] == \"PA\")]\n",
    "\n",
    "df_final_20.to_csv(\"data/ADVAN/df_20.csv\", index=False)\n",
    "df_philly_20.to_csv(\"data/ADVAN/df_philly_20.csv\", index=False)\n",
    "\n",
    "#2021\n",
    "folder_path = \"data/ADVAN/data_21/\"\n",
    "files = [file for file in os.listdir(folder_path)]\n",
    "dfs = []\n",
    "\n",
    "for file in files:\n",
    "    file_path = os.path.join(folder_path, file)\n",
    "    df = pd.read_csv(file_path)\n",
    "    dfs.append(df)\n",
    "\n",
    "df_final_21 = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "df_philly_21 = df_final_21.loc[(df_final_21[\"CITY\"] == \"Philadelphia\") & (df_final_21[\"REGION\"] == \"PA\")]\n",
    "\n",
    "df_final_21.to_csv(\"data/ADVAN/df_21.csv\", index=False)\n",
    "df_philly_21.to_csv(\"data/ADVAN/df_philly_21.csv\", index=False)\n",
    "\n",
    "#2022\n",
    "folder_path = \"data/ADVAN/data_22/\"\n",
    "files = [file for file in os.listdir(folder_path)]\n",
    "dfs = []\n",
    "\n",
    "for file in files:\n",
    "    file_path = os.path.join(folder_path, file)\n",
    "    df = pd.read_csv(file_path)\n",
    "    dfs.append(df)\n",
    "\n",
    "df_final_22 = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "df_philly_22 = df_final_22.loc[(df_final_22[\"CITY\"] == \"Philadelphia\") & (df_final_22[\"REGION\"] == \"PA\")]\n",
    "\n",
    "df_final_22.to_csv(\"data/ADVAN/df_22.csv\", index=False)\n",
    "df_philly_22.to_csv(\"data/ADVAN/df_philly_22.csv\", index=False)\n",
    "\n",
    "#2023\n",
    "folder_path = \"data/ADVAN/data_23/\"\n",
    "files = [file for file in os.listdir(folder_path)]\n",
    "dfs = []\n",
    "\n",
    "for file in files:\n",
    "    file_path = os.path.join(folder_path, file)\n",
    "    df = pd.read_csv(file_path)\n",
    "    dfs.append(df)\n",
    "\n",
    "df_final_23 = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "df_philly_23 = df_final_23.loc[(df_final_23[\"CITY\"] == \"Philadelphia\") & (df_final_23[\"REGION\"] == \"PA\")]\n",
    "\n",
    "df_final_23.to_csv(\"data/ADVAN/df_23.csv\", index=False)\n",
    "df_philly_23.to_csv(\"data/ADVAN/df_philly_23.csv\", index=False)\n",
    "\n",
    "#2024\n",
    "folder_path = \"data/ADVAN/data_24/\"\n",
    "files = [file for file in os.listdir(folder_path)]\n",
    "dfs = []\n",
    "\n",
    "for file in files:\n",
    "    file_path = os.path.join(folder_path, file)\n",
    "    df = pd.read_csv(file_path)\n",
    "    dfs.append(df)\n",
    "\n",
    "df_final_24 = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "df_philly_24 = df_final_24.loc[(df_final_24[\"CITY\"] == \"Philadelphia\") & (df_final_24[\"REGION\"] == \"PA\")]\n",
    "\n",
    "df_final_24.to_csv(\"data/ADVAN/df_24.csv\", index=False)\n",
    "df_philly_24.to_csv(\"data/ADVAN/df_philly_24.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have downloaded the foot traffic data and spend pattern data. For the sake of computer RAM capacity, we can restart the kernel and load the data in the following sections, where we will filter the data by `NAICS_CODE` and join the two data sets by the `PlaceKey` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chenf\\AppData\\Local\\Temp\\ipykernel_1524\\2411687974.py:2: DtypeWarning: Columns (17,18,19) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  advan_raw = pd.read_csv('data/ADVAN/df_philly_19_24.csv')\n"
     ]
    }
   ],
   "source": [
    "# Load the ADVAN data\n",
    "advan_raw = pd.read_csv('data/ADVAN/df_philly_19_24.csv')\n",
    "\n",
    "# Load the SafeGraph data\n",
    "sg_raw = pd.read_csv('data/SafeGraph/df_philly_19_24.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Advan foot traffic data has 373583 rows and SafeGraph spend pattern data has 24083 rows.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Advan foot traffic data has {advan_raw.shape[0]} rows and SafeGraph spend pattern data has {sg_raw.shape[0]} rows.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "advan_raw['NAICS_CODE'] = advan_raw['NAICS_CODE'].astype(str)\n",
    "\n",
    "advan = advan_raw.loc[\n",
    "    ~advan_raw['POPULARITY_BY_HOUR'].isnull()\n",
    "    ].loc[\n",
    "    advan_raw['NAICS_CODE'].str.startswith('42') |  # Wholesale Trade\n",
    "    advan_raw['NAICS_CODE'].str.startswith('44') |  # Retail Trade\n",
    "    advan_raw['NAICS_CODE'].str.startswith('45') |  # Retail Trade\n",
    "    advan_raw['NAICS_CODE'].str.startswith('72')    # Accommodation and Food Services\n",
    "]\n",
    "\n",
    "advan_gdf = gpd.GeoDataFrame(\n",
    "    advan, \n",
    "    geometry=gpd.points_from_xy(advan['LONGITUDE'], advan['LATITUDE']),\n",
    "    crs='EPSG:4326'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can save the joined data of each year to `.geojson` file for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "advan_sg = advan_gdf.merge(\n",
    "    sg_raw,\n",
    "    left_on=['PLACEKEY', 'DATE_RANGE_START'],\n",
    "    right_on=['PLACEKEY', 'SPEND_DATE_RANGE_START'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Convert to the same CRS\n",
    "advan_gdf = advan_gdf.to_crs(2272)\n",
    "advan_sg = advan_sg.to_crs(2272)\n",
    "\n",
    "# Select columns\n",
    "advan_sg = advan_sg[['PLACEKEY', 'LOCATION_NAME', 'TOP_CATEGORY', 'LATITUDE', 'LONGITUDE', 'DATE_RANGE_START', 'RAW_VISIT_COUNTS', 'RAW_VISITOR_COUNTS', 'RAW_TOTAL_SPEND', 'RAW_NUM_TRANSACTIONS', 'RAW_NUM_CUSTOMERS']]\n",
    "advan_sg['DATE_RANGE_START'] = advan_sg['DATE_RANGE_START'].dt.year\n",
    "\n",
    "advan_sg = gpd.GeoDataFrame(advan_sg, geometry=gpd.points_from_xy(advan_sg['LONGITUDE'], advan_sg['LATITUDE']), crs='EPSG:4326')\n",
    "\n",
    "# Save the data\n",
    "advan_sg.to_file('data/advan_sg.geojson', driver='GeoJSON')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "musa-550",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
